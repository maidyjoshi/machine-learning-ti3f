{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDLexCog2ykm"
      },
      "source": [
        "## Tugas Praktikum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIEQ1USr2ykq"
      },
      "source": [
        "Gunakan tf.GradientTape untuk men track nilai gradient. Anda dapat mempelajari lebih lanjut tentang pendekatan ini dengan membaca eager execution guide.\n",
        "Prosedurnya adalah :\n",
        "1. Jalankan Model dan hitung loss dengan tf.GradientTape.\n",
        "2. Hitung update dan terapkan pada model dengan optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf7Lg8TZ2ykr"
      },
      "source": [
        "### Mengulang Praktikum 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Tf_kPPBTBV9G"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BknBnF7BV9K",
        "outputId": "ba5ffa52-be62-4b81-c557-13584fe10cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m_LXJJhBV9L",
        "outputId": "0f41d5a6-41b1-4b63-afec-1028a7a2170d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylnuFzMIBkxa",
        "outputId": "3cec9ced-60e0-4a17-aa11-306d627b734c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBzajNJWBnlS",
        "outputId": "d2fa6902-e9cf-472d-e9c9-9624c8118878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SECZw_WkBpqj",
        "outputId": "34af6277-2057-434a-e7fd-7e453271a173"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QfoaNZScBruk"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_mry4zpxBurI"
      },
      "outputs": [],
      "source": [
        "ids = ids_from_chars(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UIQ-_zviB51p"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVegNn4CB108",
        "outputId": "1db65b71-3dec-4520-e6b7-d0ebf8c37d6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZ2ZvwTiB7kR",
        "outputId": "d1d12c6a-ad47-4977-9fb2-7a22eafbe513"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "g_vh8UaGB9f2"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na7T-5ioCA5U",
        "outputId": "4472ef1a-5855-4ec8-8cca-b5340fb88793"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "v9RIBjf6CEgV"
      },
      "outputs": [],
      "source": [
        "\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HtIL1iRCHZr",
        "outputId": "05ab8eaa-2c94-4573-b31a-a49cf4f3c0e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zs_d9kg9CJeZ"
      },
      "outputs": [],
      "source": [
        "seq_length = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPtEuDizCLLd",
        "outputId": "5b6b94f0-69c5-43bb-8b12-b7b06c0a17c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qymRlRcBCNDT",
        "outputId": "022159f0-5e71-4974-914e-382991df8842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hPrQKHGqCQEO"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77G75zKnCRq8",
        "outputId": "49dac0aa-40b3-4f59-ed3b-e71d1f76b0f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0WNgsixPCSgv"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v529INprCUxa",
        "outputId": "9d7c8cda-74f9-4b0b-b989-14ad19c57009"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFzs38ulCXGF",
        "outputId": "d3bb9dc7-abdc-4307-95f4-b98c3d98e84a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BeuGjLXHCbVd"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "SzC5FKI-CcNz"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jmsLoOhHCeMo"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdYegFFhCg3V",
        "outputId": "e5efce03-b7a3-4df2-dc6e-8a47cfcd8679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5476y7kECjhn",
        "outputId": "4bdd9b10-20e3-4bf0-b905-433322ef5cd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "sPLAMMblClDm"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbyX-xbTCm0r",
        "outputId": "0151a4e8-f638-4e88-f5f8-2fcea172a906"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([33, 17, 27, 11, 65,  2, 39, 40, 35, 44, 54, 21, 42, 48, 35, 46, 12,\n",
              "       20, 29, 34, 64, 39, 65,  8, 57, 45, 40, 56, 24, 16, 15, 46, 36, 13,\n",
              "       12, 58, 42, 47, 55, 27, 21,  7, 34, 54, 34, 48, 35, 50,  5,  8, 15,\n",
              "       27, 50, 22, 10, 58, 37, 22,  6, 52, 60, 17, 12, 49, 31, 64, 65, 47,\n",
              "       39, 49, 12,  3, 10,  2, 12, 46, 42, 59, 50,  8, 13, 61, 37,  7, 53,\n",
              "       61,  8, 45, 55, 36, 17,  6,  1, 51,  9, 28, 40, 52, 62, 43])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxdDXeYYCoeJ",
        "outputId": "b274b61d-9b36-4c1f-9fc5-a8a1844ce687"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'ORTHUMBERLAND:\\nNor I.\\n\\nCLIFFORD:\\nCome, cousin, let us tell the queen these news.\\n\\nWESTMORELAND:\\nFare'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"TDN:z ZaVeoHciVg;GPUyZz-rfaqKCBgW?;schpNH,UoUiVk&-BNkI3sXI'muD;jRyzhZj;!3 ;gctk-?vX,nv-fpWD'\\nl.Oamwd\"\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "0uD0PY4QCqHE"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwsQznroCr0V",
        "outputId": "d5072f2a-c58f-43b4-cf90-1095de8a0be3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1897697, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6Fbs0-wCtOh",
        "outputId": "d19056c8-05b8-415e-bea6-3b027ca2ab85"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.00759"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "JBE4h6ymCvDN"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "d_PyPAxdCy49"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSj_esbnCzkT",
        "outputId": "1efdf73c-9295-4b01-8fd7-b4eb7f0102f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 20s 62ms/step - loss: 2.7138\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 10s 51ms/step - loss: 1.9836\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 10s 51ms/step - loss: 1.7060\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 12s 52ms/step - loss: 1.5490\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.4514\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 12s 54ms/step - loss: 1.3847\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 1.3311\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 13s 59ms/step - loss: 1.2884\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.2476\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.2086\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 13s 57ms/step - loss: 1.1702\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.1281\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.0852\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.0409\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.9920\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.9416\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.8894\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.8360\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.7844\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.7363\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3aAPJEqDZU3"
      },
      "source": [
        "\n",
        "### Praktikum Tugas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "PVRnDxN9BIk4"
      },
      "outputs": [],
      "source": [
        "class CustomTraining(MyModel):\n",
        "    @tf.function\n",
        "    def train_step(self, inputs):\n",
        "        # Memisahkan inputs menjadi data dan label\n",
        "        inputs, labels = inputs\n",
        "\n",
        "        # Menggunakan GradientTape untuk melacak operasi perhitungan gradien\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Mendapatkan prediksi dari model\n",
        "            predictions = self(inputs, training=True)\n",
        "\n",
        "            # Menghitung nilai kerugian (loss)\n",
        "            loss = self.loss(labels, predictions)\n",
        "\n",
        "        # Menghitung gradien terhadap parameter-model\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "        # Menggunakan optimizer untuk menerapkan gradien ke parameter-model\n",
        "        self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        # Mengembalikan nilai loss\n",
        "        return {'loss': loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "U7JXWIZpBIk-"
      },
      "outputs": [],
      "source": [
        "# Membuat instance dari kelas CustomTraining\n",
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()), # Menggunakan ukuran vokabular dari layer ids_from_chars\n",
        "    embedding_dim=embedding_dim,                      # Dimensi vektor embedding\n",
        "    rnn_units=rnn_units                               # Jumlah unit dalam lapisan GRU\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "2T1QPmXUBIlA"
      },
      "outputs": [],
      "source": [
        "# Mengkompilasi model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Fungsi loss Sparse Categorical Crossentropy\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fn-LzmqMBIlC",
        "outputId": "67fad7bc-1309-4c1e-d501-b61a401da2ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 14s 58ms/step - loss: 2.7399\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7807f31c67d0>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# Melatih model dengan dataset dengan satu epoch\n",
        "model.fit(dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjLY0PbqBIlD"
      },
      "source": [
        "Atau jika ingin lebih mengetahui dalamnya, kita bisa membuat custom training loop sendiri:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_NpBgKIBIlG",
        "outputId": "c7cb9d28-7f11-43e3-d457-5e3f14c75c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1697\n",
            "Epoch 1 Batch 50 Loss 2.1011\n",
            "Epoch 1 Batch 100 Loss 1.9801\n",
            "Epoch 1 Batch 150 Loss 1.8832\n",
            "\n",
            "Epoch 1 Loss: 1.9950\n",
            "Time taken for 1 epoch 13.67 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8333\n",
            "Epoch 2 Batch 50 Loss 1.7817\n",
            "Epoch 2 Batch 100 Loss 1.6886\n",
            "Epoch 2 Batch 150 Loss 1.6303\n",
            "\n",
            "Epoch 2 Loss: 1.7190\n",
            "Time taken for 1 epoch 20.48 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6056\n",
            "Epoch 3 Batch 50 Loss 1.5730\n",
            "Epoch 3 Batch 100 Loss 1.5864\n",
            "Epoch 3 Batch 150 Loss 1.5282\n",
            "\n",
            "Epoch 3 Loss: 1.5570\n",
            "Time taken for 1 epoch 10.87 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4771\n",
            "Epoch 4 Batch 50 Loss 1.4450\n",
            "Epoch 4 Batch 100 Loss 1.4089\n",
            "Epoch 4 Batch 150 Loss 1.4176\n",
            "\n",
            "Epoch 4 Loss: 1.4564\n",
            "Time taken for 1 epoch 10.89 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.4072\n",
            "Epoch 5 Batch 50 Loss 1.4119\n",
            "Epoch 5 Batch 100 Loss 1.4361\n",
            "Epoch 5 Batch 150 Loss 1.3535\n",
            "\n",
            "Epoch 5 Loss: 1.3876\n",
            "Time taken for 1 epoch 11.30 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3295\n",
            "Epoch 6 Batch 50 Loss 1.3434\n",
            "Epoch 6 Batch 100 Loss 1.3359\n",
            "Epoch 6 Batch 150 Loss 1.3476\n",
            "\n",
            "Epoch 6 Loss: 1.3340\n",
            "Time taken for 1 epoch 20.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.3101\n",
            "Epoch 7 Batch 50 Loss 1.2706\n",
            "Epoch 7 Batch 100 Loss 1.3059\n",
            "Epoch 7 Batch 150 Loss 1.2646\n",
            "\n",
            "Epoch 7 Loss: 1.2886\n",
            "Time taken for 1 epoch 10.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2187\n",
            "Epoch 8 Batch 50 Loss 1.2372\n",
            "Epoch 8 Batch 100 Loss 1.2710\n",
            "Epoch 8 Batch 150 Loss 1.2540\n",
            "\n",
            "Epoch 8 Loss: 1.2478\n",
            "Time taken for 1 epoch 11.17 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1554\n",
            "Epoch 9 Batch 50 Loss 1.2076\n",
            "Epoch 9 Batch 100 Loss 1.2278\n",
            "Epoch 9 Batch 150 Loss 1.2172\n",
            "\n",
            "Epoch 9 Loss: 1.2084\n",
            "Time taken for 1 epoch 20.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1018\n",
            "Epoch 10 Batch 50 Loss 1.1885\n",
            "Epoch 10 Batch 100 Loss 1.1669\n",
            "Epoch 10 Batch 150 Loss 1.2290\n",
            "\n",
            "Epoch 10 Loss: 1.1682\n",
            "Time taken for 1 epoch 11.21 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 11 Batch 0 Loss 1.1139\n",
            "Epoch 11 Batch 50 Loss 1.1219\n",
            "Epoch 11 Batch 100 Loss 1.1134\n",
            "Epoch 11 Batch 150 Loss 1.1168\n",
            "\n",
            "Epoch 11 Loss: 1.1283\n",
            "Time taken for 1 epoch 11.14 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 12 Batch 0 Loss 1.0330\n",
            "Epoch 12 Batch 50 Loss 1.0787\n",
            "Epoch 12 Batch 100 Loss 1.1096\n",
            "Epoch 12 Batch 150 Loss 1.1269\n",
            "\n",
            "Epoch 12 Loss: 1.0851\n",
            "Time taken for 1 epoch 11.06 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 13 Batch 0 Loss 1.0078\n",
            "Epoch 13 Batch 50 Loss 1.0149\n",
            "Epoch 13 Batch 100 Loss 1.0820\n",
            "Epoch 13 Batch 150 Loss 1.0481\n",
            "\n",
            "Epoch 13 Loss: 1.0389\n",
            "Time taken for 1 epoch 10.96 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 14 Batch 0 Loss 0.9600\n",
            "Epoch 14 Batch 50 Loss 0.9969\n",
            "Epoch 14 Batch 100 Loss 0.9992\n",
            "Epoch 14 Batch 150 Loss 1.0128\n",
            "\n",
            "Epoch 14 Loss: 0.9914\n",
            "Time taken for 1 epoch 12.13 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 15 Batch 0 Loss 0.8842\n",
            "Epoch 15 Batch 50 Loss 0.9071\n",
            "Epoch 15 Batch 100 Loss 0.9126\n",
            "Epoch 15 Batch 150 Loss 0.9841\n",
            "\n",
            "Epoch 15 Loss: 0.9406\n",
            "Time taken for 1 epoch 12.24 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 16 Batch 0 Loss 0.8682\n",
            "Epoch 16 Batch 50 Loss 0.8543\n",
            "Epoch 16 Batch 100 Loss 0.9074\n",
            "Epoch 16 Batch 150 Loss 0.9144\n",
            "\n",
            "Epoch 16 Loss: 0.8882\n",
            "Time taken for 1 epoch 11.95 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 17 Batch 0 Loss 0.7954\n",
            "Epoch 17 Batch 50 Loss 0.8054\n",
            "Epoch 17 Batch 100 Loss 0.8536\n",
            "Epoch 17 Batch 150 Loss 0.8965\n",
            "\n",
            "Epoch 17 Loss: 0.8350\n",
            "Time taken for 1 epoch 10.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 18 Batch 0 Loss 0.7211\n",
            "Epoch 18 Batch 50 Loss 0.7503\n",
            "Epoch 18 Batch 100 Loss 0.7914\n",
            "Epoch 18 Batch 150 Loss 0.8333\n",
            "\n",
            "Epoch 18 Loss: 0.7840\n",
            "Time taken for 1 epoch 10.99 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 19 Batch 0 Loss 0.6947\n",
            "Epoch 19 Batch 50 Loss 0.7122\n",
            "Epoch 19 Batch 100 Loss 0.7282\n",
            "Epoch 19 Batch 150 Loss 0.7593\n",
            "\n",
            "Epoch 19 Loss: 0.7337\n",
            "Time taken for 1 epoch 10.98 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 20 Batch 0 Loss 0.6393\n",
            "Epoch 20 Batch 50 Loss 0.6604\n",
            "Epoch 20 Batch 100 Loss 0.7163\n",
            "Epoch 20 Batch 150 Loss 0.7219\n",
            "\n",
            "Epoch 20 Loss: 0.6875\n",
            "Time taken for 1 epoch 11.10 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 20  # Jumlah epoch pelatihan\n",
        "\n",
        "mean = tf.metrics.Mean()  # Objek Mean untuk menghitung rata-rata kerugian\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()  # Waktu awal epoch\n",
        "\n",
        "    mean.reset_states()  # Mengatur ulang state objek Mean untuk epoch baru\n",
        "\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        # Melakukan satu langkah pelatihan (train step) pada model untuk setiap batch\n",
        "        logs = model.train_step([inp, target])\n",
        "\n",
        "        # Mengupdate state objek Mean dengan nilai kerugian dari batch tersebut\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        # Mencetak informasi setiap 50 batch\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # Setiap 5 epoch, simpan weights model\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\" * 80)\n",
        "\n",
        "# Simpan weights model setelah seluruh pelatihan selesai\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE1EaVu-Iutl"
      },
      "source": [
        "### Jalankan kode diatas dan sebutkan perbedaanya dengan praktikum 2?\n",
        "Perbedaan antara kode di atas dan praktikum 2 terletak pada tingkat kendali yang diberikan train model. Praktikum 2 menggunakan pendekatan \"teacher-forcing\", yang berarti selama training, model diberi masukan yang sebenarnya (ground truth) sebagai langkah berikutnya dalam urutan, bukan hasil prediksi model. Pendekatan ini digunakan untuk mencegah kesalahan akumulasi yang dapat terjadi ketika prediksi yang salah diberikan sebagai masukan berikutnya.\n",
        "\n",
        "Di sisi lain, kode di atas memberikan lebih banyak kendali dalam hal pelatihan. Model dilatih secara ekplisit menggunakan metode `train_step` yang memanfaatkan `tf.GradientTape` untuk melacak gradien dan menerapkan update pada model. Hal ini memungkinkan penggunaan pendekatan yang lebih fleksibel terhadap pelatihan, di mana setiap langkah pelatihan dapat diatur dengan lebih rinci. Selain itu, metode tersebut juga memungkinkan penyimpanan checkpoint model pada interval waktu tertentu."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}