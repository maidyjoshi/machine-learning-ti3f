{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6FQHkHeq8j7"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i2WNzdujIwLj"
      },
      "outputs": [],
      "source": [
        "# Import TensorFlow\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Dataset Shakespeare"
      ],
      "metadata": {
        "id": "6aVjhbABu8sJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj7ywO3AI6CI",
        "outputId": "f648ea18-fe1b-4c78-acbe-c10dde5a8299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "5PASJHlIvIgd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2U-itL2JQCu",
        "outputId": "e66f96fb-1f3d-49d7-bf96-eb27abfec059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8') # Buka file\n",
        "print(f'Length of text: {len(text)} characters') # cetak panjang teks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0jbxX7dJXJA",
        "outputId": "e689d631-303f-4906-b3e8-d79ce6812f14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntG3SO0AJZbl",
        "outputId": "bae6fb90-6fb5-407e-ac9c-ccb5c10df1e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text)) # buat kosakata\n",
        "print(f'{len(vocab)} unique characters') # cetak unique characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEBpcYxJq8kD"
      },
      "source": [
        "## Olah Teks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoQKs-kpJa_m",
        "outputId": "6de93721-8f1a-48e2-c8d7-23f4ef6566f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Menentukan beberapa teks contoh sebagai list\n",
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "# Membagi setiap teks menjadi karakter\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "\n",
        "# Menampilkan hasilnya\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "r0f1kQA8JnDS"
      },
      "outputs": [],
      "source": [
        "# untuk mengonversi karakter menjadi indeks numerik.\n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeubiVzZJwAe",
        "outputId": "5f1ffdc6-f365-4cc0-dc6a-1ff1899599b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Menggunakan layer StringLookup sebelumnya untuk mengonversi karakter Unicode menjadi indeks numerik\n",
        "# 'chars' adalah variabel yang berisi karakter Unicode\n",
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wTqPQsYZJ2fa"
      },
      "outputs": [],
      "source": [
        "# Membuat layer StringLookup baru untuk mengonversi indeks numerik kembali menjadi karakter Unicode\n",
        "# Menggunakan vocabulary dari ids_from_chars dan mengaktifkan invert=True\n",
        "# mask_token=None menunjukkan bahwa tidak ada token mask yang digunakan\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqhSjEADJ-Np",
        "outputId": "2f05989a-9268-4a54-a455-42bc21add2e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Lapisan ini mengconvert kembali karakter dari vektor ID, dan mengembalikannya sebagai karakter tf.RaggedTensor\n",
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UM-nVHEMKEzp",
        "outputId": "c0c17c46-344f-4875-8d20-1ab285ca7474"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Menggabungkan karakter Unicode dalam 'chars' menjadi string tunggal\n",
        "# axis=-1 menunjukkan bahwa penggabungan dilakukan sepanjang dimensi terakhir\n",
        "# .numpy() mengonversi hasil tensor TensorFlow menjadi nilai NumPy\n",
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "I0e5SeVVKRfo"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "    # Menggunakan layer chars_from_ids untuk mengonversi indeks numerik kembali menjadi karakter Unicode\n",
        "    # Menggabungkan karakter Unicode menjadi string tunggal\n",
        "    # axis=-1 menunjukkan bahwa penggabungan dilakukan sepanjang dimensi terakhir\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediksi"
      ],
      "metadata": {
        "id": "BbcNoRWp0Xb5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqTtRvgsKX0u",
        "outputId": "6e7fbfe2-2250-4a47-e16f-684780475509"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Menggunakan layer ids_from_chars untuk mengonversi teks menjadi indeks numerik\n",
        "# tf.strings.unicode_split(text, 'UTF-8') membagi teks menjadi karakter Unicode\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UE46kR_3Kn1I"
      },
      "outputs": [],
      "source": [
        "# Membuat objek tf.data.Dataset dari tensor all_ids\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JiqA6JtKric",
        "outputId": "31ff02bc-493a-4e3a-cfe0-d39cc8e6673d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "# Iterasi melalui 10 elemen pertama dari dataset ids_dataset\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
        "\n",
        "# Menetapkan panjang urutan (sequence length) menjadi 100\n",
        "seq_length = 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC1xJlO4KzDo",
        "outputId": "a9a41cc2-3cb6-45f9-b02c-6b27f8c328a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "# Metode batch memungkinkan Anda dengan mudah mengonversi karakter individual ini menjadi urutan ukuran yang diinginkan.\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFv7uYcSK3_C",
        "outputId": "a2481b59-a88c-4334-8057-2808302f88b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "# Iterasi melalui 5 elemen pertama dari dataset sequences\n",
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Ndbwkp0iK8vg"
      },
      "outputs": [],
      "source": [
        "# fungsi yang mengambil urutan sebagai masukan, menduplikasi, dan menggesernya untuk menyelaraskan masukan dan label untuk setiap langkah waktu\n",
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "split_input_target(list(\"Tensorflow\"))\n",
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKM4eWg6LOrg",
        "outputId": "a6b39ac9-7c2e-4292-d7d1-804e88fe8e4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "  print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OugjkBk7LWaB",
        "outputId": "b1e108b4-26e3-465c-ee42-8e447c999ee5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Membuat Batch Training\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51Kp-ssvq8kW"
      },
      "source": [
        "## Buat Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qyp0gNLjLb1f"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "b6stBk5gLhJj"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__(self)\n",
        "\n",
        "        # Lapisan embedding untuk mengonversi indeks numerik menjadi vektor embedding\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Lapisan GRU untuk memodelkan urutan data\n",
        "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True)\n",
        "\n",
        "        # Lapisan dense untuk output dengan ukuran sejumlah vocab_size\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = inputs\n",
        "\n",
        "        # Menggunakan lapisan embedding\n",
        "        x = self.embedding(x, training=training)\n",
        "\n",
        "        # Jika states tidak diberikan, menginisialisasi states dengan nilai default dari lapisan GRU\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "\n",
        "        # Memproses urutan data melalui lapisan GRU\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "\n",
        "        # Melalui lapisan dense untuk mendapatkan output\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        # Mengembalikan output dan states jika diperlukan\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XFpbnBNKLjXp"
      },
      "outputs": [],
      "source": [
        "# Membuat instance dari kelas MyModel\n",
        "model = MyModel(\n",
        "    vocab_size=vocab_size,       # Ukuran vokabular\n",
        "    embedding_dim=embedding_dim, # Dimensi vektor embedding\n",
        "    rnn_units=rnn_units           # Jumlah unit dalam lapisan GRU\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVPUaPiNq8kX"
      },
      "source": [
        "## Uji Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FgszJmWLlgz",
        "outputId": "e35ea066-bf79-4cf1-a799-47242f48e4f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "# Uji Model\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiBZywFFLpbI",
        "outputId": "97069275-272f-44af-d763-829149d7b408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "10WQVuj8LsLK"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coJkQkWFLunh",
        "outputId": "25f315fc-1b0b-4d82-a16f-3421d9de6a8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([23, 34, 59, 59, 23, 62, 61, 22, 46, 60, 33, 47, 29, 18, 36, 38, 20,\n",
              "       45, 35, 48, 10, 29, 62, 28, 10, 21, 64, 36, 59,  1, 53, 25, 48, 17,\n",
              "       53, 34,  9, 24, 56, 43, 12, 44, 54, 46, 32, 58, 53, 29, 23, 21, 36,\n",
              "       20, 13, 43,  4, 65, 43, 46, 45, 21, 64, 12, 63, 33, 51,  2, 22, 62,\n",
              "       55, 59, 44, 56, 60, 17, 29, 22, 22, 46, 16, 59, 40, 11,  2,  8, 30,\n",
              "        0,  2, 10,  3, 65, 57, 49, 10,  9,  4, 36,  4, 62,  4, 19])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdCKRaj1L0g-",
        "outputId": "a2dbc707-433e-4bfd-968a-09efe22c0031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'ank thee, gentle Percy; and be sure\\nI count myself in nothing else so happy\\nAs in a soul remembering'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'JUttJwvIguThPEWYGfVi3PwO3HyWt\\nnLiDnU.Kqd;eogSsnPJHWG?d$zdgfHy;xTl IwptequDPIIgCta: -Q[UNK] 3!zrj3.$W$w$F'\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHxtNiO4q8kZ"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7l-7R8rYL4rE"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgzZBzMjL8Vw",
        "outputId": "68980806-3fce-4d11-914d-5327c1ab1dd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1906443, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKlnNaMOL_C1",
        "outputId": "a8c6075a-bfda-44bd-d7ad-fa5eae285f02"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.065346"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "UivwcqLTMCHg"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "RXUu7UoSME81"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "377zZLuMMJwr",
        "outputId": "4e4f4c45-f30c-42a4-afeb-942931eec527"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 14s 55ms/step - loss: 2.7164\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 1.9800\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 13s 56ms/step - loss: 1.7027\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.5448\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.4481\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.3803\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 13s 58ms/step - loss: 1.3290\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.2852\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.2445\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 13s 58ms/step - loss: 1.2055\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.1665\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.1255\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.0823\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 13s 57ms/step - loss: 1.0380\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.9905\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.9392\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.8869\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.8355\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.7833\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.7345\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdulkELcq8kl"
      },
      "source": [
        "## Generate Teks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "cWCP7LzJMSkE"
      },
      "outputs": [],
      "source": [
        "# Berikut ini membuat prediksi satu langkah\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "mPgcmQwFMbT-"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YX4AtExMehc",
        "outputId": "9079af35-f437-43e2-958b-970104ee3fe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Nay, I fear thy mind:\n",
            "Let me enjurious time hath tomorrows to our free\n",
            "Spies of so success? Henceforth free op nothing\n",
            "They used with violent forest mother in a fight:\n",
            "Your mistress is my lover: but he has it.\n",
            "This your concluding face!\n",
            "Dream of the dang-to-night should lift every one;\n",
            "Wesher all virtuous and unsuee-foot, man, which are\n",
            "As cheap as-little abreast, could with his fingers,\n",
            "A thousand tongue to seek the base of time.\n",
            "Hath crump consul? name her.\n",
            "\n",
            "GRUMIO:\n",
            "That in a brazen ir fire vengeance blood,\n",
            "To wake it walking it to see.\n",
            "\n",
            "ISABELLA:\n",
            "\n",
            "us:\n",
            "And thou shalt to Richard: what say you, undess\n",
            "Of say she's upon the field: and you because\n",
            "I am abrable. Good faith, he will go along\n",
            "with thine eyes, flee from once again,\n",
            "And he, that now 'Commanding that hath done me well:\n",
            "Spore him some account! White-bear she breaks with said,\n",
            "Heaven lost his eyes and eyebrows.\n",
            "\n",
            "First Senator:\n",
            "Then, in angels like toild,\n",
            "Upon his name is as a brack,\n",
            "Account thee by the first.\n",
            "\n",
            "WARWICK:\n",
            "Darest t \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.5912258625030518\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAPi6svLMwbJ",
        "outputId": "757d56bd-ed7d-4a69-a96b-5ed77fead116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nHenceful account thy name of life,\\nI'll do't, some warrant for that place and time\\nThe passonce of the key?\\nThere spake me nothing: so much lengthen lengthens out\\nof the form: then have strong night, as to the sun,\\nThat doing thunders to me as we past the\\npacing liber than his ownsil high, so by crooking is the\\nwhite regain.\\n\\nSecond Lady:\\nSound King Riscline of this truer Warwick?\\nWe are not speak. Ah, what says God,\\n'Tis bear thy soul's deign till thus hagled of this sweet;\\nOr both passing well for thanks that doubt,\\nBoth one my injunion frounds his hagrand\\nbefore his word: see wot to what answer, you\\n\\nDUKE VINCENTIO:\\nMore crimined Lord pretty boldnes, something steel, that\\nsinging in the fight, time luirer power for in any\\nbluel trut; for once to your fault was thought,\\nTo smell out with a verge.\\n\\nFirst Servingman:\\nI know thee, father, in faith, I know how here you shall find\\nEnong up thus.'\\n\\nNurse:\\nO Prothee, thou! and what news?\\n\\nRICHARD:\\nIt:\\nGive me asural. Hold, for a while will\"\n",
            " b\"ROMEO:\\nLook to the will! why, swallowell! Great Englishmen question\\nThat thou respose the same of Rome to comfort him that\\nWhich have indeed their wagged make take runa,\\n'he is not am I with a groans blushing, indeed\\nThe sleeves with your storehouses;\\nOf my reasons flank him so.\\n\\nCATESBY:\\nMy lords! why dost thou ground?\\n\\nFLORIZEL:\\nMy tradple trial! but one word for her sorrow\\nsend him no other foes.\\n\\nARIEL:\\nMy lord?\\n\\nKING RICHARD III:\\nBut where lies out of? 'Tis such as you,\\nAnd he shall stanley like an incends.\\n\\nHORTENSIO:\\nCan there you love my business, and so beceal himself\\nAmong marquess one Paris; and Emilia\\nYet's within, this is well, and Satisfted him,\\nShort and her mask me in this place.\\n\\nGRUMIO:\\nHere's a good dulness. Come on, i' God's name.\\n\\nROMEO:\\nI varry me not to me, she housed outry, a weary way\\nUnworthy then.\\n\\nHASTINGS:\\nMy husband will mingle me well sooner grace for biggs;\\nAnd I am glad to rotuon'd for that.\\nRichard, Gellar, for thy father was as grief;\\nSo have we here marrie\"\n",
            " b\"ROMEO:\\nThe heavens guess will it weep.\\n\\nQUEEN ELIZABETH:\\nTo see a fleasure my instant blaze,\\nWhen she was glory, o'er all good extremesable.\\nNow would you not, so it for she's death, my tender Apathorica,\\nHath you for man or too fond from him.\\n\\nGLOUCESTER:\\nTell him they peess, which makes the bid hath been by what to crime\\nExecuted against your face; and thoughts,\\nWhich haply you last odds. By this angly bend thy\\ntale; ne'er lay been beauty with the bell.\\nAnd must waken with the blood and labour grow:\\nAnd yet she's dead, shalt, good entranct warm in\\nform.\\nBess'd o' Rome. Look to him, with whom, sir, I\\nwould I learn of me as deserves to double,\\nAnd yet not made our first worse than he.\\n\\nGONZALO:\\nLucy.\\n\\nDUKE VINCENTIO:\\nWhat is the king? why I drew to have?\\n\\nISABELLA:\\nAlas, alas!\\nWhy, love me, you love so;--that ne'er shake thyself!\\nWhen that it pleased, to the will! why, she's at last omit;\\nNo man is worse pulsed and match'd you: a protute\\nAnd pick not apploided at the trankful case.\\nThis day \"\n",
            " b\"ROMEO:\\nThis fair presence more, his body is remember.\\nAnd, madam! you'll not fear a husband withal\\nTo some more conceit, and, out.\\n\\nPage:\\nLoy a tormentable spring to their ancient,\\nNot like o'er the rather, then have punish'd us\\nHave windous trenches? If you'ld have stood for love to show\\nA damned slaves of my parenta's banishmen are\\nA most unlikely, and his own all,\\nIn my fault, how like my charity of honour\\nThat youth o'er the absence on his purse.\\n\\nGREMIO:\\nMadam, if you'll smeak, you\\nWhat raiment will but remember with my husband.\\nI hope I shall, sir, now we were warming,\\nIf e'er I live untignation to be\\nEven guess that we laude, Tranio,\\nTo credit his own as is, 'Gwardly in 'D:\\nArm, by-good fool, it is but a grie se:\\nOr, without crown, and house then would have safety has y\\narm'd fierys for that way; but, in feeding war, in justice\\nPlants as succom since I set on thy champion\\nTo vatien our gown taken sun.\\nAnd I beseech thee hand, I am thou grast's in him,\\nHis wisdom farewell known her dow\"\n",
            " b\"ROMEO:\\nShould be so twenty years or that sings\\nMust make his favour to the well, Aufidius,\\nIs pack with one of theirs. Thou hast given him something\\ndispleasure of a woman's pine, or beautify.\\n\\nSecond Citizen:\\nAy, madam, for every man them would\\nonce might be punish'd; but through with whop, I pray;\\nThe duke, that long but we have strong turned,\\nTo grant whisper dicterers are to light,\\nSome scone not to be woned, but that you have\\nThat she was to make you justice, but but remember'd him,\\nHis daughter'd isle, this foul plague in's thoughts,\\nSupplied his body to the fine is, Rulland; follow.\\n\\nMENENIUS:\\n\\nClown:\\nNot witness of tressad to be rough\\nWhen it swears, and stand will I'll nono you sound.\\nThis will I carry unto the Tower.\\n\\nANGELO:\\nHow haught ut, sir, you have been enough the Lady Grey!\\nMy horse will maintain it in this oath I air,\\nBut they accortance enough, for I have done,\\nTo save this ruin once again, disguised:\\nI mean, and I with two bears-blind\\nOne of thy name that kill him,\\nHath p\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.184813022613525\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNefko9tMzel",
        "outputId": "576ea865-86b4-4562-ac80-df0da0f6724d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7b65b069d8d0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Gramer, is King Henry leave?\n",
            "By my trorn with old praying, may show\n",
            "do purge Elsew silence in this \n"
          ]
        }
      ],
      "source": [
        "# Ekspor Model Generator\n",
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')\n",
        "\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}