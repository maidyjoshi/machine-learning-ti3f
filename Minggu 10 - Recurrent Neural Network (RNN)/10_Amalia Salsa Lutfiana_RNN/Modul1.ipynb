{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqvXB8MI1Yfd",
        "outputId": "616bfbc2-8477-49af-fafb-db0a00afe167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6w8LeZ1-1nM8",
        "outputId": "30c4994c-0300-45f8-eb56-57762604bb9e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (4.9.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.4.0)\n",
            "Requirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.1.8)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.23.5)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (3.20.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.31.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.3.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (4.66.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.14.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (6.1.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (4.5.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2023.7.22)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from promise->tensorflow-datasets) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.61.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Operasi Numerik\n",
        "import numpy as np\n",
        "# Melatih jaringan saraf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "# Nonaktifkan bar\n",
        "tfds.disable_progress_bar()"
      ],
      "metadata": {
        "id": "XA0Nrd0J1vpj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat grafik dan visualisasi\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Mendifiisikan fungsi plot_graph\n",
        "def plot_graphs(history, metric):\n",
        "    # Membuat garis untuk metrik pada setiap epoch pada data pelatihan\n",
        "    plt.plot(history.history[metric])\n",
        "    # Membuat garis untuk metrik pada setiap epoch\n",
        "    plt.plot(history.history['val_'+metric], '')\n",
        "    # Memberi label pada sumbu x\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    # Memberi label pada sumbu y\n",
        "    plt.ylabel(metric)\n",
        "    # Menyertakan legenda untuk membedakan antara data pelatihan dan data validasi\n",
        "    plt.legend([metric, 'val_'+metric])\n"
      ],
      "metadata": {
        "id": "Fbb9mFeg1z2D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup input pipeline**\n",
        "\n",
        "Dataset ulasan film IMDB adalah kumpulan data klasifikasi biner—semua ulasan memiliki sentimen positif atau negatif."
      ],
      "metadata": {
        "id": "8bl8ZNHS2P2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Memuat dataset IMDb\n",
        "dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
        "                          as_supervised=True)\n",
        "# Memisahkan dataset menjadi dua bagian\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "# Menampilkan spesifikasi elemen dari dataset pelatihan\n",
        "train_dataset.element_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF4S46QG2Xam",
        "outputId": "41243514-006b-40f5-e270-bb84eb622c7c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n",
            "Dataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awalnya ini mengembalikan dataset (teks, pasangan label):"
      ],
      "metadata": {
        "id": "OxOYB8pE24Yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengambil satu contoh dari dataset pelatihan\n",
        "for example, label in train_dataset.take(1):\n",
        "  #  Mencetak teks ulasan dari contoh yang diambil\n",
        "  print('text: ', example.numpy())\n",
        "  #  Mencetak label dari contoh yang diambil\n",
        "  print('label: ', label.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yRocEKg25Dl",
        "outputId": "ff528388-d1df-42cc-b0d0-47223a1d5e32"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:  b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
            "label:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berikutnya acak data untuk pelatihan dan membuat kumpulan pasangan (teks, label) ini:"
      ],
      "metadata": {
        "id": "2jxe9Z-z29Rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ukuran buffer yang digunakan saat melakukan pengacakan\n",
        "BUFFER_SIZE = 10000\n",
        "# Ukuran batch yang digunakan saat membentuk batch dari dataset\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Proses pelatihan\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Mengambil satu batch dari dataset pelatihan\n",
        "for example, label in train_dataset.take(1):\n",
        "  # Mencetak tiga contoh teks ulasan dari batch yang diambil\n",
        "  print('texts: ', example.numpy()[:3])\n",
        "  print()\n",
        "  # Mencetak tiga label yang sesuai dengan tiga contoh teks ulasan yang diambil\n",
        "  print('labels: ', label.numpy()[:3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85SPJHy3297P",
        "outputId": "ac7186bf-77fd-4d52-9f72-3b1ea80b944a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "texts:  [b\"This mindless movie is a piece of crap and boring like the full house repetitions. For all the people who want to see a great, exciting and cool horror movie shouldn't even think about watching this bunch of mindless work. a F- in my opinion. I have one question, what were they thinking? Let's make a list: 1) bad script 2) bad script 3) bad script 4) bad acting 5) bad directing and last but not least a bad script. I mean I am not like grumping about every movie, but I was disappointed when I watched it. This movie should be banned into a box, locked and sunk down into the sea. So please don't do something like this again, please, please, please!!!!\"\n",
            " b'Some people might consider this movie a piece of artwork - to be able to express your imagination on film in order to create a movie filled with antagonizing pain and death.. I personally think that this movie is a disgust, which should have never been released. This movie is repulsive, illogical and meaningless. Not only is it a complete waste of time but it makes you sick for days to come. The appalling images shown in the film not only make you grasp for air but they set in your mind and it takes days to forget them. Such a shame that people waste their imagination on such inhumane suffering.. \"Kill Bill\" would be another example but at least \"Kill Bill\" has its purpose, meaning, climax and resolution..'\n",
            " b'Madhur Bhandarkar directs this film that is supposed to expose the lifestyle of the rich and famous while also providing a commentary on the integrity of journalism today.<br /><br />Celebrities party endlessly, they like to be seen at these parties, and to get due exposure in the media. In fact the film would have us believe that this exposure MAKES celebrities out of socialites and the newspapers have a huge hand in this. IMO there is much more synergy between the celebrities and media and it is a \"I need you, you need me\" kind of relationship. However, the media needs celebrities more and not vice versa. Anyhow, in this milieu of constant partying is thrown the social column (page 3 of the newspaper) reporter Konkana Sen Sharma. She is shown as this celebrity maker, very popular at the social gatherings. She has a good friend in the gay Abhijeet and in the struggling model Rohit (Bikram Saluja). She rooms with an air-hostess \\xc2\\x96 the sassy Pearl (Sandhya Mridul), and a struggling actress - Gayatri (Tara Sharma). The editor of the newspaper is Boman Irani and a firebrand crime beat reporter is played by Atul Kulkarni. The movie has almost too many plot diversions and characters but does work at a certain level. The rich are shown to be rotten to the core for the most part, the movie biz shown to be sleazy to the max with casting couch scenarios, exploitation of power, hunger for media exposure. Into all this is layered in homosexuality, a homosexual encounter that seems to not have much to do with the story or plot, rampant drug use, pedophilia, police \"encounter\" deaths. In light of all this Pearl\\'s desire to have a super rich husband, a socialite daughter indulging in a sexual encounter in a car, the bitching women, all seem benign ills.<br /><br />The film has absolutely excellent acting by Konkana Sen Sharma, Atul Kulkarni has almost no role \\xc2\\x96 a pity in my opinion. But the supporting cast is more than competent (Boman Irani is very good). This is what saves the film for me. Mr. Bhandarkar bites off way more than he can chew or process onto celluloid and turns the film into a free for all bash. I wish he had focused on one or two aspects of societal ills and explored them more effectively. He berates societal exploitation yet himself exploits all the masala ingredients needed for a film to be successful. We have an item number in the framework of a Bollywood theme party, the drugged out kids dance a perfectly choreographed dance to a Western beat. I hope the next one from Madhur Bhandarkar dares to ditch even more of the Hindi film stereotyped ingredients. The film is a brave (albeit flawed) effort, certainly worth a watch.']\n",
            "\n",
            "labels:  [0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Buat Teks Encoder**"
      ],
      "metadata": {
        "id": "0_Z07L_03DWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teks mentah yang dimuat oleh tfds perlu diproses sebelum dapat digunakan dalam model. Cara termudah memproses teks untuk pelatihan adalah menggunakan lapisan TextVectorization. Lapisan ini memiliki banyak kemampuan, namun pada tutorial ini menggunakan perilaku default. Buat lapisan tersebut, dan teruskan teks kumpulan data ke metode .adapt lapisan:"
      ],
      "metadata": {
        "id": "P8ZtIOi_3F3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mendefinisikan ukuran vokabuler yang akan digunakan\n",
        "VOCAB_SIZE = 1000\n",
        "# Mengonversi teks ke dalam representasi numerik\n",
        "encoder = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE)\n",
        "# Memetakan fungsi pada setiap contoh dalam dataset\n",
        "encoder.adapt(train_dataset.map(lambda text, label: text))"
      ],
      "metadata": {
        "id": "_ttqjv-L3Hsc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metode .adapt mengatur kosakata lapisan. 20 token pertama dapat dilihat dengan kode berikut. Setelah padding dan token yang tidak diketahui, mereka diurutkan berdasarkan frekuensi:"
      ],
      "metadata": {
        "id": "7FmdqGf_3LGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengembalikan vokabuler dalam bentuk Python list\n",
        "vocab = np.array(encoder.get_vocabulary())\n",
        "#  Mengambil 20 kata pertama dari vokabuler\n",
        "vocab[:20]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3s5AJWWN3L7X",
        "outputId": "e22cc4a2-8f81-4821-9d1d-a8143624f29c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
              "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n",
              "      dtype='<U14')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setelah kosakata diatur, lapisan dapat mengkodekan teks ke dalam indeks. Tensor indeks diberi bantalan 0 ke urutan terpanjang dalam batch (kecuali jika Anda menetapkan output_sequence_length tetap):"
      ],
      "metadata": {
        "id": "G8IMgYQX3fK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengubah teks menjadi numerik\n",
        "encoded_example = encoder(example)[:3].numpy()\n",
        "encoded_example\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1ugzIYs3dsQ",
        "outputId": "976d9a94-df51-4cbe-8e67-6f008bd9c2d0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 11,   1,  18, ...,   0,   0,   0],\n",
              "       [ 47,  83, 227, ...,   0,   0,   0],\n",
              "       [  1,   1,   1, ...,   0,   0,   0]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dengan pengaturan default, prosesnya tidak dapat dibalik sepenuhnya. Ada dua alasan utama untuk itu:\n",
        "-Nilai default untuk argumen standarisasi preprocessing.TextVectorization adalah \"lower_and_strip_punctuation\".\n",
        "-Ukuran kosa kata yang terbatas dan kurangnya fallback berbasis karakter menghasilkan beberapa token yang tidak diketahui."
      ],
      "metadata": {
        "id": "ETkeTf6O3urm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mencetak informasi untuk 3 elemen pertama\n",
        "for n in range(3):\n",
        "  # Mencetak teks asli dari contoh pada indeks\n",
        "  print(\"Original: \", example[n].numpy())\n",
        "  # Mencetak teks yang telah diubah ke dalam representasi numerik\n",
        "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMAHsyBX30uf",
        "outputId": "02dfbecb-6801-4fa4-a4c5-fa19e6f1ff49"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  b\"This mindless movie is a piece of crap and boring like the full house repetitions. For all the people who want to see a great, exciting and cool horror movie shouldn't even think about watching this bunch of mindless work. a F- in my opinion. I have one question, what were they thinking? Let's make a list: 1) bad script 2) bad script 3) bad script 4) bad acting 5) bad directing and last but not least a bad script. I mean I am not like grumping about every movie, but I was disappointed when I watched it. This movie should be banned into a box, locked and sunk down into the sea. So please don't do something like this again, please, please, please!!!!\"\n",
            "Round-trip:  this [UNK] movie is a piece of crap and boring like the full house [UNK] for all the people who want to see a great [UNK] and cool horror movie [UNK] even think about watching this bunch of [UNK] work a [UNK] in my opinion i have one question what were they thinking lets make a [UNK] 1 bad script 2 bad script 3 bad script 4 bad acting 5 bad directing and last but not least a bad script i mean i am not like [UNK] about every movie but i was disappointed when i watched it this movie should be [UNK] into a [UNK] [UNK] and [UNK] down into the [UNK] so please dont do something like this again please please please                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
            "\n",
            "Original:  b'Some people might consider this movie a piece of artwork - to be able to express your imagination on film in order to create a movie filled with antagonizing pain and death.. I personally think that this movie is a disgust, which should have never been released. This movie is repulsive, illogical and meaningless. Not only is it a complete waste of time but it makes you sick for days to come. The appalling images shown in the film not only make you grasp for air but they set in your mind and it takes days to forget them. Such a shame that people waste their imagination on such inhumane suffering.. \"Kill Bill\" would be another example but at least \"Kill Bill\" has its purpose, meaning, climax and resolution..'\n",
            "Round-trip:  some people might [UNK] this movie a piece of [UNK] to be able to [UNK] your [UNK] on film in order to create a movie [UNK] with [UNK] [UNK] and death i [UNK] think that this movie is a [UNK] which should have never been released this movie is [UNK] [UNK] and [UNK] not only is it a complete waste of time but it makes you [UNK] for days to come the [UNK] [UNK] shown in the film not only make you [UNK] for air but they set in your mind and it takes days to forget them such a shame that people waste their [UNK] on such [UNK] [UNK] kill bill would be another example but at least kill bill has its [UNK] [UNK] [UNK] and [UNK]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
            "\n",
            "Original:  b'Madhur Bhandarkar directs this film that is supposed to expose the lifestyle of the rich and famous while also providing a commentary on the integrity of journalism today.<br /><br />Celebrities party endlessly, they like to be seen at these parties, and to get due exposure in the media. In fact the film would have us believe that this exposure MAKES celebrities out of socialites and the newspapers have a huge hand in this. IMO there is much more synergy between the celebrities and media and it is a \"I need you, you need me\" kind of relationship. However, the media needs celebrities more and not vice versa. Anyhow, in this milieu of constant partying is thrown the social column (page 3 of the newspaper) reporter Konkana Sen Sharma. She is shown as this celebrity maker, very popular at the social gatherings. She has a good friend in the gay Abhijeet and in the struggling model Rohit (Bikram Saluja). She rooms with an air-hostess \\xc2\\x96 the sassy Pearl (Sandhya Mridul), and a struggling actress - Gayatri (Tara Sharma). The editor of the newspaper is Boman Irani and a firebrand crime beat reporter is played by Atul Kulkarni. The movie has almost too many plot diversions and characters but does work at a certain level. The rich are shown to be rotten to the core for the most part, the movie biz shown to be sleazy to the max with casting couch scenarios, exploitation of power, hunger for media exposure. Into all this is layered in homosexuality, a homosexual encounter that seems to not have much to do with the story or plot, rampant drug use, pedophilia, police \"encounter\" deaths. In light of all this Pearl\\'s desire to have a super rich husband, a socialite daughter indulging in a sexual encounter in a car, the bitching women, all seem benign ills.<br /><br />The film has absolutely excellent acting by Konkana Sen Sharma, Atul Kulkarni has almost no role \\xc2\\x96 a pity in my opinion. But the supporting cast is more than competent (Boman Irani is very good). This is what saves the film for me. Mr. Bhandarkar bites off way more than he can chew or process onto celluloid and turns the film into a free for all bash. I wish he had focused on one or two aspects of societal ills and explored them more effectively. He berates societal exploitation yet himself exploits all the masala ingredients needed for a film to be successful. We have an item number in the framework of a Bollywood theme party, the drugged out kids dance a perfectly choreographed dance to a Western beat. I hope the next one from Madhur Bhandarkar dares to ditch even more of the Hindi film stereotyped ingredients. The film is a brave (albeit flawed) effort, certainly worth a watch.'\n",
            "Round-trip:  [UNK] [UNK] [UNK] this film that is supposed to [UNK] the [UNK] of the [UNK] and famous while also [UNK] a [UNK] on the [UNK] of [UNK] [UNK] br [UNK] [UNK] [UNK] they like to be seen at these [UNK] and to get due [UNK] in the [UNK] in fact the film would have us believe that this [UNK] makes [UNK] out of [UNK] and the [UNK] have a huge hand in this [UNK] there is much more [UNK] between the [UNK] and [UNK] and it is a i need you you need me kind of relationship however the [UNK] needs [UNK] more and not [UNK] [UNK] [UNK] in this [UNK] of [UNK] [UNK] is [UNK] the [UNK] [UNK] [UNK] 3 of the [UNK] [UNK] [UNK] [UNK] [UNK] she is shown as this [UNK] [UNK] very [UNK] at the [UNK] [UNK] she has a good friend in the [UNK] [UNK] and in the [UNK] [UNK] [UNK] [UNK] [UNK] she [UNK] with an [UNK]  the [UNK] [UNK] [UNK] [UNK] and a [UNK] actress [UNK] [UNK] [UNK] the [UNK] of the [UNK] is [UNK] [UNK] and a [UNK] crime [UNK] [UNK] is played by [UNK] [UNK] the movie has almost too many plot [UNK] and characters but does work at a certain level the [UNK] are shown to be [UNK] to the [UNK] for the most part the movie [UNK] shown to be [UNK] to the [UNK] with casting [UNK] [UNK] [UNK] of power [UNK] for [UNK] [UNK] into all this is [UNK] in [UNK] a [UNK] [UNK] that seems to not have much to do with the story or plot [UNK] [UNK] use [UNK] police [UNK] [UNK] in light of all this [UNK] [UNK] to have a [UNK] [UNK] husband a [UNK] daughter [UNK] in a sexual [UNK] in a car the [UNK] women all seem [UNK] [UNK] br the film has absolutely excellent acting by [UNK] [UNK] [UNK] [UNK] [UNK] has almost no role  a [UNK] in my opinion but the supporting cast is more than [UNK] [UNK] [UNK] is very good this is what [UNK] the film for me mr [UNK] [UNK] off way more than he can [UNK] or [UNK] [UNK] [UNK] and turns the film into a free for all [UNK] i wish he had [UNK] on one or two [UNK] of [UNK] [UNK] and [UNK] them more [UNK] he [UNK] [UNK] [UNK] yet himself [UNK] all the [UNK] [UNK] needed for a film to be [UNK] we have an [UNK] number in the [UNK] of a [UNK] theme [UNK] the [UNK] out kids dance a perfectly [UNK] dance to a [UNK] [UNK] i hope the next one from [UNK] [UNK] [UNK] to [UNK] even more of the [UNK] film [UNK] [UNK] the film is a [UNK] [UNK] [UNK] effort certainly worth a watch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Buat Model**"
      ],
      "metadata": {
        "id": "j_Qlcy4G4N7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat model sequential dengan lapisan-lapisan yang telah didefinisikan\n",
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    # Mengambil representasi numerik dari 'encoder' dan mengonversinya ke dalam vektor embedding\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=64,\n",
        "        mask_zero=True),\n",
        "    # Memproses urutan teks dalam kedua arah\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    # memproses fitur-fitur yang dihasilkan oleh lapisan LSTM sebelumnya\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    # Digunakan sebagai layer output\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "JpbeogQm4Qii"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mencetak apakah setiap lapisan dalam model mendukung masking atau tidak\n",
        "print([layer.supports_masking for layer in model.layers])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIfcfjwQ4W-i",
        "outputId": "7e34d4b6-9eae-4380-bc9a-785eb44510a2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False, True, True, True, True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Penjelasan**\n",
        "\n",
        "Lapisan pertama tidak mendukung masking, sedangkan pada 4 lapisan berikutnya mendukung"
      ],
      "metadata": {
        "id": "skh725UC4g93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mendefinisikan sebuah contoh teks sebagai string\n",
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "# Melakukan prediksi pada teks sample yang awalnya harus dikonversi dulu ke array numpy\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "# Mencetak hasil prediksi\n",
        "print(predictions[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVQz5IXW4qJ9",
        "outputId": "9dcedfb9-66b2-4c34-9dab-9ba881c5e6b7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 4s 4s/step\n",
            "[0.00036225]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat sebuah teks panjang yang diulang sebanyak 2000 kali\n",
        "padding = \"the \" * 2000\n",
        "#  Melakukan prediksi pada sample teks, dan padding (the 200 kali)\n",
        "predictions = model.predict(np.array([sample_text, padding]))\n",
        "# Mencetak prediksi sentimen\n",
        "print(predictions[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9RzE6cd4uWz",
        "outputId": "01635b0e-3300-40f2-c668-0fbbcc576502"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 363ms/step\n",
            "[0.00036225]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengonfigurasi model setelah model tersebut didefinisikan\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Z3Tgx8Fs4w5f"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Model**"
      ],
      "metadata": {
        "id": "QlISg7Bv4zPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#latih model\n",
        "#jumlah iterasi melatih model(epoch)=10\n",
        "#validation_data=test_dataset:data validasi yang digunakan untuk mengukur kinerja model selama pelatihan. Data ini berisi teks ulasan dan labelnya.\n",
        "#validation_steps=30:jumlah langkah validasi yang akan diambil selama setiap epoch\n",
        "history = model.fit(train_dataset, epochs=10, validation_data=test_dataset,\n",
        "validation_steps=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqtaWXMX41uU",
        "outputId": "771abbf7-1c52-4761-8c60-94c88fafd6ad"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 817s 2s/step - loss: 0.6479 - accuracy: 0.5698 - val_loss: 0.5116 - val_accuracy: 0.7599\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 763s 2s/step - loss: 0.4119 - accuracy: 0.8149 - val_loss: 0.3745 - val_accuracy: 0.8411\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 769s 2s/step - loss: 0.3420 - accuracy: 0.8493 - val_loss: 0.3515 - val_accuracy: 0.8542\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 704s 2s/step - loss: 0.3246 - accuracy: 0.8591 - val_loss: 0.3448 - val_accuracy: 0.8552\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 702s 2s/step - loss: 0.3153 - accuracy: 0.8636 - val_loss: 0.3260 - val_accuracy: 0.8521\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 750s 2s/step - loss: 0.3117 - accuracy: 0.8649 - val_loss: 0.3231 - val_accuracy: 0.8526\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 798s 2s/step - loss: 0.3070 - accuracy: 0.8682 - val_loss: 0.3451 - val_accuracy: 0.8599\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 742s 2s/step - loss: 0.3070 - accuracy: 0.8676 - val_loss: 0.3206 - val_accuracy: 0.8594\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 708s 2s/step - loss: 0.3017 - accuracy: 0.8714 - val_loss: 0.3217 - val_accuracy: 0.8552\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 723s 2s/step - loss: 0.2994 - accuracy: 0.8709 - val_loss: 0.3190 - val_accuracy: 0.8568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengevaluasi performa model pada dataset pengujian\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "# Mencetak nilai loss pada dataset pengujian\n",
        "print('Test Loss:', test_loss)\n",
        "# Mencetak akurasi pada dataset pengujian\n",
        "print('Test Accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxIijhY4456Q",
        "outputId": "4d659e71-5c49-4960-ccad-883291471a88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 31/391 [=>............................] - ETA: 2:42 - loss: 0.3200 - accuracy: 0.8558"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat figure untuk grafik dengan ukuran 16x8 inci\n",
        "plt.figure(figsize=(16, 8))\n",
        "# Membuat subplot pertama pada gambar\n",
        "plt.subplot(1, 2, 1)\n",
        "# Memanggil fungsi 'plot_graphs'\n",
        "plot_graphs(history, 'accuracy')\n",
        "# Menetapkan batas sumbu y\n",
        "plt.ylim(None, 1)\n",
        "# Membuat subplot kedua pada gambar\n",
        "plt.subplot(1, 2, 2)\n",
        "# Memanggil fungsi 'plot_graphs'\n",
        "plot_graphs(history, 'loss')\n",
        "# Menetapkan batas sumbu y\n",
        "plt.ylim(0, None)"
      ],
      "metadata": {
        "id": "e9lUyJwg4_Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Penjelasan**\n",
        "\n",
        "Nilai accuracy semakin naik seiring dengan beerjalannya epoch, sedangkan nilai loss / kerugian terus berkurang seiring berjalannya epoch\n",
        "\n",
        "Accuracy : akurasi model pada data pelatihan selama berjalannya epoch. Jika akurasi pelatihan terus meningkat seiring berjalannya epoch, itu menunjukkan bahwa model belajar dengan baik dari data pelatihan.\n",
        "\n",
        "Val Accuracy : akurasi model pada data validasi yang merupakan data yang tidak digunakan dalam pelatihan. Jika akurasi validasi naik bersamaan dengan akurasi pelatihan, itu menunjukkan bahwa model dapat menggeneralisasi dengan baik ke data yang belum pernah dilihat sebelumnya.\n",
        "\n",
        "Loss: Nilai kehilangan model pada data pelatihan seiring berjalannya epoch. Jika nilai loss pelatihan terus menurun seiring dengan peningkatan jumlah epoch, itu menunjukkan bahwa model terus mempelajari data pelatihan.\n",
        "\n",
        "Val Loss: Validation loss adalah nilai kehilangan model pada data validasi yang merupakan data yang tidak digunakan dalam pelatihan. Jika nilai validation loss juga menurun seiring dengan peningkatan epoch, itu menunjukkan bahwa model dapat melakukan generalisasi dengan baik pada data yang belum pernah dilihat sebelumnya."
      ],
      "metadata": {
        "id": "NP3-6Z85LIFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mendefinisikan kalimat baru sebagai string\n",
        "sample_text = ('The movie was cool. The animation and the graphics were out of this world. I would recommend this movie.')\n",
        "# Menggunakan model untuk melakukan prediksi\n",
        "predictions = model.predict(np.array([sample_text]))"
      ],
      "metadata": {
        "id": "ww7iB2s7LRXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Penjelasan**\n",
        "\n",
        "Hasil prediksi \"1/1\" menunjukkan bahwa model memprediksi teks tersebut sebagai sentimen positif dan prediksinya benar.\n"
      ],
      "metadata": {
        "id": "OQB01PNTLUQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stack two or more LSTM layers**"
      ],
      "metadata": {
        "id": "Tkuo0nqHLZNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat model sequential dengan lapisan yang telah didefinisikan\n",
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "     tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n",
        "     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "     tf.keras.layers.Dense(64, activation='relu'),\n",
        "     tf.keras.layers.Dropout(0.5),\n",
        "     tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "hC4VgZvZMAfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengonfigurasi model\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        " optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        " metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "cAxSSExTMCbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melatih model dengan iterasi epoch 10\n",
        "history = model.fit(train_dataset, epochs=10,\n",
        " validation_data=test_dataset,\n",
        " validation_steps=30)"
      ],
      "metadata": {
        "id": "ng93kmWmMFoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melakukan evaluasi akurasi dan loss pada model\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "YSqFVwzvMIJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memprediksi sentimen pada satu contoh teks\n",
        "sample_text = ('The movie was not good. The animation and the graphics were terrible. I would not recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "QN5JI__GMKY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Penjelasan**\n",
        "\n",
        "Penilaian model rendah terhadap teks"
      ],
      "metadata": {
        "id": "N0144UxqMOjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mencetak plot akurasi dan loss\n",
        "plt.figure(figsize=(16, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')"
      ],
      "metadata": {
        "id": "Ij5ub_9ZMT7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Penjelasan**\n",
        "\n",
        "Accuracy : akurasi model pada data pelatihan selama berjalannya epoch. Jika akurasi pelatihan terus meningkat seiring berjalannya epoch, itu menunjukkan bahwa model belajar dengan baik dari data pelatihan.\n",
        "\n",
        "Val Accuracy : akurasi model pada data validasi yang merupakan data yang tidak digunakan dalam pelatihan. Jika akurasi validasi naik bersamaan dengan akurasi pelatihan, itu menunjukkan bahwa model dapat menggeneralisasi dengan baik ke data yang belum pernah dilihat sebelumnya.\n",
        "\n",
        "Loss: Nilai kehilangan model pada data pelatihan seiring berjalannya epoch. Jika nilai loss pelatihan terus menurun seiring dengan peningkatan jumlah epoch, itu menunjukkan bahwa model terus mempelajari data pelatihan.\n",
        "\n",
        "Val Loss: Validation loss adalah nilai kehilangan model pada data validasi yang merupakan data yang tidak digunakan dalam pelatihan. Jika nilai validation loss juga menurun seiring dengan peningkatan epoch, itu menunjukkan bahwa model dapat melakukan generalisasi dengan baik pada data yang belum pernah dilihat sebelumnya."
      ],
      "metadata": {
        "id": "69aBSjtzMWVW"
      }
    }
  ]
}