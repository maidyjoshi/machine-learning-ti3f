{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i2WNzdujIwLj"
      },
      "outputs": [],
      "source": [
        "# Import TensorFlow\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj7ywO3AI6CI",
        "outputId": "ebdfde2a-3432-40ae-9dc9-815a5e8c1c29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 1s 1us/step\n"
          ]
        }
      ],
      "source": [
        "# Download Dataset Shakespeare\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2U-itL2JQCu",
        "outputId": "ee0de4fe-f889-47bc-cfe3-ed030618830c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# Load Data\n",
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0jbxX7dJXJA",
        "outputId": "5a8108e4-2215-407f-eaff-1b8040bdb361"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntG3SO0AJZbl",
        "outputId": "69c6b0bd-aa1f-4395-8609-b028070dfe2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Olah Teks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoQKs-kpJa_m",
        "outputId": "f2b7fd39-f729-4f53-e93d-7c8716135e28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Olah Teks\n",
        "# Vectorize Teks\n",
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "r0f1kQA8JnDS"
      },
      "outputs": [],
      "source": [
        "# sekarang buat tf.keras.layers.StringLookup layer:\n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeubiVzZJwAe",
        "outputId": "a8ce104a-c3ae-44af-ea6b-72981f58c666"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# perintah diatas mengconvert token menjadi id\n",
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wTqPQsYZJ2fa"
      },
      "outputs": [],
      "source": [
        "# kosakata asli yang dihasilkan dengan diurutkan(set(teks)) gunakan metode get_vocabulary()\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqhSjEADJ-Np",
        "outputId": "b10d8c9a-cf56-4be2-f4c7-747f603bb14c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Lapisan ini mengconvert kembali karakter dari vektor ID, dan mengembalikannya sebagai karakter tf.RaggedTensor\n",
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UM-nVHEMKEzp",
        "outputId": "f783193f-1e60-4270-8bce-2eb7382db52c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Anda dapat menggunakan tf.strings.reduce_join untuk menggabungkan kembali karakter menjadi string.\n",
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "I0e5SeVVKRfo"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqTtRvgsKX0u",
        "outputId": "b1d6d0fc-7638-4cbb-a173-9ad2a6e8001b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Prediksi\n",
        "# mengonversi vektor teks menjadi aliran indeks karakter.\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UE46kR_3Kn1I"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JiqA6JtKric",
        "outputId": "51eea0fc-55ac-4872-c42c-6dd226ce15f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
        "\n",
        "seq_length = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC1xJlO4KzDo",
        "outputId": "c0f95105-befa-4147-b8cc-b34c9b2d669a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "# Metode batch memungkinkan Anda dengan mudah mengonversi karakter individual ini menjadi urutan ukuran yang diinginkan.\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFv7uYcSK3_C",
        "outputId": "23fc8af1-8dce-4554-e8cc-698a0e15ce15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "# akan lebih mudah untuk melihat apa yang dilakukan jika Anda menggabungkan token kembali menjadi string:\n",
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Ndbwkp0iK8vg"
      },
      "outputs": [],
      "source": [
        "# fungsi yang mengambil urutan sebagai masukan, menduplikasi, dan menggesernya untuk menyelaraskan masukan dan label untuk setiap langkah waktu\n",
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "split_input_target(list(\"Tensorflow\"))\n",
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKM4eWg6LOrg",
        "outputId": "4289309e-2b2e-43de-9d7e-43519bc4561d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "  print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OugjkBk7LWaB",
        "outputId": "00d01feb-d369-45cc-b4b9-c7e2bb91b711"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Membuat Batch Training\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Buat Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qyp0gNLjLb1f"
      },
      "outputs": [],
      "source": [
        "# Buat Model\n",
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "b6stBk5gLhJj"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "XFpbnBNKLjXp"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Uji Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FgszJmWLlgz",
        "outputId": "cfb4218c-aa1f-40b4-e94d-1eaaebd99a08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "# Uji Model\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiBZywFFLpbI",
        "outputId": "6d8c645f-dd42-47f4-f7ca-1e90431b4c75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "10WQVuj8LsLK"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coJkQkWFLunh",
        "outputId": "3f92eed4-3ab8-4ff9-de93-8653782d18c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 2, 40, 18, 21, 52, 12, 47, 24, 49,  2, 19,  6, 23, 25, 13, 19,  6,\n",
              "       56, 39, 47,  2, 32, 46, 45, 25, 40, 49, 64, 52, 30, 50,  3, 53, 21,\n",
              "       22, 16, 13, 46, 63, 24, 51, 59, 10, 50, 61, 38, 44, 46, 19, 35, 57,\n",
              "        1,  5, 39, 60, 55, 61,  7,  5, 28,  9, 55, 30,  1, 39, 60, 52, 59,\n",
              "       42, 51, 32,  9, 52, 63, 14, 18, 20, 34, 16, 26, 36, 60, 24, 52, 64,\n",
              "       20,  0, 62,  3, 35, 21, 18,  5, 40, 62, 47, 38, 54, 18, 21])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdCKRaj1L0g-",
        "outputId": "b023b6ba-9900-45f3-bc8b-4b72df08fc2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b'll\\npatience; and, in roaring for a chamber-pot,\\ndismiss the controversy bleeding the more entangled\\n'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\" aEHm;hKj F'JL?F'qZh SgfLajymQk!nHIC?gxKlt3kvYegFVr\\n&Zupv,&O.pQ\\nZumtclS.mxAEGUCMWuKmyG[UNK]w!VHE&awhYoEH\"\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "7l-7R8rYL4rE"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgzZBzMjL8Vw",
        "outputId": "3c781a76-5632-4806-8323-55232d7d9fe3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.188329, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKlnNaMOL_C1",
        "outputId": "864ea669-e2a2-4218-bc4f-f6a9a77250ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "65.912575"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "UivwcqLTMCHg"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "RXUu7UoSME81"
      },
      "outputs": [],
      "source": [
        "# Konfigurasi Checkpoints\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "377zZLuMMJwr",
        "outputId": "58788bd7-ca3b-456a-cda9-618cd3a96a1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 21s 60ms/step - loss: 2.7301\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.9973\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.7088\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.5463\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.4471\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.3793\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.3263\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.2812\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.2402\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.2010\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.1595\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.1179\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.0741\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 13s 58ms/step - loss: 1.0283\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 13s 59ms/step - loss: 0.9789\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.9275\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.8747\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.8222\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.7716\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 13s 60ms/step - loss: 0.7239\n"
          ]
        }
      ],
      "source": [
        "# Lakukan Proses Training\n",
        "EPOCHS = 20\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Teks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "cWCP7LzJMSkE"
      },
      "outputs": [],
      "source": [
        "# Berikut ini membuat prediksi satu langkah\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "mPgcmQwFMbT-"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YX4AtExMehc",
        "outputId": "fb658731-4096-4cca-e587-a8877b7fa3bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "Away! let them live so much the present beast,\n",
            "The general wounds with right before.\n",
            "\n",
            "First Officer:\n",
            "come to, my lord: this is a common bock.\n",
            "\n",
            "BRUTUS:\n",
            "By the sad stories, Nor time to thee,\n",
            "In thy consent at your fortune's silk,\n",
            "that the yarrhest escape foul jewell thine death.\n",
            "This, wherein every seems that I, Richard, cannot.\n",
            "\n",
            "BRAKENBURY:\n",
            "With taught I give thee joint-blood in the good\n",
            "house-husband; I should be those for madians live\n",
            "He hath wither and change of smiles;\n",
            "Which you, might have endments not only Tybalt, with\n",
            "this bestrement.\n",
            "\n",
            "Proshecond George! Spare thou nexeff!\n",
            "The monage of a pine minutes, and leaves the king\n",
            "Is dear upon the dearer throne.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Who never give consent to bring it thee,\n",
            "Thou, and gentlemen, please you, ere I\n",
            "Carry sir. Pray you, tell me I shall\n",
            "I no and to my tears I walk upon this proding\n",
            "How he my wife and long from the equal firm\n",
            "O very name, post-horws amazedle.\n",
            "\n",
            "HaSTINGS:\n",
            "This yound mistim, these perjuries in the maid:\n",
            "The betal fully mu \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.3332161903381348\n"
          ]
        }
      ],
      "source": [
        "# Karena sedikitnya jumlah epoch pelatihan, model belum belajar membentuk kalimat runtut.\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAPi6svLMwbJ",
        "outputId": "9db9df78-5dc1-40ce-89e9-8335fbed49b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nThis shall mine in one Kate Henry of God with him;\\nAnd mine with blower for these two broken fence.\\n\\nQUEEN ELIZABETH:\\nIn that which says that Romeo hast cut out,\\nThe crown with lamentable thoughts from broke,\\nAnd trouble in the strace of head;\\nThe world is secret meat of it.\\nWhat, shall not be alone, for in a dead-malmer for that\\nwhich cannot call it bleeding them?\\n\\nHORTENSIO:\\nTurn this thing early to? Nor no man's land.\\n\\nHASTINGS:\\nMore pity, ye; your will, marrying a prison!\\nHow now! where were none but shoulders to the puils!\\nGood king Courage, manifors like consent is slain,\\nTitus Bolingbroke, to make a poison\\nYour laughs burks, pre-joy; which is made proudly end.\\n\\nDUKE OF YORK:\\nWhat bloody tribunes, better by this time to see this good Kate?\\n\\nBISHOP OF ELY:\\nTo-morrow, tell me, is speechle gods,\\nAnd may crack thee in still joy and heart; and myself\\nhave leave again in the furthest breast,\\nThat I should seem your faith to come by him.\\nI'll swear pilgry blaze of beauty did leave\\nHe s\"\n",
            " b\"ROMEO:\\nThis civil was, whether we do his wife.\\n\\nJOHN OF GAUNT:\\nO, fale your wissing fails that keep my thought offenders not;\\nNor never yet on heart of what is three or friends,\\nInferrethed come; for thou must here.\\nMark'd you, Sir Habbird, I am swift\\nControls it: and tutormillest be thy finger:\\nMake a shame will be a purge meeting to bring\\nTince that thy way with trust in again;\\nFor there it is, in love him that\\ndid this were jointness of his thing but streaking bowels.\\nMore light! for who, on my days hassips thize\\nAnd dress I palmet with childing.\\n\\nPETRUCHIO:\\nThou deeds'd not with her to tell the world will tribuly\\nBe thou with molabity. What's here?\\n\\nSecond Keeper:\\nYou bid me sickness to a measure, which is none of you\\nwith lamentable coward comes now will make it all.\\n\\nGRUMIO:\\nAnd row me thanks, my ladysh'd and allay-night\\nTill tell but revenged or throw as it as house as it\\nTo the sad story make more citie: vingerous a report\\nhere. Thy husband, being pity is his nature,\\nNow the foolishm\"\n",
            " b\"ROMEO:\\nWhy, there's none will be witness with thee?\\n\\nARIEL:\\nSweet nature, I'll but swear to prison.\\n\\nKING HENRY VI:\\nExecute, for what straight from the Tesher\\nBut draw their way.\\n\\nFirst Murderer:\\nTake this their weighty.\\n\\nHERMIONE:\\nSir, you see, sir, it is too crook'd for,\\nInchonitivery and for Clifford slew thy faith he's gone,\\nAnd she brings memon limp her doom:\\nWhich often, thy love, they will prove a widower;\\nSay, I promise it, I should belded his friends\\nThat that's the whick thou wast no salck.\\n\\nMISTRESS OVERDONE:\\nWhat's with a mistress citizel?\\n\\nISABELLA:\\nThere will take the warl thing still; I'll prove mine armory\\nWith any room in eagsh and entreat,\\nTo counterment may in the English crown in peace,\\nIn secret men condemned swiff abord?\\n\\nGLOUCESTER:\\nWhile we part keeps the burthen for her flowers of wounds,\\nThat hast forsworn they where they should add but round,\\nAnd stood to strike mine endest thing,\\nWill then let me in him.\\n\\nTYRREL:\\nThe eagle is dead, and thyself he stands in\\nthe pra\"\n",
            " b\"ROMEO:\\nStay, you talk, and this thy best march on earth.\\n\\nJULIET:\\nTut, I thank thee, and seat. O please my brother,\\nGood faithful vow, shall say to thee I may: beseech you all,\\nI'll make your bride to trust to them and croth in advanced,\\nAnd make bold statutes in the world, took you\\nI can; send him to cries 'Deny' nie death, I warrant\\nherein on me, by disinence to my blood,\\nOr I must hold thy person; sacres not of datter, Juliet,\\nWe think thee here, thy husband Anter 'pardon'd,\\nThat I am three-quarters furnished me?\\nWhat sayest thou, if the drops of woe so long\\nThat beauty talk'd of what? hadst thou\\nThat knows not that I am as this old grants.\\n\\nGentleman:\\nTo thinks I liquo it twice or sometime tired.\\nFarewell, thou qualker'd, trained substitutes.\\n\\nDUKE VINCENTIO:\\nRight cousin, I will frank it you shall never hear\\nA pond as well and liberty strong.\\nWith arms of none but dreams; and all thou hast\\nMust bey to-night; you that knows means to treas.\\nBut hath the open air, well for the raughter.\\n\\nK\"\n",
            " b\"ROMEO:\\nThe sea, ninet enough to break his father:\\nThe villain I shall not rat his fen\\nthe scale. How is it is, I will be king.\\n\\nWARWICK:\\nHow now! when? shall we drown?\\n\\nMISTRESS OVERDONE:\\nThus, what western silent! the most I give thee from\\nthee.\\n\\nCOMINIUS:\\nThou know'st in bower, when he that loves thee,\\nRaming, a pardon keep much intormone\\nFor gratings to your brother's black.\\n\\nANTONIO:\\nBelike, there seemeth green unknown;\\nFor montalling, and tellest thou, peace, Pompey, that\\nnews in the Tower and I did play the gods;\\nAnd I am made no more three out. O, he's a pase to plead\\nAnd proved the sacred heart gawntly plant to thee;\\nThat with false brood, not so; for I desire here be still\\nAnd leing people divine behind; menance.\\n\\nISABELLA:\\nI know not. Busyive, in quiet of his son. I pray;\\nMake popul that I bege; a lovel a man,\\nThat Plant thou theme's end.\\n\\nThird Servingman:\\nReady, neighbour.\\n\\nKATHARINA:\\nAnd I beside my liege, you are resign in readiness\\nmy shame; like the lions, and therefore fine!\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.165689706802368\n"
          ]
        }
      ],
      "source": [
        "# model menghasilkan 5 keluaran dalam waktu yang hampir sama dengan waktu yang dibutuhkan untuk menghasilkan 1 keluaran di atas.\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNefko9tMzel",
        "outputId": "ebe3f7f9-5c6a-4cd5-f81f-09f18bacf4eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x79bd3a9f3e20>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "Three pencelt is fishes far fortune meanly to whipped.\n",
            "\n",
            "LUCENTIO:\n",
            "Virtenes, and furron's earth, to \n"
          ]
        }
      ],
      "source": [
        "# Ekspor Model Generator\n",
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')\n",
        "\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
