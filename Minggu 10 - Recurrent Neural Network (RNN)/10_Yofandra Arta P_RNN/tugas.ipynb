{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Idxzs1Cu3HrI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "\n",
        "tfds.disable_progress_bar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "mRXD7s8f5ayr"
      },
      "outputs": [],
      "source": [
        "# Mendapatkan path file untuk dataset Shakespeare dari URL\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mO8R7UHs5cTR",
        "outputId": "e8d95584-67dd-4e50-cc74-5c83f8c7e55a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# membaca, lalu mendekode konten tersebut dari binary menjadi string menggunakan encoding ‘utf-8’\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# mencetak jumlah karakter dalam teks tersebut\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjQVPyQE5qze",
        "outputId": "c7315c97-b892-46ab-a2e3-979228c03dd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "# mencari jumlah karakter unik dalam teks\n",
        "vocab = sorted(set(text)) # mengubah teks menjadi set untuk menghilangkan duplikat\n",
        "print(f'{len(vocab)} unique characters') # mencetak jumlah karakter unik dalam teks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LCV7YiQ5snl",
        "outputId": "c2c2bd9a-b375-42de-c1bb-988fb2e3c8a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_texts = ['abcdefg', 'xyz'] # mendefinisikan list string yang akan diproses\n",
        "chars = tf.strings.unicode_split (example_texts, input_encoding='UTF-8') # membagi setiap string dalam example_texts menjadi list karakter\n",
        "chars # mencetak hasil dari operasi split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpemv3nQ5wB2",
        "outputId": "6bfa262c-bbcd-4a0d-a804-ca9ee2f47119"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# membuat instance dari layer StringLookup dengan vocabulari yang ditentukan oleh vocab\n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None) \n",
        "\n",
        "ids = ids_from_chars(chars) # mengubah chars menjadi indeks integer menggunakan ids_from_chars\n",
        "ids # mencetak array indeks integer yang mewakili chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMu69Qzr5xPu",
        "outputId": "7be28161-cbf2-4a0a-e815-67844673d2cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# membuat instance dari layer StringLookup dengan vocabulari yang ditentukan oleh ids_from_chars.get_vocabulary()\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "\n",
        "chars = chars_from_ids(ids) # mengubah ids kembali menjadi karakter menggunakan chars_from_ids\n",
        "chars # mencetak array karakter yang mewakili ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKz71j0w6IiN",
        "outputId": "76660cec-a6b5-464a-e65f-c7b4838165e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy() # menggabungkan elemen-elemen dalam array chars menjadi string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT_oR36Z6L6c",
        "outputId": "c1f621de-3349-4c91-a4bf-ba54b7f1d8b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Membuat fungsi mengubah list indeks integer kembali menjadi string\n",
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "\n",
        "# membagi teks menjadi list karakter dan hasilnya disimpan di variabel all_ids\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqpb01Xd6PKs",
        "outputId": "2c58ced4-bde5-4c70-ad4c-1e27f413511c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids) # membuat tf.data.Dataset dari all_ids\n",
        "\n",
        "# Menampilkan karakter-karakter pertama dalam 10 tensor ID\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yqf2TARS6RVF",
        "outputId": "1586c1a0-e3d5-4374-8993-f71aa637ee85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "# Menentukan panjang urutan yang diinginkan\n",
        "seq_length = 100\n",
        "\n",
        "# Membuat urutan dari dataset dengan panjang yang ditentukan\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# Menampilkan urutan karakter pertama dalam satu batch\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPhfIvw-6rO9",
        "outputId": "f66c7001-eedd-4685-f11c-46db1fbf862f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "# Menampilkan teks dari lima urutan pertama\n",
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iesXK5R6twn",
        "outputId": "6f001bc5-a6b0-4ccb-a94d-efdcfb07d908"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "# Mendefinisikan fungsi untuk membagi input dan target dari suatu urutan\n",
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "split_input_target(list(\"Tensorflow\")) # Memanggil fungsi pada contoh urutan\n",
        "\n",
        "dataset = sequences.map(split_input_target) # Membuat dataset dari urutan dengan input dan target yang terpisah\n",
        "\n",
        "# Menampilkan contoh input dan target dari dataset\n",
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "  print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iya471SW6yI_",
        "outputId": "fcc6233a-2a2d-49f2-f3b0-e06ef717ff7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ukuran batch\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Ukuran buffer untuk mengacak dataset\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "# Membuat dataset dengan mengacak, mengelompokkan, dan memuat dengan prefetch\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset # Menampilkan dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "N1MbM24k606a"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "# Mendefinisikan kelas model yang merupakan turunan dari tf.keras.Model\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "# Membuat objek model menggunakan kelas yang telah didefinisikan\n",
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFRdzZxdYeek",
        "outputId": "7d8a85f8-6588-4094-fe9b-566b6a083d3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     multiple                  16896     \n",
            "                                                                 \n",
            " gru_2 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_2 (Dense)             multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Melakukan prediksi pada satu batch contoh input dari dataset\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "model.summary() # Menampilkan ringkasan arsitektur model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF9PeQZrYhL7",
        "outputId": "19c4af56-0cbd-488d-ceaf-993db7ff9e37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b\"ke.\\n\\nCLARENCE:\\nWe know thy charge, Brakenbury, and will obey.\\n\\nGLOUCESTER:\\nWe are the queen's abject\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b'be3nfkdjiU,zAEwzMbTbe:d&$fMerWKj?jFB [UNK]kYfK.jJ\\ni$R!wRaQG-\\nPFp!f,AcCOoh[UNK]afGixrPVCnQzFOZCwqI,e&Fo.nLcI[UNK]'\n"
          ]
        }
      ],
      "source": [
        "# Menghasilkan indeks teracak dari distribusi prediksi\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "\n",
        "sampled_indices\n",
        "\n",
        "# Menampilkan contoh input dan prediksi karakter berikutnya\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MVBmeeGYl1j",
        "outputId": "5a16c05f-d22f-4ee4-b74a-bff66114751e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1893554, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# Menggunakan SparseCategoricalCrossentropy sebagai fungsi loss\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Menghitung rata-rata loss pada contoh batch\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WRl-1-oYnhr",
        "outputId": "fbed6865-40ef-41bf-ebbb-b197b7978b01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "65.98024"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Menghitung eksp dari rata-rata loss\n",
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "SPLiV-9GYq0B"
      },
      "outputs": [],
      "source": [
        "# mengkompilasi model dengan menentukan optimizer dan fungsi loss\n",
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hAwNu5jYtHw",
        "outputId": "4046e0ad-8d3c-4066-813f-b0c9414d2017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 16s 66ms/step - loss: 2.7278\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 13s 64ms/step - loss: 1.9931\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.7117\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.5511\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.4517\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.3832\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.3308\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.2860\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 13s 64ms/step - loss: 1.2444\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 16s 62ms/step - loss: 1.2051\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 1.1650\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 14s 62ms/step - loss: 1.1237\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 14s 61ms/step - loss: 1.0799\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.0333\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.9856\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.9343\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 0.8808\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.8295\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.7779\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 0.7286\n"
          ]
        }
      ],
      "source": [
        "# mendefinisikan direktori di mana checkpoint akan disimpan\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "# mendefinisikan prefix untuk nama file checkpoint\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "# membuat callback ModelCheckpoint yang akan menyimpan bobot model ke file checkpoint setelah setiap epoch\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "EPOCHS = 20 # mendefinisikan jumlah epoch\n",
        "\n",
        "# melatih model dengan dataset yang ditentukan selama jumlah epoch yang ditentukan, dan menyimpan bobot model setelah setiap epoch\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "zLcjMFus62lJ"
      },
      "outputs": [],
      "source": [
        "# mendefinisikan kelas CustomTraining yang mewarisi dari MyModel\n",
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs): # mendefinisikan metode train_step yang akan dipanggil pada setiap langkah pelatihan\n",
        "      inputs, labels = inputs # memisahkan data input dan label\n",
        "      with tf.GradientTape() as tape: # membuat instance dari tf.GradientTape\n",
        "          predictions = self(inputs, training=True) # melakukan forward pass melalui model (menggunakan metode call dari MyModel) untuk menghasilkan prediksi dari data input\n",
        "          loss = self.loss(labels, predictions) # menghitung loss antara label dan prediksi menggunakan fungsi loss dari model\n",
        "      grads = tape.gradient(loss, model.trainable_variables) # menghitung gradien loss terhadap semua variabel yang dapat dilatih dalam model\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables)) # menerapkan gradien ke variabel yang dapat dilatih menggunakan optimizer dari model\n",
        "\n",
        "      return {'loss': loss} # mengembalikan dictionary yang berisi loss yang dihitung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKjFYd0u7I7Y",
        "outputId": "9bc06e93-3da9-4a1d-f36f-6bda801c71ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "172/172 [==============================] - 16s 62ms/step - loss: 2.7185\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c86c9ecf3d0>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# membuat instance dari kelas CustomTraining dengan parameter yang ditentukan   \n",
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "\n",
        "# mengkompilasi model dengan optimizer Adam dan fungsi loss Sparse Categorical Crossentropy\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "# melatih model dengan dataset yang ditentukan selama 1 epoch\n",
        "model.fit(dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3US81wl7Sff",
        "outputId": "f1792eda-1bef-46cf-cac2-9293afb8f151"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.2056\n",
            "Epoch 1 Batch 50 Loss 2.0146\n",
            "Epoch 1 Batch 100 Loss 1.9134\n",
            "Epoch 1 Batch 150 Loss 1.8436\n",
            "\n",
            "Epoch 1 Loss: 1.9795\n",
            "Time taken for 1 epoch 12.56 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.7835\n",
            "Epoch 2 Batch 50 Loss 1.7563\n",
            "Epoch 2 Batch 100 Loss 1.6492\n",
            "Epoch 2 Batch 150 Loss 1.6222\n",
            "\n",
            "Epoch 2 Loss: 1.7049\n",
            "Time taken for 1 epoch 12.54 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6266\n",
            "Epoch 3 Batch 50 Loss 1.5986\n",
            "Epoch 3 Batch 100 Loss 1.5129\n",
            "Epoch 3 Batch 150 Loss 1.4986\n",
            "\n",
            "Epoch 3 Loss: 1.5456\n",
            "Time taken for 1 epoch 12.17 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4749\n",
            "Epoch 4 Batch 50 Loss 1.4303\n",
            "Epoch 4 Batch 100 Loss 1.4335\n",
            "Epoch 4 Batch 150 Loss 1.4588\n",
            "\n",
            "Epoch 4 Loss: 1.4490\n",
            "Time taken for 1 epoch 12.81 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.4170\n",
            "Epoch 5 Batch 50 Loss 1.3444\n",
            "Epoch 5 Batch 100 Loss 1.3691\n",
            "Epoch 5 Batch 150 Loss 1.3958\n",
            "\n",
            "Epoch 5 Loss: 1.3807\n",
            "Time taken for 1 epoch 13.17 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3307\n",
            "Epoch 6 Batch 50 Loss 1.3216\n",
            "Epoch 6 Batch 100 Loss 1.2504\n",
            "Epoch 6 Batch 150 Loss 1.2940\n",
            "\n",
            "Epoch 6 Loss: 1.3281\n",
            "Time taken for 1 epoch 13.51 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2762\n",
            "Epoch 7 Batch 50 Loss 1.2596\n",
            "Epoch 7 Batch 100 Loss 1.2725\n",
            "Epoch 7 Batch 150 Loss 1.2720\n",
            "\n",
            "Epoch 7 Loss: 1.2845\n",
            "Time taken for 1 epoch 13.43 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2358\n",
            "Epoch 8 Batch 50 Loss 1.2386\n",
            "Epoch 8 Batch 100 Loss 1.2290\n",
            "Epoch 8 Batch 150 Loss 1.2426\n",
            "\n",
            "Epoch 8 Loss: 1.2427\n",
            "Time taken for 1 epoch 12.84 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.2025\n",
            "Epoch 9 Batch 50 Loss 1.1674\n",
            "Epoch 9 Batch 100 Loss 1.1945\n",
            "Epoch 9 Batch 150 Loss 1.2310\n",
            "\n",
            "Epoch 9 Loss: 1.2037\n",
            "Time taken for 1 epoch 12.03 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1556\n",
            "Epoch 10 Batch 50 Loss 1.1669\n",
            "Epoch 10 Batch 100 Loss 1.1893\n",
            "Epoch 10 Batch 150 Loss 1.1982\n",
            "\n",
            "Epoch 10 Loss: 1.1633\n",
            "Time taken for 1 epoch 12.19 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10 # mendefinisikan jumlah epoch\n",
        "\n",
        "mean = tf.metrics.Mean() # menghitung rata-rata loss selama setiap epoch\n",
        "\n",
        "# Melakukan pelatihan selama beberapa epoch\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states() # Mereset nilai metrik Mean untuk setiap epoch\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):# Iterasi melalui setiap batch dalam dataset\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # Menyimpan model setiap 5 epoch\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "# Menyimpan model setelah pelatihan selesai\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
