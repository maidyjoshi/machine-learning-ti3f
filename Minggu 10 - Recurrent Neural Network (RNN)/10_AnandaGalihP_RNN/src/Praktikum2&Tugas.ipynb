{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "y45myRkN-lht"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf  # Mengimpor modul TensorFlow untuk digunakan dalam kode.\n",
        "import numpy as np  # Mengimpor modul NumPy dengan alias np untuk digunakan dalam kode.\n",
        "import os  # Mengimpor modul os untuk berinteraksi dengan sistem operasi seperti path, direktori, dan lainnya.\n",
        "import time  # Mengimpor modul time untuk bekerja dengan waktu dalam kode.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "# Mendapatkan file 'shakespeare.txt' dari URL yang disediakan menggunakan fungsi get_file dari modul keras.utils di TensorFlow."
      ],
      "metadata": {
        "id": "FuqMez53-0Jl"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# Membaca file 'shakespeare.txt' dengan mode 'rb' (read binary) kemudian melakukan decoding teks menggunakan utf-8.\n",
        "\n",
        "print(f'Length of text: {len(text)} characters')\n",
        "# Menampilkan panjang teks dalam karakter dari file yang telah dibaca."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbAmby1T-68F",
        "outputId": "95c57569-0a82-4e14-80a5-6c7e3fcd9fa8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:250])\n",
        "# Menampilkan 250 karakter pertama dari teks yang telah dibaca dari file 'shakespeare.txt'."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG4nqx2kBv0q",
        "outputId": "af7c7ba2-d8b5-4f75-8790-f758a2e4fa9c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "# Membuat himpunan karakter unik (vocab) dari teks yang telah dibaca, lalu diurutkan.\n",
        "\n",
        "print(f'{len(vocab)} unique characters')\n",
        "# Menampilkan jumlah karakter unik dalam teks."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6zvO2KWBzaR",
        "outputId": "85544f13-1ece-4596-c563-46f51463289d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "# Mendefinisikan beberapa teks contoh dalam sebuah list.\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "# Membagi teks-teks contoh menjadi karakter-karakter menggunakan fungsi unicode_split dari TensorFlow,\n",
        "# dengan encoding input yang ditentukan sebagai UTF-8."
      ],
      "metadata": {
        "id": "zJqw99odB4mI"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)\n",
        "# Membuat layer StringLookup dari TensorFlow untuk mengonversi karakter ke ID menggunakan daftar karakter unik (vocab)\n",
        "# tanpa menggunakan mask_token."
      ],
      "metadata": {
        "id": "jzIlvAP6B8JJ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "# Menggunakan layer StringLookup yang telah dibuat sebelumnya untuk mengonversi karakter-karakter menjadi ID.\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGX22kiHB_mg",
        "outputId": "cf79caec-0f3c-4f55-8fdd-b06f8293a22c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "# Membuat layer StringLookup baru untuk mengonversi ID kembali ke karakter dengan menggunakan vocabulary\n",
        "# yang sama dengan yang digunakan sebelumnya, dengan parameter invert=True."
      ],
      "metadata": {
        "id": "QFeIPtJUCIrS"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "# Menggunakan layer StringLookup yang telah dibuat sebelumnya untuk mengonversi ID kembali menjadi karakter.\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtKQcUccCJ4h",
        "outputId": "03e9999b-8458-4d45-bdef-537dcfbb12d7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()\n",
        "# Menggunakan fungsi reduce_join dari TensorFlow untuk menggabungkan karakter-karakter menjadi string pada axis -1 (axis terakhir) dan mengonversi hasilnya menjadi array NumPy."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLPIfGM2CNJN",
        "outputId": "c5e95bc3-f2b1-4e3f-ee28-804b2406c692"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "# Membuat fungsi text_from_ids yang menggunakan chars_from_ids untuk mengonversi IDs kembali menjadi teks."
      ],
      "metadata": {
        "id": "A8HH66jYCT83"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "# Menggunakan ids_from_chars untuk mengonversi seluruh teks menjadi IDs dengan membagi teks menjadi karakter menggunakan UTF-8.\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYFm1UoPC0WN",
        "outputId": "b3a68dac-9f96-4be4-c218-928100125625"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "# Membuat objek Dataset dari tensor all_ids menggunakan from_tensor_slices dari TensorFlow."
      ],
      "metadata": {
        "id": "hVVKrnf6CuOL"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    # Mengambil 10 elemen pertama dari ids_dataset dan mengiterasinya.\n",
        "\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
        "    # Mengonversi IDs kembali menjadi karakter menggunakan chars_from_ids,\n",
        "    # lalu didecode menjadi string dengan utf-8 dan mencetak hasilnya."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RNpRW6KC5la",
        "outputId": "fbf254b1-b53f-45f3-f6e8-9b397bc7ae4a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "# Menetapkan panjang urutan (sequence length) sebesar 100."
      ],
      "metadata": {
        "id": "UZN78wsjC-L4"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "# Membagi dataset ids menjadi urutan-urutan dengan panjang seq_length+1 dan mengabaikan bagian sisa.\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "    print(chars_from_ids(seq))\n",
        "# Mengambil 1 urutan pertama dari dataset sequences dan mengonversi IDs kembali menjadi karakter."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaDh8_P0DDi5",
        "outputId": "863b897e-c38b-4faf-d037-a33cfe04e7ee"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())\n",
        "# Mengambil 5 urutan pertama dari dataset sequences, mengonversi IDs kembali menjadi teks, dan mencetak hasilnya dalam bentuk array NumPy."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzVOfmDsDKbw",
        "outputId": "56d54129-f691-47b2-dbc3-0a600a74b5fd"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "# Membuat fungsi split_input_target untuk membagi urutan menjadi input dan target dengan menggeser satu karakter."
      ],
      "metadata": {
        "id": "jhsG3QbfDOvE"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))\n",
        "# Menggunakan fungsi split_input_target untuk membagi urutan karakter \"Tensorflow\" menjadi input dan target."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSBuMVZhDVeP",
        "outputId": "6214e152-cddc-4084-fd77-68f8f2ccae39"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)\n",
        "# Menggunakan fungsi map untuk menerapkan fungsi split_input_target ke setiap elemen dalam dataset sequences."
      ],
      "metadata": {
        "id": "AHq-gfbGDahK"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())\n",
        "# Mengambil 1 contoh input dan target dari dataset yang telah dimodifikasi, mengonversi mereka kembali menjadi teks, dan mencetaknya."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQtZAzYYEUzu",
        "outputId": "95003c60-2fd1-4956-bfe1-7e9d8543f461"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "# Menetapkan ukuran batch sebesar 64.\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "# Menetapkan ukuran buffer untuk melakukan pengacakan dataset.\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "# Melakukan pengacakan, pembagian menjadi batch, dan prefetching untuk meningkatkan performa dataset.\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhGAL1nwFYPZ",
        "outputId": "e62a2394-e76c-43aa-9491-95e433998d54"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "# Mendapatkan panjang vocabulary yang digunakan dalam layer StringLookup.\n",
        "\n",
        "embedding_dim = 256\n",
        "# Menetapkan dimensi embedding sebesar 256.\n",
        "\n",
        "rnn_units = 1024\n",
        "# Menetapkan jumlah unit dalam RNN sebesar 1024."
      ],
      "metadata": {
        "id": "0qImegPbFdUF"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__(self)\n",
        "        # Mendefinisikan model dengan layer-layer yang dibutuhkan untuk membangun arsitektur model.\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        # Layer embedding untuk mengubah ID-token menjadi representasi vektor.\n",
        "\n",
        "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True)\n",
        "        # Layer GRU (Gated Recurrent Unit) sebagai bagian dari model untuk menangani urutan data dengan kemampuan mengembalikan urutan dan state.\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "        # Layer Dense untuk melakukan output prediksi pada setiap timestamp.\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = inputs\n",
        "        x = self.embedding(x, training=training)\n",
        "        # Memasukkan input ke dalam layer embedding untuk mendapatkan representasi vektor.\n",
        "\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        # Menginisialisasi states jika tidak ada states yang diberikan sebagai input.\n",
        "\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        # Memasukkan input ke dalam layer GRU untuk memperoleh output dan states.\n",
        "\n",
        "        x = self.dense(x, training=training)\n",
        "        # Memasukkan output dari GRU ke dalam layer Dense untuk mendapatkan hasil prediksi.\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x\n",
        "        # Mengembalikan hasil prediksi atau hasil dan states tergantung pada nilai return_state."
      ],
      "metadata": {
        "id": "0lMbevtcFevS"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "# Membuat objek model menggunakan class MyModel yang telah dibuat sebelumnya dengan parameter yang telah ditentukan sebelumnya."
      ],
      "metadata": {
        "id": "TEW0fRtTFwfT"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uji model\n",
        "\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    # Memperoleh prediksi dari contoh batch input menggunakan model.\n",
        "\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "    # Menampilkan bentuk (shape) dari prediksi yang dihasilkan oleh model.\n",
        "    # (batch_size, sequence_length, vocab_size) menunjukkan bahwa model mengeluarkan prediksi untuk setiap urutan pada setiap batch dengan dimensi vocab_size.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcmEopAIGM85",
        "outputId": "b7e1c347-0107-43ec-9429-efed80ec52ca"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "# Menampilkan ringkasan (summary) dari arsitektur model yang telah dibuat, menunjukkan informasi tentang layer dan jumlah parameter yang dapat disesuaikan."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-xwztX7GRYD",
        "outputId": "2bbb5d3e-c29d-4538-aac4-cb00dcb88059"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     multiple                  16896     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "# Menggunakan fungsi tf.random.categorical untuk mengambil sampel indeks dari distribusi probabilitas dalam example_batch_predictions.\n",
        "\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "# Menghilangkan dimensi yang tidak perlu dengan menggunakan tf.squeeze, lalu mengonversi hasilnya menjadi array NumPy."
      ],
      "metadata": {
        "id": "xCnh3a4uGVQS"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "al8jqQx5Gfwa",
        "outputId": "8f84b895-9792-4993-b139-e10aa5c93c62"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([17,  5, 17, 54, 15, 21, 45, 29, 28, 36, 63, 33, 27, 27, 47, 39,  9,\n",
              "       46, 58, 21, 43, 55, 62, 38, 33,  3, 63, 53, 40, 61, 43, 45, 27, 55,\n",
              "       39, 23, 19, 57, 11, 27, 54, 10, 29,  0, 52, 13, 44, 41, 54, 64, 18,\n",
              "       63, 27, 50, 65, 14, 55,  8, 37, 56, 21, 15, 34, 15, 18, 59, 25, 41,\n",
              "       62, 26, 34, 28,  8, 65, 52, 24, 40, 35, 36, 25, 59, 47, 22, 19, 41,\n",
              "       34, 51, 62, 61,  3, 27, 31, 13, 37,  2, 53, 12, 30, 34, 60])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "# Mencetak contoh input dari batch pertama sebagai teks.\n",
        "\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())\n",
        "# Mencetak prediksi karakter berikutnya berdasarkan sampel indeks yang diambil dari model sebagai teks."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ-wym25GgOf",
        "outputId": "5a325fb2-35e8-4214-826b-d9d4c00a011b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'd you that do abet him in this kind\\nCherish rebellion and are rebels all.\\n\\nNORTHUMBERLAND:\\nThe noble'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'D&DoBHfPOWxTNNhZ.gsHdpwYT!xnavdfNpZJFr:No3P[UNK]m?eboyExNkzAp-XqHBUBEtLbwMUO-zmKaVWLthIFbUlwv!NR?X n;QUu'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train model\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# Mendefinisikan fungsi loss menggunakan SparseCategoricalCrossentropy untuk menghitung nilai loss pada model yang mengeluarkan logits.\n"
      ],
      "metadata": {
        "id": "VgCT9qc_HVAm"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "# Menghitung rata-rata loss dengan menggunakan fungsi loss pada target dan prediksi yang dihasilkan oleh model.\n",
        "\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "# Menampilkan bentuk (shape) dari prediksi yang dihasilkan oleh model.\n",
        "\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)\n",
        "# Menampilkan nilai rata-rata loss pada contoh batch.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2XglieyH4-0",
        "outputId": "e5aad945-2119-49b0-b29b-063482cc8cdc"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.189529, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()\n",
        "# Menghitung eksponen dari nilai rata-rata loss menggunakan fungsi tf.exp dan mengonversi hasilnya menjadi array NumPy.\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "# Mengompilasi model dengan optimizer 'adam' dan fungsi loss yang telah ditentukan sebelumnya.\n",
        "\n",
        "# Direktori tempat checkpoints akan disimpan.\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Nama file checkpoint.\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "# Membuat callback ModelCheckpoint untuk menyimpan bobot (weights) model selama pelatihan.\n"
      ],
      "metadata": {
        "id": "3JhN8TfoICj3"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lakukan Proses Training\n",
        "\n",
        "EPOCHS = 20\n",
        "# Menetapkan jumlah epochs sebesar 20.\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "# Melatih model menggunakan dataset yang telah dipersiapkan sebelumnya selama jumlah epochs yang telah ditentukan,\n",
        "# dengan menggunakan callback ModelCheckpoint untuk menyimpan bobot model pada setiap epoch.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuxucJ0RIDNv",
        "outputId": "31e5d78a-35b4-4a3a-bbcd-4c4673a96405"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 15s 59ms/step - loss: 2.7001\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.9753\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.6936\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.5357\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.4398\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 12s 63ms/step - loss: 1.3735\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.3206\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.2756\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 1.2355\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 1.1941\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 1.1542\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.1122\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.0682\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.0209\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.9729\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.9213\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.8685\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.8160\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.7661\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.7167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Teks\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.model = model\n",
        "        self.chars_from_ids = chars_from_ids\n",
        "        self.ids_from_chars = ids_from_chars\n",
        "\n",
        "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "        sparse_mask = tf.SparseTensor(\n",
        "            # Put a -inf at each bad index.\n",
        "            values=[-float('inf')] * len(skip_ids),\n",
        "            indices=skip_ids,\n",
        "            # Match the shape to the vocabulary\n",
        "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        # Convert strings to token IDs.\n",
        "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "        # Run the model.\n",
        "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "        predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                              return_state=True)\n",
        "        # Only use the last prediction.\n",
        "        predicted_logits = predicted_logits[:, -1, :]\n",
        "        predicted_logits = predicted_logits / self.temperature\n",
        "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "        predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "        # Sample the output logits to generate token IDs.\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "        # Convert from token ids to characters\n",
        "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "        # Return the characters and model state.\n",
        "        return predicted_chars, states\n"
      ],
      "metadata": {
        "id": "JSB-5zSyIJhP"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "# Membuat objek OneStep yang menggunakan model, fungsi untuk mengonversi karakter ke ID, dan fungsi untuk mengonversi ID ke karakter.\n",
        "\n",
        "start = time.time()\n",
        "# Membuat variabel start untuk mengukur waktu mulai.\n",
        "\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "# Memulai dengan teks 'ROMEO:' dan menyimpannya dalam variabel next_char, serta membuat list result untuk menyimpan karakter-karakter yang akan di-generate.\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "    result.append(next_char)\n",
        "# Menggunakan model OneStep untuk meng-generate 1000 karakter berikutnya dari teks yang telah diberikan.\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "# Menggabungkan hasil karakter yang di-generate menjadi sebuah teks dan mengukur waktu selesai.\n",
        "\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)\n",
        "# Mencetak hasil teks yang di-generate dan waktu yang dibutuhkan untuk proses generasi karakter.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p5OWwLEIRVz",
        "outputId": "925797f1-b6db-494a-80dd-b73c7905185a"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "When I was born.\n",
            "Go you to pluen here! Mark a corn to rue.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Are you more, by God's grave, be not again:\n",
            "Degree is the clamour of one father.\n",
            "But what may intended but a father's\n",
            "that methinks up this hour with her kings.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Come yo in cold and patrician,\n",
            "Pronounce is but rotten person.\n",
            "\n",
            "MIRANDA:\n",
            "Sir baid, your drum,\n",
            "And pluch his rope more pleasant gate of Nobforch,\n",
            "I'll hear no more: she does be sud\n",
            "And scorn to seek him help, help you,\n",
            "And give him limber, shall get thee to your horse:\n",
            "Is Romeo when it speak of wine.\n",
            "\n",
            "MONTAGUE:\n",
            "And leave me, sir; for of his order father,\n",
            "Your worthy garments are.\n",
            "\n",
            "ROMEO:\n",
            "Sir, my mother!' mind! canst give your leave!\n",
            "\n",
            "KING RICHARD II:\n",
            "Were you the mutiners, nor the heavens\n",
            "one--to greeting. Now, if an unmaid ask,\n",
            "That when she see our lasts with your ancestor.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "What will they say? must he good?\n",
            "This is the selfsame ways.\n",
            "\n",
            "CLARENCE:\n",
            "O, 'tis the fearful swords of Clarectes.\n",
            "\n",
            "ANGELO:\n",
            "Speed how you in anothe \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.296556234359741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# eksplor model generator\n",
        "\n",
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "# Menyimpan model OneStep ke dalam direktori 'one_step'.\n",
        "\n",
        "one_step_reloaded = tf.saved_model.load('one_step')\n",
        "# Memuat kembali model yang telah disimpan.\n",
        "\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "# Memulai dengan teks 'ROMEO:' dan membuat list result untuk menyimpan karakter-karakter yang akan di-generate.\n",
        "\n",
        "for n in range(100):\n",
        "    next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "    result.append(next_char)\n",
        "# Menggunakan model yang telah dimuat kembali untuk meng-generate 100 karakter berikutnya dari teks yang telah diberikan.\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))\n",
        "# Mencetak hasil teks yang di-generate.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmtxsPzuIds1",
        "outputId": "ec08e535-2a85-4f8f-e1c7-206e288b33f6"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7a15fe739e10>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "See, such a lodden, let a groan father sin:\n",
            "Does cot on church, and if it let my head;\n",
            "Which on him\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tugas"
      ],
      "metadata": {
        "id": "Qh3jOZdlJOZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "    @tf.function\n",
        "    def train_step(self, inputs):\n",
        "        inputs, labels = inputs\n",
        "        # Memisahkan input dan label dari data yang diberikan.\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(inputs, training=True)\n",
        "            # Memperoleh prediksi dari model pada mode pelatihan.\n",
        "\n",
        "            loss = self.loss(labels, predictions)\n",
        "            # Menghitung loss antara label dan prediksi.\n",
        "\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        # Menghitung gradien loss terhadap parameter yang dapat diubah (trainable variables) dalam model.\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        # Menggunakan optimizer untuk menerapkan gradien ke parameter dalam model.\n",
        "\n",
        "        return {'loss': loss}\n",
        "        # Mengembalikan nilai loss dari iterasi pelatihan."
      ],
      "metadata": {
        "id": "wYepdno3Ip3D"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "# Membuat objek model CustomTraining dengan parameter yang telah ditentukan sebelumnya.\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "# Mengompilasi model dengan optimizer Adam dan fungsi loss SparseCategoricalCrossentropy.\n",
        "\n",
        "model.fit(dataset, epochs=1)\n",
        "# Melatih model menggunakan dataset yang telah dipersiapkan sebelumnya selama 1 epoch.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uh0Tmh5ZJggw",
        "outputId": "8d56ff19-ff9b-4cd6-bd5e-6dacad1c6b0f"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 19s 62ms/step - loss: 2.7187\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a15e4305f60>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "# Menetapkan jumlah epochs sebesar 10.\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "# Membuat objek mean dari kelas tf.metrics.Mean untuk menghitung rata-rata.\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    # Memulai penghitungan waktu untuk setiap epoch.\n",
        "\n",
        "    mean.reset_states()\n",
        "    # Mengatur ulang nilai state pada objek mean untuk setiap epoch.\n",
        "\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        # Melakukan satu iterasi pelatihan (train step) menggunakan model pada dataset.\n",
        "\n",
        "        mean.update_state(logs['loss'])\n",
        "        # Memperbarui nilai rata-rata loss dengan loss dari iterasi terkini.\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "            # Mencetak informasi loss setiap 50 batch.\n",
        "\n",
        "    # Menyimpan model setiap 5 epochs.\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "# Menyimpan bobot (weights) model setelah semua epochs selesai.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-5n-ff7JhCt",
        "outputId": "fc7af122-78be-4034-882d-42614368eb27"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1647\n",
            "Epoch 1 Batch 50 Loss 2.0687\n",
            "Epoch 1 Batch 100 Loss 1.9699\n",
            "Epoch 1 Batch 150 Loss 1.8768\n",
            "\n",
            "Epoch 1 Loss: 1.9866\n",
            "Time taken for 1 epoch 14.18 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.7966\n",
            "Epoch 2 Batch 50 Loss 1.7216\n",
            "Epoch 2 Batch 100 Loss 1.6927\n",
            "Epoch 2 Batch 150 Loss 1.6508\n",
            "\n",
            "Epoch 2 Loss: 1.7083\n",
            "Time taken for 1 epoch 11.44 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.5957\n",
            "Epoch 3 Batch 50 Loss 1.5514\n",
            "Epoch 3 Batch 100 Loss 1.5673\n",
            "Epoch 3 Batch 150 Loss 1.4790\n",
            "\n",
            "Epoch 3 Loss: 1.5487\n",
            "Time taken for 1 epoch 12.49 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4589\n",
            "Epoch 4 Batch 50 Loss 1.4354\n",
            "Epoch 4 Batch 100 Loss 1.4370\n",
            "Epoch 4 Batch 150 Loss 1.4193\n",
            "\n",
            "Epoch 4 Loss: 1.4503\n",
            "Time taken for 1 epoch 12.73 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3922\n",
            "Epoch 5 Batch 50 Loss 1.3433\n",
            "Epoch 5 Batch 100 Loss 1.3462\n",
            "Epoch 5 Batch 150 Loss 1.3740\n",
            "\n",
            "Epoch 5 Loss: 1.3819\n",
            "Time taken for 1 epoch 13.15 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3492\n",
            "Epoch 6 Batch 50 Loss 1.3219\n",
            "Epoch 6 Batch 100 Loss 1.3685\n",
            "Epoch 6 Batch 150 Loss 1.3368\n",
            "\n",
            "Epoch 6 Loss: 1.3293\n",
            "Time taken for 1 epoch 12.23 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2388\n",
            "Epoch 7 Batch 50 Loss 1.2729\n",
            "Epoch 7 Batch 100 Loss 1.2963\n",
            "Epoch 7 Batch 150 Loss 1.2817\n",
            "\n",
            "Epoch 7 Loss: 1.2836\n",
            "Time taken for 1 epoch 12.35 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2085\n",
            "Epoch 8 Batch 50 Loss 1.2264\n",
            "Epoch 8 Batch 100 Loss 1.2616\n",
            "Epoch 8 Batch 150 Loss 1.2691\n",
            "\n",
            "Epoch 8 Loss: 1.2420\n",
            "Time taken for 1 epoch 20.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1895\n",
            "Epoch 9 Batch 50 Loss 1.2057\n",
            "Epoch 9 Batch 100 Loss 1.2278\n",
            "Epoch 9 Batch 150 Loss 1.2188\n",
            "\n",
            "Epoch 9 Loss: 1.2028\n",
            "Time taken for 1 epoch 11.44 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1110\n",
            "Epoch 10 Batch 50 Loss 1.1528\n",
            "Epoch 10 Batch 100 Loss 1.1458\n",
            "Epoch 10 Batch 150 Loss 1.1990\n",
            "\n",
            "Epoch 10 Loss: 1.1625\n",
            "Time taken for 1 epoch 12.75 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Terdapat beberapa perbedaan antara kode tugas dengan kode pada praktikum 2:\n",
        "\n",
        "1. Penyesuaian Model:\n",
        "\n",
        "- Praktikum 2: Pada praktikum 2, kode mendefinisikan dan menggunakan model recurrent neural network (RNN) sederhana untuk menghasilkan teks yang mirip dengan teks latihan.\n",
        "- Tugas: Kode terlihat lebih kompleks dan merupakan pengembangan lanjutan dari model RNN yang disesuaikan dengan kebutuhan tugas tertentu. Ini mencakup penggunaan kelas CustomTraining yang merupakan turunan dari model yang sudah ada (MyModel), dengan fitur-fitur tambahan yang diintegrasikan seperti fungsi train_step.\n",
        "\n",
        "2. Pelatihan Model:\n",
        "\n",
        "- Praktikum 2: Praktikum 2 lebih fokus pada generasi teks dan cara mengatur dataset serta melatih model RNN untuk menghasilkan teks yang mirip.\n",
        "- Tugas: Kode melibatkan proses pelatihan yang lebih terperinci dengan pembuatan loop pelatihan yang menghitung loss, melakukan optimisasi, dan mencetak informasi loss serta waktu pelatihan untuk setiap epoch.\n",
        "\n",
        "3. Penanganan Checkpoint:\n",
        "\n",
        "- Praktikum 2: Praktikum 2 mungkin tidak secara eksplisit menangani penyimpanan checkpoint (titik kontrol) model untuk digunakan di masa depan.\n",
        "- Tugas: terdapat logika yang menyimpan bobot (weights) model setiap 5 epochs menggunakan model.save_weights, yang merupakan praktek umum untuk menyimpan kemajuan pelatihan model secara teratur untuk penggunaan selanjutnya.\n",
        "\n",
        "4. Pengukuran Metrik:\n",
        "\n",
        "- Praktikum 2: Pada praktikum 2, pengukuran metrik (seperti loss) mungkin tidak disimpan atau dicatat secara eksplisit.\n",
        "- Tugas: objek tf.metrics.Mean() untuk menghitung rata-rata loss selama pelatihan. Hal ini membantu dalam melacak dan melaporkan rata-rata loss dari berbagai batch yang diproses selama setiap epoch."
      ],
      "metadata": {
        "id": "sqegrDxWMTIJ"
      }
    }
  ]
}