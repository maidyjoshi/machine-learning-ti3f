{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JPccQXGBV8b"
      },
      "source": [
        "**Praktikum 2**\n",
        "\n",
        "Generator Teks dengan RNN\n",
        "\n",
        "---\n",
        "\n",
        "Praktikum ini mendemonstrasikan cara melakukan genearsi text menggunakan RNN. Dataset yang digunkan adalah dataset Shakespeare's writing from Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Jika diberikan urutan karakter dari data ini (\"Shakespear\"), latih model untuk memprediksi karakter berikutnya dalam urutan (\"e\"). Urutan teks yang lebih panjang dapat dihasilkan dengan memanggil model berulang kali.\n",
        "\n",
        "Note: Enable GPU acceleration to execute this notebook faster. In Colab: Runtime > Change runtime type > Hardware accelerator > GPU.\n",
        "\n",
        "Tutorial ini menggunakan tf.keras dan eager execution. Berikut adalah contoh output ketika model dalam tutorial ini dilatih selama 30 epoch, dan dimulai dengan prompt \"Q\":\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hVF9kKLBV9C"
      },
      "source": [
        "QUEENE:\n",
        "\n",
        "I had thought thou hadst a Roman; for the oracle,\n",
        "Thus by All bids the man against the word,\n",
        "Which are so weak of care, by old care done;\n",
        "Your children were in your holy love,\n",
        "And the precipitation through the bleeding throne.\n",
        "\n",
        "BISHOP OF ELY:\n",
        "\n",
        "Marry, and will, my lord, to weep in such a one were prettiest;\n",
        "Yet now I was adopted heir\n",
        "Of the world's lamentable day,\n",
        "To watch the next way with his father with his face?\n",
        "\n",
        "ESCALUS:\n",
        "\n",
        "The cause why then we are all resolved more sons.\n",
        "\n",
        "VOLUMNIA:\n",
        "\n",
        "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
        "And love and pale as any will to that word.\n",
        "\n",
        "QUEEN ELIZABETH:\n",
        "\n",
        "But how long have I heard the soul for this world,\n",
        "And show his hands of life be proved to stand.\n",
        "\n",
        "PETRUCHIO:\n",
        "\n",
        "I say he look'd on, if I must be content\n",
        "To stay him from the fatal of our country's bliss.\n",
        "His lordship pluck'd from this sentence then for prey,\n",
        "And then let us twain, being the moon,\n",
        "were she such a case as fills m\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_1jhReVBV9D"
      },
      "source": [
        "Meskipun beberapa kalimat memiliki tata bahasa, sebagian besar tidak masuk akal. Model belum mempelajari arti kata-kata, namun anggap saja:\n",
        "\n",
        "- Modelnya berbasis karakter. Saat pelatihan dimulai, model tidak mengetahui cara mengeja kata dalam bahasa Inggris, atau bahkan kata-kata tersebut merupakan satuan teks.\n",
        "- Struktur keluarannya menyerupai sandiwaraâ€”blok teks umumnya dimulai dengan nama pembicara, dengan huruf kapital semua mirip dengan kumpulan data.\n",
        "- Seperti yang ditunjukkan di bawah, model dilatih pada kumpulan teks kecil (masing-masing 100 karakter), dan masih mampu menghasilkan rangkaian teks yang lebih panjang dengan struktur yang koheren.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1MevMioBV9E"
      },
      "source": [
        "**Setup**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYmJjfEvBV9F"
      },
      "source": [
        "**Import TensorFlow**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Tf_kPPBTBV9G"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf # digunakan untuk membangun dan melatih model machine learning\n",
        "import numpy as np # digunakan untuk menangani array multidimensi dan operasi matematis\n",
        "import os # digunakan untuk berinteraksi dengan sistem operasi\n",
        "import time # digunakan untuk mengukur waktu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX-EzgJ2BV9J"
      },
      "source": [
        "**Download Dataset Shakespeare**\n",
        "\n",
        "Sesuaikan dengan lokasi data yang Anda punya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1BknBnF7BV9K"
      },
      "outputs": [],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt') # mengunduh file dari URL ke direktori cache Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7qJQ62gBV9L"
      },
      "source": [
        "**Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m_LXJJhBV9L",
        "outputId": "a9edec85-56b9-445a-8ffb-6bd96ae802da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8') # digunakan untuk membaca file teks yang dikodekan dalam UTF-8 dan mengembalikan konten file tersebut sebagai string\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters') # mencetak panjang dari string text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBuWgip5BV9M",
        "outputId": "11302bf2-8f7c-436c-87bf-016e5c3da9b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250]) # Menampilkan 250 karakter pertama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4u4PX14BV9M",
        "outputId": "4ef7c53b-fe9c-4d6e-9c2c-b547749230c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text)) # membuat vocab dengan set dari teks yang diurutkan\n",
        "print(f'{len(vocab)} unique characters') # menampilkan panjang variabel vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmq6OpkZBV9N"
      },
      "source": [
        "**Olah Teks**\n",
        "\n",
        "**Vectorize Teks**\n",
        "\n",
        "Sebelum training, Anda perlu mengonversi string menjadi representasi numerik. tf.keras.layers.StringLookup dapat mengubah setiap karakter menjadi ID numerik. Caranya adalah teks akan dipecah menjadi token terlebih dahulu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAcMWrnhBV9N",
        "outputId": "ff4e912b-46d2-4e3b-dedd-e153e8d58ce8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_texts = ['abcdefg', 'xyz'] # Membuat list example_texts\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8') # memecah teks yang diberikan menjadi urutan poin kode Unicode\n",
        "chars # menampilkan variabel chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD5s61Y1BV9O"
      },
      "source": [
        "sekarang buat tf.keras.layers.StringLookup layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0h46ojB9BV9O"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None) # membuat lapisan StringLookup TensorFlow yang dapat digunakan untuk mengkonversi token teks menjadi ID numerik"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC0-xiLhBV9O"
      },
      "source": [
        "perintah diatas mengconvert token menjadi id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVMQNrN4BV9O",
        "outputId": "eee6d173-1ff8-402f-bbc1-5cb203c3a7ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ids = ids_from_chars(chars) # mengkonversi urutan poin kode Unicode chars menjadi urutan ID numerik menggunakan lapisan StringLookup\n",
        "ids # Menampilkan variabel ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_gE0cufBV9P"
      },
      "source": [
        "Karena tujuan tutorial ini adalah untuk menghasilkan teks, penting juga untuk membalikkan representasi ini. Untuk ini Anda dapat menggunakan kode tf.keras.layers.StringLookup(..., invert=True).\n",
        "\n",
        "Catatan: pada kode ini, daripada meneruskan kosakata asli yang dihasilkan dengan diurutkan(set(teks)) gunakan metode get_vocabulary() dari tf.keras.layers.StringLookup sehingga token [UNK] disetel dengan cara yang sama."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FBvYkc58BV9P"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None) # membuat lapisan StringLookup TensorFlow dengan tambahan argument invert=True dan tidak menggunakan token mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8naz4-hcBV9P"
      },
      "source": [
        "Lapisan ini mengconvert kembali karakter dari vektor ID, dan mengembalikannya sebagai karakter tf.RaggedTensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlBn1faIBV9Q",
        "outputId": "4a0fee9b-ca20-4678-bc88-d3742c2c3de7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chars = chars_from_ids(ids) # mengkonversi urutan poin kode Unicode chars menjadi urutan ID numerik menggunakan lapisan StringLookup\n",
        "chars # Menampilkan variabel ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1Fyxa8ZBV9Q"
      },
      "source": [
        "Anda dapat menggunakan tf.strings.reduce_join untuk menggabungkan kembali karakter menjadi string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BizSxfABV9R",
        "outputId": "813b077e-913d-4726-86da-ee00410b89d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy() # menggabungkan nilai pada chars menjadi string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5l03cglLBV9R"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1) # mengembalikan string yang digabungkan dari semua token teks dalam urutan ID numerik ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odQzTC0CBV9R"
      },
      "source": [
        "**Prediksi**\n",
        "\n",
        "---\n",
        "\n",
        "Diberikan sebuah karakter, atau serangkaian karakter, karakter apa yang paling mungkin berikutnya? Ini adalah tugas yang harus Anda latih agar model dapat melakukannya. Masukan ke model akan berupa urutan karakter, dan Anda melatih model untuk memprediksi keluaran berupa karakter berikut pada setiap langkah waktu. Karena RNN mempertahankan keadaan internal yang bergantung pada elemen yang terlihat sebelumnya, mengingat semua karakter dihitung hingga saat ini, karakter apa selanjutnya?\n",
        "\n",
        "**Membuat Trianing Set dan Target**\n",
        "\n",
        "Selanjutnya bagilah teks menjadi contoh sequence. Setiap masukan sequence akan berisi karakter seq_length dari teks. Untuk setiap masukan sequence, target prediksi berisi teks dengan panjang yang sama, hanya digeser satu karakter ke kanan. Jadi, bagi teks menjadi beberapa bagian seq_length+1. Misalnya, seq_length adalah 4 dan teks kita adalah \"Hello\". Urutan masukannya adalah \"Hell\", dan urutan targetnya adalah \"ello\". Untuk melakukan ini, pertama-tama gunakan fungsi tf.data.Dataset.from_tensor_slices untuk mengonversi vektor teks menjadi aliran indeks karakter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7LiXmRsBV9R",
        "outputId": "7f8695b2-8c62-4000-f366-5710680e95d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1], dtype=int64)>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8')) # mengkonversi semua karakter dalam teks text menjadi ID numerik menggunakan lapisan StringLookup ids_from_chars\n",
        "all_ids # Menamplikan variabel all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3LjCCmJbBV9R"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids) # membuat dataset TensorFlow dari urutan ID numerik all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAkyzyAwBV9R",
        "outputId": "efa62c72-53ef-4d03-afbf-3d43a050df1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10): # Melakukan perulangan pada 10 elemen pertama dari dataset ids_dataset\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8')) #  mencetak token teks yang sesuai dengan ID numerik ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9lpBJiEuBV9S"
      },
      "outputs": [],
      "source": [
        "seq_length = 100 # Inisialisasi variabel seq_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2Ntb6NhBV9S"
      },
      "source": [
        "Metode batch memungkinkan Anda dengan mudah mengonversi karakter individual ini menjadi urutan ukuran yang diinginkan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-2Laku8BV9S",
        "outputId": "f4d871bc-10ea-40a9-a6b3-c370b97dc1e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True) # membuat batch tensor ID numerik dari dataset ids_dataset, dengan ID urutan selanjutnya sebagai target dan mengabaikan batch terakhir jika panjangnya kurang dari seq_length+1\n",
        "\n",
        "for seq in sequences.take(1): # Melakikan perulangan nilai pertama dari data sequence\n",
        "  print(chars_from_ids(seq)) # mencetak token teks yang sesuai dengan urutan ID numerik seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjKcB0IVBV9S"
      },
      "source": [
        "akan lebih mudah untuk melihat apa yang dilakukan jika Anda menggabungkan token kembali menjadi string:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moEbE8TmBV9S",
        "outputId": "03e37500-8d3f-490c-9599-ae84b9da7812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5): # Melakukan perulangan pada 5 elemen pertama dari dataset ids_dataset\n",
        "    print(text_from_ids(seq).numpy()) # mencetak teks yang sesuai dengan urutan ID numerik seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd5-zJXaBV9T"
      },
      "source": [
        "Untuk pelatihan, Anda memerlukan kumpulan data pasangan (input, label). Dimana input dan label merupakan urutan. Pada setiap langkah waktu, inputnya adalah karakter saat ini dan labelnya adalah karakter berikutnya. Berikut adalah fungsi yang mengambil urutan sebagai masukan, menduplikasi, dan menggesernya untuk menyelaraskan masukan dan label untuk setiap langkah waktu:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1AbfPLc3BV9T"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1] # mengambil seluruh nilai kecuali nilai terakhir\n",
        "    target_text = sequence[1:] # mengambil seluruh nilai kecuali nilai pertama\n",
        "    return input_text, target_text # mengembalikan variabel input_text dan target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rwDkJ8DBV9T",
        "outputId": "bf2aa66b-b4fc-4f87-ab8d-30eded18407f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_input_target(list(\"Tensorflow\")) # menjalankan function split_input_target dengan argument list 'Tensorflow'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "oioAwinABV9T"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target) # memetakan setiap elemen dalam dataset sequences ke urutan ID numerik yang berisi input teks dan target teks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MvRX17wBV9T",
        "outputId": "53fe2170-cb01-447b-9f26-6d60b86b7412"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1): # melakukan perulangan pada elemen pertama dataset dan membaginya ke variabel input_example dan target_example\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy()) # menampilkan nilai input_example\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy()) # menampilkan nilai target_example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGCb_qfABV9Z"
      },
      "source": [
        "**Membuat Batch Training**\n",
        "\n",
        "Anda menggunakan tf.data untuk membagi teks menjadi sequence yang dapat diatur. Namun sebelum memasukkan data ini ke dalam model, Anda perlu mengacak data dan mengemasnya ke dalam batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2hQzB7XBV9a",
        "outputId": "f4b3cf1d-cb8a-4a55-e1f8-2e80978d5430"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64 # Inisialisasi variabel BATCH_SIZE\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000 # Inisialisasi variabel BUFFER_SIZE\n",
        "\n",
        "dataset = (\n",
        "    dataset # Membuat dataset\n",
        "    .shuffle(BUFFER_SIZE) # mengacak urutan elemen dalam dataset dan menyimpan elemen-elemen yang diacak berdasarkan buffer_size\n",
        "    .batch(BATCH_SIZE, drop_remainder=True) # mengelompokkan elemen-elemen dalam dataset menjadi batch dan  elemen-elemen yang tidak dapat dibagi oleh BATCH_SIZE akan dibuang\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)) # TensorFlow secara otomatis menentukan ukuran buffer yang optimal untuk prefetch\n",
        "\n",
        "dataset # menampilkan variabel dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJHQM7mCBV9a"
      },
      "source": [
        "**Buat Model**\n",
        "\n",
        "---\n",
        "\n",
        "Bagian ini mendefinisikan model sebagai subkelas keras.Model (untuk lebih detilnya, lihat [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)).\n",
        "\n",
        "Model yang kita bangun memiliki 3 lapisan neural network :\n",
        "\n",
        "- tf.keras.layers.Embedding: Lapisan masukan. Tabel pencarian yang dapat dilatih yang akan memetakan setiap karakter-ID ke vektor dengan dimensi embedding_dim;\n",
        "- tf.keras.layers.GRU: lapisan RNN dengan ukuran unit=rnn_units (Anda juga dapat menggunakan lapisan LSTM di sini.)\n",
        "- tf.keras.layers.Dense: Lapisan keluaran, dengan keluaran vocab_size. Ini menghasilkan satu logit untuk setiap karakter dalam kosakata. Ini adalah log kemungkinan setiap karakter menurut model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "nX-s8aH6BV9a"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary()) # menghitung ukuran vocabulary dari lapisan StringLookup ids_from_chars\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256 # Inisialisasi variabel embedding_dim\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024 # Inisialisasi variabel rnn_units"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dwo5tooJBV9b"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # membuat embedding layer untuk memproses urutan ID numerik yang dihasilkan oleh lapisan StringLookup\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units, # Jumlah unit sebanyak rnn_units\n",
        "                                   return_sequences=True, # mengembalikan urutan vektor numerik yang sama dengan panjang urutan input\n",
        "                                   return_state=True) # membuat lapisan GRU untuk memproses urutan vektor numerik yang dihasilkan oleh lapisan embedding dan mengembalikan keadaan internal dari lapisan GRU\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size) # membuat layer dense dengan argument vacab_size\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs # menyimpan variabel inputs ke dalam variabel x\n",
        "    x = self.embedding(x, training=training) # memproses urutan ID numerik x menggunakan lapisan embedding self.embedding dengan set mode training berdasarkan variabel training\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x) # mendapatkan keadaan awal dari lapisan GRU\n",
        "    x, states = self.gru(x, initial_state=states, training=training) # memproses urutan vektor numerik x menggunakan lapisan GRU self.gru dengan keadaan awal states dan mode berdasarkan variabel training\n",
        "    x = self.dense(x, training=training) # memproses urutan vektor numerik x menggunakan lapisan dense\n",
        "\n",
        "    if return_state:\n",
        "      return x, states # mengembalikan nilai x dan states\n",
        "    else:\n",
        "      return x # hanya mengembalikan nilai x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "JMz6PozNBV9b"
      },
      "outputs": [],
      "source": [
        "model = MyModel( # Memanggil function MyModel()\n",
        "    vocab_size=vocab_size, # menambahkan argument berupa variabel vocab_size\n",
        "    embedding_dim=embedding_dim, # menambahkan argument berupa variabel embedding_dim\n",
        "    rnn_units=rnn_units) # menambahkan argument berupa variabel rnn_units"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVS2icnwBV9c"
      },
      "source": [
        "Untuk setiap karakter, model mencari penyematan, menjalankan GRU satu langkah waktu dengan penyematan sebagai masukan, dan menerapkan dense layer untuk menghasilkan log yang memprediksi kemungkinan log karakter berikutnya:\n",
        "\n",
        "![image](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_training.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h12ONGgiBV9c"
      },
      "source": [
        "Note: Untuk pelatihan Anda bisa menggunakan model keras.Sequential di sini. Untuk menghasilkan teks nanti, Anda harus mengelola status internal RNN. Akan lebih mudah untuk memasukkan opsi input dan output status di awal, daripada mengatur ulang arsitektur model nanti. untuk detailnya bisa dilihat Keras RNN guide.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me9xMNixBV9c"
      },
      "source": [
        "**Uji Model**\n",
        "\n",
        "Coba jalankan model dan cek apakah sidah sesuai dengan output\n",
        "\n",
        "pertama, cek bentuk dari output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZduJhuuBV9d",
        "outputId": "1db8168d-feed-4c52-cb7a-fcb7e97bfc43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1): # Melukan perulangan dengan elemen pertama yang dibagi dalam variabel input_example_batch dan target_example_batch\n",
        "    example_batch_predictions = model(input_example_batch) # menghasilkan prediksi untuk batch input input_example_batch menggunakan model\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\") # menampilkan ukuran example_batch_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQc-KSRmBV9d"
      },
      "source": [
        "Dalam contoh di atas, panjang urutan masukan adalah 100 tetapi model dapat dijalankan pada masukan dengan panjang berapa pun:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMAQvflWBV9e",
        "outputId": "20e9580d-2beb-4e5d-807b-c6f0c41740c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary() # mencetak ringkasan dari model machine learning yang telah dibuat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a0mFIkJBV9e"
      },
      "source": [
        "Untuk mendapatkan prediksi aktual dari model, Anda perlu mengambil sampel dari distribusi keluaran, untuk mendapatkan indeks karakter aktual. Distribusi ini ditentukan oleh logit pada kosakata karakter.\n",
        "\n",
        "Catatan: Penting untuk mengambil sampel dari distribusi ini karena mengambil argmax dari distribusi tersebut dapat dengan mudah membuat model terjebak dalam infinote loop. Cobalah untuk contoh pertama di batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "RMxq6b1XBV9e"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1) # mengambil sampel dari distribusi kategorikal yang ditentukan oleh vektor probabilitas example_batch_predictions[0] dengan jumlah sampel yang diambil adalah 1\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy() # menghapus dimensi dari tensor dan mengubah tensor sampled_indices menjadi array NumPy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH9j7zixBV9e"
      },
      "source": [
        "Hal ini memberi kita, pada setiap langkah waktu, prediksi indeks karakter berikutnya:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwPOGG8bBV9f",
        "outputId": "1a9fad59-f1f6-4056-84fe-62fbfee74009"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([48, 23, 41, 56, 30, 51, 64, 16, 26, 65, 17, 16, 31,  8, 37, 51, 19,\n",
              "        6, 35,  8, 47, 47, 46, 44, 17, 14, 15, 57, 46, 55, 38, 40, 58, 47,\n",
              "        4, 26, 47, 21, 38, 29, 61, 19, 25, 16, 65, 24, 63, 52, 33, 20,  6,\n",
              "       55, 55, 62, 38,  8,  3, 29, 39, 31,  7, 16, 18, 13, 57, 63, 62, 62,\n",
              "       47, 50, 20, 57, 36, 53, 34, 38,  0, 10,  8, 61, 25, 50, 12, 10, 55,\n",
              "       31, 25, 19, 18,  5, 45, 55, 45, 37, 52, 33, 64,  7, 31, 25],\n",
              "      dtype=int64)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sampled_indices # menampilkan array variabel sampled_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ckfs3FWBV9f"
      },
      "source": [
        "Dekode kode berikut untuk melihat teks yang diprediksi oleh model tidak terlatih ini:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMXLvrRZBV9g",
        "outputId": "d6ba7e6d-86b8-4499-e753-c844a6a9e0f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b\"all your tears! I am your sorrow's nurse,\\nAnd I will pamper it with lamentations.\\n\\nDORSET:\\nComfort, \"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"iJbqQlyCMzDCR-XlF'V-hhgeDABrgpYash$MhHYPvFLCzKxmTG'ppwY-!PZR,CE?rxwwhkGrWnUY[UNK]3-vLk;3pRLFE&fpfXmTy,RL\"\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy()) # mencetak input pertama dalam batch input_example_batch\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy()) # mencetak prediksi karakter berikutnya untuk input pertama dalam batch input_example_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7z7r61TBV9g"
      },
      "source": [
        "**Train Model**\n",
        "\n",
        "Pada titik ini permasalahan dapat dianggap sebagai permasalahan klasifikasi standar. Permasalahan dapat disimpulkan dengan : Berdasarkan status RNN sebelumnya, dan masukan langkah kali ini, prediksi kelas karakter berikutnya.\n",
        "\n",
        "**Tambahan optimizer dan fungsi loss**\n",
        "\n",
        "---\n",
        "\n",
        "loss function tf.keras.losses.sparse_categorical_crossentropy standar berfungsi dalam kasus ini karena diterapkan di seluruh dimensi terakhir prediksi. Karena model Anda mengembalikan logits, Anda perlu mengatur flag from_logits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "dxs14ouzBV9g"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True) # membuat fungsi kerugian Sparse Categorical Crossentropy dengan menerima logits sebagai input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzNXWDiDBV9g",
        "outputId": "867bf00d-9cd3-42d5-fd7c-1b440dd13081"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.188815, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions) # menghitung rata-rata kerugian untuk batch input example_batch_predictions\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") # menampilkan ukuran example_batch_predictions\n",
        "print(\"Mean loss:        \", example_batch_mean_loss) # menampilkan mean loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9twSHVrBV9h"
      },
      "source": [
        "Model yang baru diinisialisasi tidak boleh terlalu yakin dengan dirinya sendiri, semua log keluaran harus memiliki besaran yang sama. Untuk mengonfirmasi hal ini, Anda dapat memeriksa bahwa eksponensial dari loss rata-rata harus kira-kira sama dengan ukuran kosakata. Loss yang jauh lebih tinggi berarti model tersebut yakin akan jawaban yang salah, dan memiliki inisialisasi yang buruk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lDX_yklBV9h",
        "outputId": "240a47d1-a9e5-4dc4-985b-721c0bbc515d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "65.94461"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy() # menghitung eksponen dari rata-rata kerugian untuk batch input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQe_KQynBV9h"
      },
      "source": [
        "Konfigurasikan prosedur pelatihan menggunakan metode tf.keras.Model.compile. Gunakan tf.keras.optimizers.Adam dengan argumen default dan fungsi loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "VJbxcFzWBV9i"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss) # Compile model denga optimizer ADAM dan argument variabel loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmTU2D5jBV9i"
      },
      "source": [
        "**Konfigurasi Checkpoints**\n",
        "\n",
        "Gunakan tf.keras.callbacks.ModelCheckpoint untuk memastikan bahwa checkpoint disimpan selama pelatihan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "0f2RO0yVBV9i"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints' # membuat direktori untuk menyimpan checkpoint model\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\") # membuat prefix nama file untuk checkpoint model\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True) # menyimpan checkpoint model machine learning ke direktori checkpoint_dir dengan prefix nama file ckpt_{epoch} dan yang disimpan hanya berupa bobotnya"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Iq99F7_BV9i"
      },
      "source": [
        "**Lakukan Proses Training**\n",
        "\n",
        "Agar waktu pelatihan tidak terlalu lama, gunakan 10 epoch untuk melatih model. Di Colab, setel runtime ke GPU untuk pelatihan yang lebih cepat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "242OwNopBV9j",
        "outputId": "c3af6a61-36a1-4fe5-a51f-3e5a98538ed8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "172/172 [==============================] - 1074s 6s/step - loss: 2.7315\n",
            "Epoch 2/20\n",
            " 38/172 [=====>........................] - ETA: 12:59 - loss: 2.1421"
          ]
        }
      ],
      "source": [
        "EPOCHS = 20 # Inisialisasi variabel EPOCHS\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback]) # melatih model  dengan dataset dataset selama sebanyak EPOCHS, dengan menggunakan callback untuk menyimpan checkpoint model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBRXjHeDBV9j"
      },
      "source": [
        "**Generate Teks**\n",
        "\n",
        "---\n",
        "\n",
        "Cara termudah untuk menghasilkan teks dengan model ini adalah dengan menjalankannya dalam loop, dan menyimpan status internal model saat Anda menjalankannya.\n",
        "\n",
        "![image](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_sampling.png?raw=1)\n",
        "\n",
        "Setiap kali Anda memanggil model, Anda memasukkan beberapa teks dan state internal. Model mengembalikan prediksi untuk karakter berikutnya dan state barunya. Masukkan kembali prediksi dan state ke model untuk terus menghasilkan teks.\n",
        "\n",
        "---\n",
        "\n",
        "Berikut ini membuat prediksi satu langkah:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs9tOLpJBV9j"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None] # membuat tensor yang berisi ID token [UNK]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())]) # membuat tensor sparse yang berisi nilai -float('inf') untuk setiap indeks token [UNK]\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask) # mengkonversi tensor sparse sparse_mask menjadi tensor padat\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8') # memecah string yang terkandung dalam tensor inputs menjadi karakter-karakter Unicode menggunakan encoding UTF-8\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor() # mengkonversi input karakter input_chars menjadi tensor ID token\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True) # Menghitung prediksi model machine learning self.model untuk input input_ids dan mengembalikan prediksi dan status model.\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :] # mengambil prediksi model untuk karakter berikutnya dari input input_ids\n",
        "    predicted_logits = predicted_logits/self.temperature # meningkatkan keragaman dan kreativitas dari prediksi model\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask # mengabaikan token [UNK] saat memprediksi karakter berikutnya\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1) # mengambil satu sampel karakter berikutnya dari prediksi model\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1) #  menghapus dimensi tambahan dari tensor predicted_ids\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids) #  mengkonversi ID karakter yang diprediksi menjadi karakter yang sebenarnya\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states # mengembalikan prediksi karakter dan statets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yW6eIRzGBV9k"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars) # membuat model one-step untuk menghasilkan teks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D-o_JHnBV9k"
      },
      "source": [
        "Jalankan secara berulang untuk menghasilkan beberapa teks. Melihat teks yang dihasilkan, Anda akan melihat model mengetahui kapan harus menggunakan huruf besar, membuat paragraf, dan meniru kosakata menulis seperti Shakespeare. Karena sedikitnya jumlah epoch pelatihan, model belum belajar membentuk kalimat runtut."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4ycSUKaBV9l",
        "outputId": "e42ee950-a51f-4063-f0b5-481a26c054ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "Thou dost advise me: O, thank you'lo know how wears,\n",
            "Which were in, to quicken you; it is no other.\n",
            "\n",
            "KING HENRY PI:\n",
            "We master, sund it that, and thankful voices, sir; for thou\n",
            "art seen such sister with you. We pray, we must speak well.\n",
            "\n",
            "SAMPSON:\n",
            "Not yet. in the devil is my love's perduced!\n",
            "\n",
            "Second Messenger:\n",
            "My liege, that I may saw her here, yet doth,\n",
            "Pursued my men make the accusation do I swear\n",
            "To meet you like a thorny welt company:\n",
            "And this same headly let give no other things.\n",
            "\n",
            "WARWICK:\n",
            "Son George Shall the seat to this care?\n",
            "\n",
            "Servant:\n",
            "God gentle honest gentlemen! im he thereby\n",
            "These pride he look'd upon our hands\n",
            "This trensure he was that tiged in my tent\n",
            "Vidow in the falation. We march on, Without I\n",
            "As better betiment in a subject length.\n",
            "3 KING HENRY VI\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Thou liest, Citizen:\n",
            "Neighbours, Glumio maked Marianatise,\n",
            "Which never frown'd in blood and banish'd Hereford,\n",
            "To frank up in your tidings. Gloumstand you\n",
            "Of my poor hold, how for your news?--and cries 'God sa \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.22420597076416\n"
          ]
        }
      ],
      "source": [
        "start = time.time() # mencatat waktu mulai dari suatu proses\n",
        "states = None # Memberikan nilai awal None pada states\n",
        "next_char = tf.constant(['ROMEO:']) # membuat tensor konstan yang berisi karakter ROMEO, yang nilainya tidak dapat diubah\n",
        "result = [next_char] # membuat list yang berisi tensor konstan\n",
        "\n",
        "for n in range(1000): # Melakukan 1000 perulangan\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states) # menghasilkan karakter berikutnya dari input teks menggunakan model one-step one_step_model\n",
        "  result.append(next_char) # Menambahkan nilai next_char ke list result\n",
        "\n",
        "result = tf.strings.join(result) # menggabungkan semua elemen dari list result menjadi satu string.\n",
        "end = time.time() # mencatat waktu proses berakhir\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80) # mencetak teks yang dihasilkan oleh model one-step, diikuti oleh garis bawah sepanjang 80 karakter dalam format decode utf-8\n",
        "print('\\nRun time:', end - start) # Menampilkan waktu runtime dari selisih waktu mulai dan selesai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bj1onWHBV9l"
      },
      "source": [
        "Hal termudah yang dapat Anda lakukan untuk meningkatkan hasil adalah dengan melatihnya lebih lama (coba EPOCHS = 30). Anda juga dapat bereksperimen dengan string awal yang berbeda, mencoba menambahkan lapisan RNN lain untuk meningkatkan akurasi model, atau menyesuaikan parameter suhu untuk menghasilkan prediksi yang kurang lebih acak.\n",
        "\n",
        "Jika Anda ingin model menghasilkan teks lebih cepat, hal termudah yang dapat Anda lakukan adalah membuat teks secara batch. Pada contoh di bawah, model menghasilkan 5 keluaran dalam waktu yang hampir sama dengan waktu yang dibutuhkan untuk menghasilkan 1 keluaran di atas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFSER_uuBV9m",
        "outputId": "5867c507-c4eb-40f9-e65e-72f29af3d4e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nLady's heart let this sad keeping him.\\n\\nNORTHUMBERLAND:\\nTrue bawner Edward be an almost circues,\\nWhen he dared sir, we have not in this fellow of aged\\nTo sigh me this sea framed, must give thee thin:\\nThrow dwelling in the sun season: therefore, great\\nThe stars, which that can I condem these death,\\nYour girth shoes wounds old.\\n\\nDUKE VINCENTIO:\\nWill't please you, sir?\\n\\nFLOReLLA:\\nIt is appear: may steal upon the crown,\\nTo strong a tale flower. Now says her blows.\\n\\nKING Edward, I will well I know my departure\\nTo some hold man. Only wife, and see a troop:\\nUnless, by heaven, I clear the Lady friends,\\nAnd, for the fault length pretty one, so much help to two\\nGalled.\\n\\nFirst Lord:\\nWhat for I have, thou dost,\\nAnd she these banners only to our reports.\\nAy, and therefore, no done! Cath's but needs but in a cold cold fire:\\nNow then we are unparallel'd.\\n\\nPRINCE:\\nAnd since I cannot, good, forbear atide,\\nI may not leave this sapsies behowe the officertainment\\nreturness of the rogul tidings of merit, \"\n",
            " b\"ROMEO:\\nYet, may meet again.\\n\\nFirst Servant:\\nHe is not housetises, when such a winders come alive?\\n\\nNurse:\\nThis ready; sir, nor feeling, silence! answer a merry,\\nYet, as I tell him, you have been brakling one!\\nA persons with thy hand that lies\\nCitizens and cannoble turns of mine.\\n\\nPETRUCHIO:\\nNay, hear themselves are.\\n\\nSecond Keeper:\\nUnless you crown him to revive my Gloucester's death?\\nWell, well, no doubt, she is no sound upon one is a day.\\nO, no, go to corripate me: speak. Bold bear-bought of the eye,\\nThis man in the throne ach of appetite\\nBy Herculars, the against her merits,\\nBut methinks, prove interruptionation\\not now here: those dones away two of meret'stalliar,\\nBehold this new marriage says, my fellow's woman:\\nAnd he is merely sting; and undertake to lose!\\nFroname is greatness! he is condemn'd: not bestoo's dead:\\nNow, by his majesty, to watch them, they are\\nshe never crave to ask the bitcer-baseney\\ngood one--when at last we sear of honour\\nThan his sister of her custom, and Second me\\nAn\"\n",
            " b\"ROMEO:\\nA sheparle, this' long, to make me knows this\\nmuch please to be green and redeemed\\nwith a legs of heavy. Hark, how our peason should\\nMost withst thou title than the French knows.\\n\\nVOLUMNIA:\\nThe sudden must be made, nay.\\n\\nGONZALO:\\nWell, he loves and fortune make the hearts;\\nBut in an Earl of Purish, slipp'd him are revenge.\\n\\nBAPTISTA:\\nYet, know we all.\\n\\nShepherd:\\nHear him, let's good my lord; 'tis a vile traitor\\nThat hath thursworn to hear me say it is the lady.\\nBut, sood the steepher dangerous things!\\n\\nJULIET:\\nI will be satisfied; let him deserve no less\\nBe all the swords of commandment of the appellant; 'twere's with,\\nWho then you'ld change gladly pestering.\\n\\nPROSPERO:\\nThou shalt not?\\n\\nShepherd:\\nThen, ESwas, children!\\n\\nISABELLA:\\nHow say you?\\n\\nSecond Gentleman:\\nI may, sirrah, provide yourself, now I send thee thine,\\nAnd now in Debbitain.\\n\\nHASTINGS:\\n'Sig is the sight against Chichishmen, let it come then back of Montague.\\nO heep in foels.\\n\\nHENRY BOLINGBROKE:\\nMarry, sir, so well, but ha\"\n",
            " b\"ROMEO:\\nIf ruin op nothing first: for an end\\nIf you'll go to honour my master's son:\\nMy pursuing nunsting hathaning schoolmasters are so pancts\\nOf his great knavery.\\n\\nCOMINIUS:\\nI will to him;\\nI have too money of the duke.\\n\\nLUCENTIO:\\nYet look converts him: 'tis a good dull; for, tell me\\nIf this reforting with the roves,\\nArm'd and casting forth an old supposed\\nFound brung his knee doth grief; and his abounding\\ndespair not such a while.\\n\\nCORIOLANUS:\\nNow then the shame, one that are and to death?\\nI tell you, sir, what stays her broofs and heirs doth,\\nbut he, his meeting of all the view;\\nOnly, that I credit, some thing to be confess,\\nI fear no more.\\n\\nANGELO:\\n'Tis time\\nHis young Rutlazing true up instruction. Forgat!\\nO, peace, fortune; be thou well, sir; and\\nThat he constries flowers. To him then, if thou be the rest:\\nCall them awhile, and will obey his wings.\\nCousin, camillo, now for one written? now I beg, we felt,\\nLet him be spentally is a lungful spirit\\nTo violetice; here's me in my elyect.\\n\\nAN\"\n",
            " b\"ROMEO:\\nGive me attainds, we shall needs must royal dun!\\nAnd let it be the suit of the whiles.\\n\\nABHORSON:\\nCome the tender, that's not, how say you, the sword of in this land\\nWould she be batted in this antimour that must be\\nmany more to use it.\\n\\nANTONIO:\\nThank you so stir him to his unhappiness!\\n\\nDUCHESS OF YORK:\\nSpake not the sure done son's face?\\nWhy, sir, what news?\\n\\nA flourder-stained with the night\\nAnd lengthen on the hedge and secrecy and cowarding,\\nCoriolanuay, think you, God bid Richmond and restrains,\\nThe slaughter of the sweetest lady of sheat,\\nBehold the honey at all that to me and\\nWhy with that hence untirour'd by our humble inctitation:\\nRead now thy head and yet and eld be open'd,\\nBut teachers letter to my needless faith.\\n\\nBUCKINGHAM:\\nUpon the schoolmastern tide, wife, they have me\\nFrom the prosper sturvicians as he was so.\\n\\nBAPTISTA:\\nI had thou to a crutch, violet! Fortunain,\\nWhose holy faults, which tired in mind to sea,\\nThan 'twere good old conquerna are gentle laws.\\nHow do yo\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "ROMEO:\n",
            "Lady's heart let this sad keeping him.\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "True bawner Edward be an almost circues,\n",
            "When he dared sir, we have not in this fellow of aged\n",
            "To sigh me this sea framed, must give thee thin:\n",
            "Throw dwelling in the sun season: therefore, great\n",
            "The stars, which that can I condem these death,\n",
            "Your girth shoes wounds old.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Will't please you, sir?\n",
            "\n",
            "FLOReLLA:\n",
            "It is appear: may steal upon the crown,\n",
            "To strong a tale flower. Now says her blows.\n",
            "\n",
            "KING Edward, I will well I know my departure\n",
            "To some hold man. Only wife, and see a troop:\n",
            "Unless, by heaven, I clear the Lady friends,\n",
            "And, for the fault length pretty one, so much help to two\n",
            "Galled.\n",
            "\n",
            "First Lord:\n",
            "What for I have, thou dost,\n",
            "And she these banners only to our reports.\n",
            "Ay, and therefore, no done! Cath's but needs but in a cold cold fire:\n",
            "Now then we are unparallel'd.\n",
            "\n",
            "PRINCE:\n",
            "And since I cannot, good, forbear atide,\n",
            "I may not leave this sapsies behowe the officertainment\n",
            "returness of the rogul tidings of merit,  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.9952306747436523\n"
          ]
        }
      ],
      "source": [
        "start = time.time() # mencatat waktu mulai dari suatu proses\n",
        "states = None # Memberikan nilai awal None pada states\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:']) # membuat tensor konstan yang berisi lima karakter ROMEO, yang nilainya tidak dapat diubah\n",
        "result = [next_char] # membuat list yang berisi tensor konstan\n",
        "\n",
        "for n in range(1000): # Melakukan 1000 perulangan\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states) # menghasilkan karakter berikutnya dari input teks menggunakan model one-step one_step_model\n",
        "  result.append(next_char) # Menambahkan nilai next_char ke list result\n",
        "\n",
        "result = tf.strings.join(result) # menggabungkan semua elemen dari list result menjadi satu string.\n",
        "end = time.time() # mencatat waktu proses berakhir\n",
        "print(result, '\\n\\n' + '_'*80) # mencetak teks yang dihasilkan oleh model one-step, diikuti oleh garis bawah sepanjang 80 karakter\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80) # mencetak teks yang dihasilkan oleh model one-step, diikuti oleh garis bawah sepanjang 80 karakter  dalam format decode utf-8\n",
        "print('\\nRun time:', end - start) # Menampilkan waktu runtime dari selisih waktu mulai dan selesai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xWIpjGyBV9m"
      },
      "source": [
        "**Ekspor Model Generator**\n",
        "\n",
        "Model satu langkah ini dapat dengan mudah disimpan dan digunakan kembali, memungkinkan Anda menggunakannya di mana pun tf.saved_model diterima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDYKCLhRBV9m",
        "outputId": "558afd25-5190-4c1a-b800-b19f6d9e42fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f7cc0aa1b70>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ],
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step') # menyimpan model one-step one_step_model ke direktori one_step\n",
        "one_step_reloaded = tf.saved_model.load('one_step') # memuat model one-step yang disimpan di direktori one_step ke dalam variabel one_step_reloaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pu3M7AmfBV9m",
        "outputId": "76fca0cb-1d13-4ae0-c9ee-a730eb35f1e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "The senators of all ares unharry: spur\n",
            "detice, your scorn'd, take up some other in our will,\n",
            "Which \n"
          ]
        }
      ],
      "source": [
        "states = None # Memberikan nilai awal None pada states\n",
        "next_char = tf.constant(['ROMEO:']) # membuat tensor konstan yang berisi karakter ROMEO, yang nilainya tidak dapat diubah\n",
        "result = [next_char] # membuat list yang berisi tensor konstan\n",
        "\n",
        "for n in range(100): # Melakukan 100 perulangan\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states) # menghasilkan karakter berikutnya dari input teks menggunakan model one-step one_step_model\n",
        "  result.append(next_char) # Menambahkan nilai next_char ke list result\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\")) # # menggabungkan semua elemen dari list result menjadi satu string dan menampilkan hasilnya dalam format decode utf-8"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
