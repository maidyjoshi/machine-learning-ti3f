{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ_d41wYBIkt"
      },
      "source": [
        "**Tugas**\n",
        "\n",
        "Prosedur pelatihan pada praktikum 2 merupakan prosedur sederhana, yang tidak memberi Anda banyak kendali. Model ini menggunakan \"teacher-forcing\" yang mencegah prediksi buruk diumpankan kembali ke model, sehingga model tidak pernah belajar untuk pulih dari kesalahan. Jadi, setelah Anda melihat cara menjalankan model secara manual, selanjutnya Anda akan mengimplementasikan custom loop pelatihan. Hal ini memberikan titik awal jika, misalnya, Anda ingin menerapkan pembelajaran kurikulum untuk membantu menstabilkan keluaran open-loop model. Bagian terpenting dari loop pelatihan khusus adalah fungsi langkah pelatihan.\n",
        "\n",
        "Gunakan  untuk men track nilai gradient. Anda dapat mempelajari lebih lanjut tentang pendekatan ini dengan membaca .\n",
        "\n",
        "Prosedurnya adalah:\n",
        "\n",
        "1. Jalankan Model dan hitung loss dengan .\n",
        "2. Hitung update dan terapkan pada model dengan optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQq2VLSNDQTy"
      },
      "source": [
        "---\n",
        "\n",
        "**Praktikum 2**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Tf_kPPBTBV9G"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf # digunakan untuk membangun dan melatih model machine learning\n",
        "import numpy as np # digunakan untuk menangani array multidimensi dan operasi matematis\n",
        "import os # digunakan untuk berinteraksi dengan sistem operasi\n",
        "import time # digunakan untuk mengukur waktu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1BknBnF7BV9K"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 4s 4us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt') # mengunduh file dari URL ke direktori cache Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m_LXJJhBV9L",
        "outputId": "99fa0918-b435-45a0-bb72-6e0bc528a957"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8') # digunakan untuk membaca file teks yang dikodekan dalam UTF-8 dan mengembalikan konten file tersebut sebagai string\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters') # mencetak panjang dari string text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylnuFzMIBkxa",
        "outputId": "0c3e59d4-e42f-43f5-bdbf-5a7cd0b349c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250]) # Menampilkan 250 karakter pertama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBzajNJWBnlS",
        "outputId": "4dc27844-f14d-4b52-f8d6-4a6fccff23cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text)) # membuat vocab dengan set dari teks yang diurutkan\n",
        "print(f'{len(vocab)} unique characters') # menampilkan panjang variabel vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SECZw_WkBpqj",
        "outputId": "57967ff1-79dc-4fad-c0d3-fb3618a9551b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_texts = ['abcdefg', 'xyz'] # Membuat list example_texts\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8') # memecah teks yang diberikan menjadi urutan poin kode Unicode\n",
        "chars # menampilkan variabel chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QfoaNZScBruk"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None) # membuat lapisan StringLookup TensorFlow yang dapat digunakan untuk mengkonversi token teks menjadi ID numerik"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mry4zpxBurI",
        "outputId": "1ebdece6-a151-48ff-dbd8-cac3d77d0b14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ids = ids_from_chars(chars) # mengkonversi urutan poin kode Unicode chars menjadi urutan ID numerik menggunakan lapisan StringLookup\n",
        "ids # Menampilkan variabel ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UIQ-_zviB51p"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None) # membuat lapisan StringLookup TensorFlow dengan tambahan argument invert=True dan tidak menggunakan token mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVegNn4CB108",
        "outputId": "f8cbc556-6e40-467f-89dd-fd0bb8df8811"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chars = chars_from_ids(ids) # mengkonversi urutan poin kode Unicode chars menjadi urutan ID numerik menggunakan lapisan StringLookup\n",
        "chars # Menampilkan variabel ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZ2ZvwTiB7kR",
        "outputId": "d4daec34-7bd4-410f-ec88-d0deeba2a1c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy() # menggabungkan nilai pada chars menjadi string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "g_vh8UaGB9f2"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1) # mengembalikan string yang digabungkan dari semua token teks dalam urutan ID numerik ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na7T-5ioCA5U",
        "outputId": "7a170ac4-ea28-4b42-f0bc-b61a8e114a7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1], dtype=int64)>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8')) # mengkonversi semua karakter dalam teks text menjadi ID numerik menggunakan lapisan StringLookup ids_from_chars\n",
        "all_ids # Menamplikan variabel all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "v9RIBjf6CEgV"
      },
      "outputs": [],
      "source": [
        "\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids) # membuat dataset TensorFlow dari urutan ID numerik all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HtIL1iRCHZr",
        "outputId": "096d8f10-8616-4ab2-aa77-7765ccd25dd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10): # Melakukan perulangan pada 10 elemen pertama dari dataset ids_dataset\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8')) #  mencetak token teks yang sesuai dengan ID numerik ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zs_d9kg9CJeZ"
      },
      "outputs": [],
      "source": [
        "seq_length = 100 # Inisialisasi variabel seq_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPtEuDizCLLd",
        "outputId": "c55ac35b-c240-4d30-ca66-09460e24f1c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True) # membuat batch tensor ID numerik dari dataset ids_dataset, dengan ID urutan selanjutnya sebagai target dan mengabaikan batch terakhir jika panjangnya kurang dari seq_length+1\n",
        "\n",
        "for seq in sequences.take(1): # Melakikan perulangan nilai pertama dari data sequence\n",
        "  print(chars_from_ids(seq)) # mencetak token teks yang sesuai dengan urutan ID numerik seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qymRlRcBCNDT",
        "outputId": "c68daa07-0486-4172-de78-b6d892b28d38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5): # Melakukan perulangan pada 5 elemen pertama dari dataset ids_dataset\n",
        "    print(text_from_ids(seq).numpy()) # mencetak teks yang sesuai dengan urutan ID numerik seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hPrQKHGqCQEO"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1] # mengambil seluruh nilai kecuali nilai terakhir\n",
        "    target_text = sequence[1:] # mengambil seluruh nilai kecuali nilai pertama\n",
        "    return input_text, target_text # mengembalikan variabel input_text dan target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77G75zKnCRq8",
        "outputId": "26cf798f-ac86-4021-9611-9904be771554"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_input_target(list(\"Tensorflow\")) # menjalankan function split_input_target dengan argument list 'Tensorflow'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0WNgsixPCSgv"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target) # memetakan setiap elemen dalam dataset sequences ke urutan ID numerik yang berisi input teks dan target teks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v529INprCUxa",
        "outputId": "a409745f-c780-4ad2-e71e-fa980839084b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1): # melakukan perulangan pada elemen pertama dataset dan membaginya ke variabel input_example dan target_example\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy()) # menampilkan nilai input_example\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy()) # menampilkan nilai target_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFzs38ulCXGF",
        "outputId": "05169931-631c-4a44-ac57-157557bb419b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64 # Inisialisasi variabel BATCH_SIZE\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000 # Inisialisasi variabel BUFFER_SIZE\n",
        "\n",
        "dataset = (\n",
        "    dataset # Membuat dataset\n",
        "    .shuffle(BUFFER_SIZE) # mengacak urutan elemen dalam dataset dan menyimpan elemen-elemen yang diacak berdasarkan buffer_size\n",
        "    .batch(BATCH_SIZE, drop_remainder=True) # mengelompokkan elemen-elemen dalam dataset menjadi batch dan  elemen-elemen yang tidak dapat dibagi oleh BATCH_SIZE akan dibuang\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)) # TensorFlow secara otomatis menentukan ukuran buffer yang optimal untuk prefetch\n",
        "\n",
        "dataset # menampilkan variabel dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BeuGjLXHCbVd"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary()) # menghitung ukuran vocabulary dari lapisan StringLookup ids_from_chars\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256 # Inisialisasi variabel embedding_dim\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024 # Inisialisasi variabel rnn_units"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "SzC5FKI-CcNz"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # membuat embedding layer untuk memproses urutan ID numerik yang dihasilkan oleh lapisan StringLookup\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units, # Jumlah unit sebanyak rnn_units\n",
        "                                   return_sequences=True, # mengembalikan urutan vektor numerik yang sama dengan panjang urutan input\n",
        "                                   return_state=True) # membuat lapisan GRU untuk memproses urutan vektor numerik yang dihasilkan oleh lapisan embedding dan mengembalikan keadaan internal dari lapisan GRU\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size) # membuat layer dense dengan argument vacab_size\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs # menyimpan variabel inputs ke dalam variabel x\n",
        "    x = self.embedding(x, training=training) # memproses urutan ID numerik x menggunakan lapisan embedding self.embedding dengan set mode training berdasarkan variabel training\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x) # mendapatkan keadaan awal dari lapisan GRU\n",
        "    x, states = self.gru(x, initial_state=states, training=training) # memproses urutan vektor numerik x menggunakan lapisan GRU self.gru dengan keadaan awal states dan mode berdasarkan variabel training\n",
        "    x = self.dense(x, training=training) # memproses urutan vektor numerik x menggunakan lapisan dense\n",
        "\n",
        "    if return_state:\n",
        "      return x, states # mengembalikan nilai x dan states\n",
        "    else:\n",
        "      return x # hanya mengembalikan nilai x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jmsLoOhHCeMo"
      },
      "outputs": [],
      "source": [
        "model = MyModel( # Memanggil function MyModel()\n",
        "    vocab_size=vocab_size, # menambahkan argument berupa variabel vocab_size\n",
        "    embedding_dim=embedding_dim, # menambahkan argument berupa variabel embedding_dim\n",
        "    rnn_units=rnn_units) # menambahkan argument berupa variabel rnn_units"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdYegFFhCg3V",
        "outputId": "65747171-c750-46d0-a0b5-25819802e447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1): # Melukan perulangan dengan elemen pertama yang dibagi dalam variabel input_example_batch dan target_example_batch\n",
        "    example_batch_predictions = model(input_example_batch) # menghasilkan prediksi untuk batch input input_example_batch menggunakan model\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\") # menampilkan ukuran example_batch_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5476y7kECjhn",
        "outputId": "813971b1-38c0-4805-ecaa-dc850eb153cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary() # mencetak ringkasan dari model machine learning yang telah dibuat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sPLAMMblClDm"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1) # mengambil sampel dari distribusi kategorikal yang ditentukan oleh vektor probabilitas example_batch_predictions[0] dengan jumlah sampel yang diambil adalah 1\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy() # menghapus dimensi dari tensor dan mengubah tensor sampled_indices menjadi array NumPy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbyX-xbTCm0r",
        "outputId": "321646e2-67e6-475c-d628-0191a8d10e00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([47, 57, 57, 52, 65, 26, 37, 28, 62, 26, 26, 46, 47, 25, 33, 43, 36,\n",
              "       35, 10,  8, 53, 48, 61, 14, 64, 57,  1, 42, 49, 31,  2, 64,  1, 22,\n",
              "       65, 46,  2, 27, 10, 34, 12, 40, 10, 57, 41,  8, 12, 55, 60, 54, 38,\n",
              "       34, 45, 65, 40, 36, 44, 64, 57, 65, 42, 38, 23,  2,  0,  8,  6, 20,\n",
              "       20, 10,  6, 60, 63, 36, 11, 18,  2, 34, 20, 63, 48, 49, 18, 44, 57,\n",
              "       50, 39, 17,  3, 50, 65, 58, 27,  5, 46, 36, 20, 43, 42,  9],\n",
              "      dtype=int64)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sampled_indices # menampilkan array variabel sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxdDXeYYCoeJ",
        "outputId": "0d5ac272-2f4b-49d2-da5b-24fe9b66b609"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b'orth that fatal screech-owl to our house,\\nThat nothing sung but death to us and ours:\\nNow death shal'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"hrrmzMXOwMMghLTdWV3-nivAyr\\ncjR y\\nIzg N3U;a3rb-;puoYUfzaWeyrzcYJ [UNK]-'GG3'uxW:E UGxijEerkZD!kzsN&gWGdc.\"\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy()) # mencetak input pertama dalam batch input_example_batch\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy()) # mencetak prediksi karakter berikutnya untuk input pertama dalam batch input_example_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "0uD0PY4QCqHE"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True) # membuat fungsi kerugian Sparse Categorical Crossentropy dengan menerima logits sebagai input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwsQznroCr0V",
        "outputId": "5abf7721-b684-4222-afe6-4af3648835f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1885705, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions) # menghitung rata-rata kerugian untuk batch input example_batch_predictions\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") # menampilkan ukuran example_batch_predictions\n",
        "print(\"Mean loss:        \", example_batch_mean_loss) # menampilkan mean loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6Fbs0-wCtOh",
        "outputId": "25360f42-e717-475a-f52a-e0a2adca907d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "65.92848"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy() # menghitung eksponen dari rata-rata kerugian untuk batch input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "JBE4h6ymCvDN"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss) # Compile model denga optimizer ADAM dan argument variabel loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "d_PyPAxdCy49"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints' # membuat direktori untuk menyimpan checkpoint model\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\") # membuat prefix nama file untuk checkpoint model\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True) # menyimpan checkpoint model machine learning ke direktori checkpoint_dir dengan prefix nama file ckpt_{epoch} dan yang disimpan hanya berupa bobotnya"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSj_esbnCzkT",
        "outputId": "3d114342-f3a7-461f-dcde-07eb6f88614e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "172/172 [==============================] - 942s 5s/step - loss: 2.7123\n",
            "Epoch 2/20\n",
            "100/172 [================>.............] - ETA: 6:50 - loss: 2.0575"
          ]
        }
      ],
      "source": [
        "EPOCHS = 20 # Inisialisasi variabel EPOCHS\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback]) # melatih model  dengan dataset dataset selama sebanyak EPOCHS, dengan menggunakan callback untuk menyimpan checkpoint model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3aAPJEqDZU3"
      },
      "source": [
        "---\n",
        "\n",
        "**Praktikum Tugas**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj4Co4NeBwjO"
      },
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None) # membuat lapisan StringLookup TensorFlow dengan tambahan argument invert=True dan tidak menggunakan token mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "PVRnDxN9BIk4"
      },
      "outputs": [],
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function # mengonversi fungsi Python menjadi graph TensorFlow\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs # Menyimpan value inputs dengan membaginya ke inputs dan labels\n",
        "      with tf.GradientTape() as tape: # Membuat GradientTape untuk menghitung gradien dari operasi TensorFlow yang direkam\n",
        "          predictions = self(inputs, training=True) # memanggil model untuk menghasilkan prediksi dengan mengaktifkan mode training\n",
        "          loss = self.loss(labels, predictions) # menghitung kerugian model dengan membandingkan labels dengan predictions\n",
        "\n",
        "      grads = tape.gradient(loss, model.trainable_variables) # menghitung gradien dari kerugian model terhadap variabel-variabel yang dapat dilatih\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables)) # memperbarui parameter model menggunakan optimizer\n",
        "\n",
        "      return {'loss': loss} # Mengembalikan dictionary dengan key 'loss' dan value variabel loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHACzeZBBIk9"
      },
      "source": [
        "Kode diatas menerapkan train_step method sesuai dengan  Keras' train_step conventions. Ini opsional, tetapi memungkinkan Anda mengubah perilaku langkah pelatihan dan tetap menggunakan keras Model.compile and Model.fit methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "U7JXWIZpBIk-"
      },
      "outputs": [],
      "source": [
        "model = CustomTraining( # Membuat Model CustomTraining\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()), # dengan kosakata yang digunakna sebanyak vocabulary yang dibuat sebelumnya\n",
        "    embedding_dim=embedding_dim, # argument embedding_dim dari variabel embedding_dim\n",
        "    rnn_units=rnn_units) # dan banyak unit rnn dengan variabel rnn_units"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "2T1QPmXUBIlA"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)) # Compile model dengan optimizer ADAM yang menhasilkan output berupa logits dengan menggunakan function loss berupa SparseCategoricalCrossentropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fn-LzmqMBIlC",
        "outputId": "a82b5913-414b-462b-8389-198db2532836"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "172/172 [==============================] - 14s 57ms/step - loss: 2.7443\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a1953b79c30>"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(dataset, epochs=1) # Melatih model dengan dataset dan 1 epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjLY0PbqBIlD"
      },
      "source": [
        "Atau jika ingin lebih mengetahui dalamnya, kita bisa membuat custom training loop sendiri:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_NpBgKIBIlG",
        "outputId": "291a33d2-21b4-4194-e853-cecd9fe42a6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1690\n",
            "Epoch 1 Batch 50 Loss 2.0413\n",
            "Epoch 1 Batch 100 Loss 1.9430\n",
            "Epoch 1 Batch 150 Loss 1.8204\n",
            "\n",
            "Epoch 1 Loss: 2.0049\n",
            "Time taken for 1 epoch 13.63 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8044\n",
            "Epoch 2 Batch 50 Loss 1.7919\n",
            "Epoch 2 Batch 100 Loss 1.7259\n",
            "Epoch 2 Batch 150 Loss 1.6558\n",
            "\n",
            "Epoch 2 Loss: 1.7305\n",
            "Time taken for 1 epoch 12.99 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6224\n",
            "Epoch 3 Batch 50 Loss 1.5703\n",
            "Epoch 3 Batch 100 Loss 1.5607\n",
            "Epoch 3 Batch 150 Loss 1.5154\n",
            "\n",
            "Epoch 3 Loss: 1.5678\n",
            "Time taken for 1 epoch 12.43 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4588\n",
            "Epoch 4 Batch 50 Loss 1.4705\n",
            "Epoch 4 Batch 100 Loss 1.4582\n",
            "Epoch 4 Batch 150 Loss 1.4159\n",
            "\n",
            "Epoch 4 Loss: 1.4667\n",
            "Time taken for 1 epoch 11.96 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.4342\n",
            "Epoch 5 Batch 50 Loss 1.4170\n",
            "Epoch 5 Batch 100 Loss 1.3971\n",
            "Epoch 5 Batch 150 Loss 1.3708\n",
            "\n",
            "Epoch 5 Loss: 1.3970\n",
            "Time taken for 1 epoch 12.89 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3367\n",
            "Epoch 6 Batch 50 Loss 1.3397\n",
            "Epoch 6 Batch 100 Loss 1.3280\n",
            "Epoch 6 Batch 150 Loss 1.3308\n",
            "\n",
            "Epoch 6 Loss: 1.3432\n",
            "Time taken for 1 epoch 12.17 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2902\n",
            "Epoch 7 Batch 50 Loss 1.3248\n",
            "Epoch 7 Batch 100 Loss 1.3094\n",
            "Epoch 7 Batch 150 Loss 1.3190\n",
            "\n",
            "Epoch 7 Loss: 1.2975\n",
            "Time taken for 1 epoch 12.75 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2548\n",
            "Epoch 8 Batch 50 Loss 1.2747\n",
            "Epoch 8 Batch 100 Loss 1.2855\n",
            "Epoch 8 Batch 150 Loss 1.2702\n",
            "\n",
            "Epoch 8 Loss: 1.2574\n",
            "Time taken for 1 epoch 11.40 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1632\n",
            "Epoch 9 Batch 50 Loss 1.2227\n",
            "Epoch 9 Batch 100 Loss 1.2647\n",
            "Epoch 9 Batch 150 Loss 1.2731\n",
            "\n",
            "Epoch 9 Loss: 1.2191\n",
            "Time taken for 1 epoch 11.43 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1761\n",
            "Epoch 10 Batch 50 Loss 1.1746\n",
            "Epoch 10 Batch 100 Loss 1.1567\n",
            "Epoch 10 Batch 150 Loss 1.1791\n",
            "\n",
            "Epoch 10 Loss: 1.1797\n",
            "Time taken for 1 epoch 11.36 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 11 Batch 0 Loss 1.1498\n",
            "Epoch 11 Batch 50 Loss 1.1560\n",
            "Epoch 11 Batch 100 Loss 1.1541\n",
            "Epoch 11 Batch 150 Loss 1.1824\n",
            "\n",
            "Epoch 11 Loss: 1.1408\n",
            "Time taken for 1 epoch 11.08 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 12 Batch 0 Loss 1.0795\n",
            "Epoch 12 Batch 50 Loss 1.1065\n",
            "Epoch 12 Batch 100 Loss 1.1127\n",
            "Epoch 12 Batch 150 Loss 1.1086\n",
            "\n",
            "Epoch 12 Loss: 1.0995\n",
            "Time taken for 1 epoch 11.02 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 13 Batch 0 Loss 1.0014\n",
            "Epoch 13 Batch 50 Loss 1.0371\n",
            "Epoch 13 Batch 100 Loss 1.0485\n",
            "Epoch 13 Batch 150 Loss 1.0396\n",
            "\n",
            "Epoch 13 Loss: 1.0553\n",
            "Time taken for 1 epoch 11.22 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 14 Batch 0 Loss 0.9872\n",
            "Epoch 14 Batch 50 Loss 0.9929\n",
            "Epoch 14 Batch 100 Loss 0.9928\n",
            "Epoch 14 Batch 150 Loss 1.0322\n",
            "\n",
            "Epoch 14 Loss: 1.0089\n",
            "Time taken for 1 epoch 11.08 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 15 Batch 0 Loss 0.9606\n",
            "Epoch 15 Batch 50 Loss 0.9662\n",
            "Epoch 15 Batch 100 Loss 0.9542\n",
            "Epoch 15 Batch 150 Loss 0.9976\n",
            "\n",
            "Epoch 15 Loss: 0.9602\n",
            "Time taken for 1 epoch 20.60 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 16 Batch 0 Loss 0.8668\n",
            "Epoch 16 Batch 50 Loss 0.9018\n",
            "Epoch 16 Batch 100 Loss 0.9080\n",
            "Epoch 16 Batch 150 Loss 0.9363\n",
            "\n",
            "Epoch 16 Loss: 0.9085\n",
            "Time taken for 1 epoch 11.78 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 17 Batch 0 Loss 0.8223\n",
            "Epoch 17 Batch 50 Loss 0.8301\n",
            "Epoch 17 Batch 100 Loss 0.8829\n",
            "Epoch 17 Batch 150 Loss 0.8660\n",
            "\n",
            "Epoch 17 Loss: 0.8573\n",
            "Time taken for 1 epoch 11.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 18 Batch 0 Loss 0.7792\n",
            "Epoch 18 Batch 50 Loss 0.7434\n",
            "Epoch 18 Batch 100 Loss 0.8321\n",
            "Epoch 18 Batch 150 Loss 0.8400\n",
            "\n",
            "Epoch 18 Loss: 0.8041\n",
            "Time taken for 1 epoch 11.61 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 19 Batch 0 Loss 0.7165\n",
            "Epoch 19 Batch 50 Loss 0.7387\n",
            "Epoch 19 Batch 100 Loss 0.7632\n",
            "Epoch 19 Batch 150 Loss 0.8128\n",
            "\n",
            "Epoch 19 Loss: 0.7544\n",
            "Time taken for 1 epoch 11.80 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 20 Batch 0 Loss 0.6567\n",
            "Epoch 20 Batch 50 Loss 0.7037\n",
            "Epoch 20 Batch 100 Loss 0.7128\n",
            "Epoch 20 Batch 150 Loss 0.7657\n",
            "\n",
            "Epoch 20 Loss: 0.7067\n",
            "Time taken for 1 epoch 11.30 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 20 # Inisisalisasi variabel EPOCH\n",
        "\n",
        "mean = tf.metrics.Mean() # membuat metrik rata-rata\n",
        "\n",
        "for epoch in range(EPOCHS): # Perulangan sebanyak EPCOHS\n",
        "    start = time.time() # Mencatat waktu mulai proses pada perulangan saat ini\n",
        "\n",
        "    mean.reset_states() # mereset metrik rata-rata\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset): # melakukan iterasi pada dataset dan mendapatkan batch data yang terdiri dari input dan target dengan menambahkan indeks disetiap elemennya\n",
        "        logs = model.train_step([inp, target]) # melatih modelg dengan menggunakan batch data yang diberikan\n",
        "        mean.update_state(logs['loss']) # memperbarui metrik rata-rata dengan loss training\n",
        "\n",
        "        if batch_n % 50 == 0: # Jika batch_n modulus 50 sama dengan 0\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\" # Menampilkan Epoch saat ini, value batch dan loss\n",
        "            print(template) # Menampilkan template\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0: # Jika epoch + 1 modulus 5 sama dengan 0\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch)) # menyimpan bobot model ke file dengan memanfaatkan checkpoint_prefix sebelumnya\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}') # Menampilkan epoch saat ini dan hasil dari mean\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec') # Menampilkan estimasi waktu dari selisih waktu saat ini dengan start\n",
        "    print(\"_\"*80) # Menampilkan 80 garis bawah\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch)) # menyimpan bobot model machine learning ke file dengan memanfaatkan checkpoint_prefix sebelumnya"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pezmllimBIlI"
      },
      "source": [
        "Jalankan kode diatas dan sebutkan perbedaanya dengan praktikum 2?\n",
        "\n",
        "- Pada **Praktikum 2** memiliki estimasi waktu yang lebih cepat untuk menjalankan proses disetiap epoch daripada **Praktikum Tugas**, dikarenakan pada **Praktikum Tugas** walaupun estimasi waktunya masih serupa dengan **Praktikum 2** yaitu berkisar antara 11s sampai 14s, namun pada **Praktikum Tugas** terjadi lonjakan estimasi waktu pada epoch 15 mencapai 12.6s.\n",
        "\n",
        "- Namun terjadi hal sebaliknya pada nilai loss yang dihasilkan, pada **Praktikum Tugas** memiliki nilai loss yang lebih sedikit daripada pada **Praktikum 2**, jika pada **Praktikum 2** nilai loss pada epoch satu adalah 2.7240 dan pada epoch dua puluh menghasilkan nilai loss 0.7258 sedangkan pada **Praktikum Tugas** dimulai dengan nilai loss sebesar 2.0049 pada epoch satu dan diakhiri dengan nilai loss sebesar 0.7067 pada epoch kedua puluh"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
