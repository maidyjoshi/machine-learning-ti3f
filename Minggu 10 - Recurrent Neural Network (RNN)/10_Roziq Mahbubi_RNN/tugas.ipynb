{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOhIPtU+KI/+Ya/pfnmDjvC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Tugas"
      ],
      "metadata": {
        "id": "57Za4W6SE490"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Setup"
      ],
      "metadata": {
        "id": "SVx9ItOpE6oj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Import TensorFlow"
      ],
      "metadata": {
        "id": "SJcAq8VqFAlG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvIeENcQCF4a"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Download Dataset Shakespeare"
      ],
      "metadata": {
        "id": "HzdOrS-wFEKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "79ZlButGCSlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Load Data"
      ],
      "metadata": {
        "id": "NNRcIKuGFHNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXWHm9hDCTK9",
        "outputId": "10e5b82b-7b03-426d-f427-829306bc9631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7C8s_D5CX8i",
        "outputId": "dec788a0-23d9-45ec-ed61-e40922a90589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Alwsy9wTCY3f",
        "outputId": "7e1b5ab6-5ef1-4fa6-c17b-81c928daf699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Olah Teks"
      ],
      "metadata": {
        "id": "ohoPg3wrFM4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Vectorize Teks"
      ],
      "metadata": {
        "id": "FXvvkzNRFPkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRexptqoCa9V",
        "outputId": "82ab7c72-28ef-4f7f-dbe4-0eceb6ca796f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "iezCXgu7CbUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-uPUAXmCccN",
        "outputId": "0dc39545-6a88-434e-b91d-0ad411aad481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "TYAaIiKACkEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2RGsvZACllc",
        "outputId": "e3d3fa4b-820e-47f5-94b1-f0f0be534ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPYhdo4fCo_r",
        "outputId": "2728be31-3a54-40e2-da21-1d589355f681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "RdyAI3rJCrLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Membuat Training Set dan Target"
      ],
      "metadata": {
        "id": "oC8TmInBLkE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixrIRGMhCuAj",
        "outputId": "4b8d50f0-de22-41ff-e8d4-cee3f2b43264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "mQsmSdHWCuzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtRtfgFNC0a7",
        "outputId": "043b1b8f-5ef3-4e21-defa-145da80cabe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "1kRRJtZ_C0Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kN5RBtPC0WI",
        "outputId": "4a00945f-8efc-4085-a18d-216f3656a43c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8grrvNGFCupJ",
        "outputId": "cd19d853-51fa-428c-a5aa-1d295c28240d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "8pbHfHjMCudt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4QgV-ACC-R3",
        "outputId": "69c65783-2f2b-4c5d-a44d-a72c461d1757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "bQbdp80PC-Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Lri1qoTC-LT",
        "outputId": "1eacdbbd-f52f-4656-f49c-598e89386763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Membuat Batch Training"
      ],
      "metadata": {
        "id": "pxkkpahYLsuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ4KYwQsC-Ip",
        "outputId": "4b8131ca-d11c-47c7-d585-f8ba5ab03bcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Buat Model"
      ],
      "metadata": {
        "id": "ob4EfsNTLvcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "3UCzv6KaC-GH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "Oe3-NDJ-C-Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "4ubsvevqC-BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Uji Model"
      ],
      "metadata": {
        "id": "R0jFiuBWL062"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZe2JucqC9-8",
        "outputId": "dadc9bea-a477-444f-e70b-0adb0be0e546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Chp2NrMcDMU4",
        "outputId": "810ff2a0-1748-49de-c337-a8a742e2cb83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     multiple                  16896     \n",
            "                                                                 \n",
            " gru_4 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_4 (Dense)             multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "7YN0PkKiDMS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clxmQznbDMQM",
        "outputId": "c667056e-1902-46dd-b099-ffee1d4c3c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([20, 51, 35, 59, 15, 44,  4, 12, 51, 12,  3, 55, 65, 21, 33, 21, 29,\n",
              "       28, 56, 17,  7, 52, 47, 12, 42, 48, 58, 51,  4, 48, 10,  7,  0, 16,\n",
              "       64, 19, 13, 28, 40, 45, 26, 24, 48,  1, 31,  6, 44,  9, 44, 32, 38,\n",
              "       23, 28, 40,  4,  7, 48, 47, 12,  5, 46,  8,  8, 29, 28, 56, 61, 23,\n",
              "       61, 30, 24, 65, 44, 34, 25, 31, 60, 64, 18, 30, 55, 50, 50, 29, 63,\n",
              "       18, 28, 65,  2,  9, 63, 11, 47, 20, 41, 49, 42, 61, 55, 61])"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMyQ5K8mDMNv",
        "outputId": "e8006a96-9f45-49fd-ac3d-95f5166c4ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'my\\nheart wept blood. Who was most marble there changed\\ncolour; some swooned, all sorrowed: if all th'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"GlVtBe$;l;!pzHTHPOqD,mh;cisl$i3,[UNK]CyF?OafMKi\\nR'e.eSYJOa$,ih;&g--POqvJvQKzeULRuyEQpkkPxEOz .x:hGbjcvpv\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train Model"
      ],
      "metadata": {
        "id": "3iGega2oL4p8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Tambahan optimizer dan fungsi loss"
      ],
      "metadata": {
        "id": "83n1I0MYL7hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "zrTOei-jDMLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLuusorzDMIj",
        "outputId": "da3f2164-5c81-41a8-ad4c-d5352b7b94da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.189324, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrK_WmcsDMF-",
        "outputId": "a700e0ec-76a8-4ee9-8941-6bf29511b515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.978165"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "nvf1wDUzDMDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "idL9cHgbDMAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "m9eOwQiYDL-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoFljodODL73",
        "outputId": "c80e1911-6d59-4d86-a458-2a72035f8b5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 16s 69ms/step - loss: 2.7112\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 15s 64ms/step - loss: 1.9881\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.7112\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.5507\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.4519\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.3836\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.3318\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.2869\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 13s 60ms/step - loss: 1.2463\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.2070\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.1686\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 1.1284\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.0846\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.0403\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.9921\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.9411\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.8902\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.8380\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.7867\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.7371\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Generate Teks"
      ],
      "metadata": {
        "id": "NDLTeZSgMG31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "xLcT6w90DL5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "BF4FUobXDc-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ah8zozxVDc6q",
        "outputId": "0b47f4af-1bcd-4d77-b241-148212c6b7bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "How now! whisper I gest; she had, my knighth like very story\n",
            "King Richard God in his gentlewoman:\n",
            "if you be met o'er apollo:\n",
            "She may do more hold out flew with weeth;\n",
            "And Henry, my gracious friend, at least\n",
            "Have spoke. Tybalt sins of England,\n",
            "Whipp'd on thy good wild, whom weep all mean\n",
            "Will come. This deadly wound thy lord: I mean--\n",
            "\n",
            "MARIANA:\n",
            "Well save you?\n",
            "\n",
            "SICINIUS:\n",
            "Clupple and tears all incents of my sheets,\n",
            "Which her weep's watching butt, for everence\n",
            "Thus have beheld the crown, and proven to,\n",
            "Resign my gracious lord, make wort myself.\n",
            "\n",
            "WARWICK:\n",
            "Take the devil's marchese retress than a conquestion.\n",
            "\n",
            "KING HENRY VI:\n",
            "Why, if you stand by eyes, gozards himself in false;\n",
            "All tongues to his bosom and to get our hands;\n",
            "His spirits ne'er a whoremaster, my part cried\n",
            "'O, Jesus! Carth gentle Paulina?\n",
            "\n",
            "SLY:\n",
            "Ay, as I am, litter with otherwess:\n",
            "Yet who ch'er he not have at me.\n",
            "\n",
            "ARIEL:\n",
            "Thou dost ill.\n",
            "\n",
            "First Servingman:\n",
            "When you should be gone and feet so fast,\n",
            "He could not bless of honour; tha \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.717982292175293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaurcmTTDjNR",
        "outputId": "887230c4-c9dd-4b40-ff83-9200bfe64ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nI take thee now, not for them woman, of great duke's ut\\ndeliver'd me, with thy weather, and remember thy right;\\nThere shops and inferior voices there,\\nThinking he touch'd spirit was myself?\\nIs loving friar hath Depture my threads\\nTo the devil that occasion whisper withal\\nmy soul so soon as a blotter of them there\\nLike a shame, till at Corioli alace:\\nAgain when you present at him I'll take:\\nFor thy most given make between twice and my cousing--\\nHortensio come ashame,\\nAs you's pash-paped winding shook, for ever\\nmarried like a deading of the people, being so look\\non him; for because they do: fresh lowly fault,\\nThough never from thy state and state,\\nViect the moon, that makes a guilty king,\\n'Fore the balm of your forces shoes?\\n\\nRIVER:\\nAt Tauntious Margaret,\\nThat again deep eyes two give me out of him,\\nWhich was so valiant book, quitted;\\nAnd so we are their fortunate-passadiest.\\n\\nFirst Servingman:\\nWho should suffer this think to the king is dead:\\nThen flyer as my rap me stain your mother,\\n\"\n",
            " b\"ROMEO:\\nIt fellow? take thee, with a divine leason.\\n\\nSICINIUS:\\nWe are toes here?\\n\\nHERMIONE:\\n'Tis none: work, and proce;\\nBut for my bench their infancy at you.\\nIn soft as well as I,--\\nWorthy Montague. Sir Romeo, will they bear it take,\\nWhere were you percument? is 't a charm, till he be ourselves?\\nWas envy to the unpoint voices of this country's lack'd,\\nNo more than can my body in his sight.\\n\\nDUKE OF AUMERLE:\\nFortent her on here ill in deffect for his honour.\\n\\nFRIAR PHOPSET:\\nHold, hold! For myself, Isabel,\\nWill have my daughter King Leit, right, for it\\nhath that pale tested to come to violate things\\nhis neck: therefore we'll hear now. O noble\\ncunsafford; and the mustard of Herefiance\\nconceives, pleasant serves in apparent game,\\nSuch for their thush that I command: which is a\\nFrench drown'd false eyes with sweet and lawful much.\\n\\nGLOUCESTER:\\n\\nKING EDWARD IV:\\nWilt thou go along these tribunes? Why forward is that fear their preser in\\nmad; for we were well to thee at judgments,\\nStoly children; le\"\n",
            " b'ROMEO:\\nKnow you thus arms,\\nConfirm and nothing; past nothing to his execution.:\\nI should love my dangerous service;\\nThinking power of conscience and them!\\n\\nTRANIO:\\nSir, with a tear of whom I live on to sup out of your hip.\\n\\nLADY GREY:\\nHorse in Somerset, for I stand forth, to\\nexcel her to the prisonerous grown youthful keeping eye,\\nProvoked by age and seat of it: and in this place will rest\\nUnto the white restory in power.\\n\\nKING RICHARD III:\\nThe more I study, and not abjoar me.\\nYou are too shiel; tell him, and long as know\\nThy territ me with yelter tongues?\\n\\nABHORSON:\\nThe gods did my knee is present doth.\\n\\nWARWICK:\\nStuff with countenance, comfort him.\\n\\nSICINIUS:\\nWhy at death is meet,\\nStrike to keep me to my friends arm past;\\nThus play to enter with such weavest to hears of man\\nHe should instrument to hear thee speak;\\nThough your meed arish what I was,\\nHis bound to backward turn your most falsely persuasion.\\nKated at Verona band, tilt him from me to see that he\\ncast offen, teach themselves as '\n",
            " b\"ROMEO:\\nSaw you first see what I speak, my lord?\\n\\nMENENIUS:\\nHas he done content you?\\n\\nAUTOLYCUS:\\nA fire, and too much alone.\\n\\nHENRY BOLINGBROKE:\\nWhat, Camillo, thou didst vengeance be too rough,\\north birth, she hath a good degenerate,\\nWho were well seen out o'er-rone.\\n\\nFirst Soldier:\\nFamiliarly on me! the dearer\\nfor, and Master Froth shall have mine arms feel\\nthou holp the worst of Menanian so himself,\\nWho both he bears, my ragged friends,\\nBut helping bale peremptory villains.\\nStand up both ours, as his house, France, forth, though the\\nsheathing dolame, that he would\\nfaintly splank to gainst all affairs: if it be a south;\\nOur slunge ronage, shine the officerity!\\nWhat 'tis my childish fresh fierce hand.\\nWhat, none that I may conceal these gratts\\nWill it rest bleed her silent king.\\n\\nSICINIUS:\\nWell, you look'd as if they show too doubt.\\nTo hide that holy widows to the king.\\n\\nKING HENRY VI:\\nNow I will love thee again enough.\\n\\nMENENIUS:\\nLet's be satisfied!\\nSkill to return from France.\\n\\nKING EDWARD\"\n",
            " b\"ROMEO:\\nMarch for that him that have burst in my seizing.\\n\\nLEONTES:\\nI'll enterhapper!\\n\\nMENENIUS:\\nWherefore shuris? of these minds?\\nWhy, how now, ushill, and the come.\\nWhen you show then, uncleaked, all is grue\\nAnd your conferesce now can change my friends: he, my lord,\\nAre well the excelse of our miserable state:\\nWhich was a dishonour lands for freshness.\\n\\nMENENIUS:\\nAy, fury to your substitut: how cemest thy wooe,\\nMore than your approbation and slaggs, minds,\\nYet deafles and crows. Bringment my heart!\\nWhat must be therefore?\\n\\nDUKE VINCENTIO:\\nMost noble Marcius.\\n\\nBIONDELLO:\\nThe eagled in the purpose down;\\nAnd if I die not loath;\\nO pitce she may be whistle. I am a piece, or else\\nThou badest me to something to my soul's puff, fall, thou wilt add;\\nFrom where you will Kate hadst thou out a sid\\ngaths that at Hontension hail.\\n\\nKING RICHARD III:\\nWhy do not speak our counsel in this fall? Hark!\\n\\nGaintanou, and love of this I came,\\nAnd webed in the being clouds more perid and privilige,\\nHave written to\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.429490327835083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tugas : Custom Training"
      ],
      "metadata": {
        "id": "ub6hVBlR4XA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "ey1L_kzb4a21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "CmHYSlxV4hC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "p8y-_5cz4ilJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYXsQrmF4jzb",
        "outputId": "57261306-5de0-490a-ed0a-a129784b2810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 15s 57ms/step - loss: 2.7052\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f463ebbca30>"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Looping Custom Training manual"
      ],
      "metadata": {
        "id": "YkqPIaT64q-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYAsjA4I4qVW",
        "outputId": "ea43007c-7573-4857-8090-aca006424738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1706\n",
            "Epoch 1 Batch 50 Loss 2.0746\n",
            "Epoch 1 Batch 100 Loss 1.9631\n",
            "Epoch 1 Batch 150 Loss 1.8608\n",
            "\n",
            "Epoch 1 Loss: 1.9828\n",
            "Time taken for 1 epoch 13.94 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8354\n",
            "Epoch 2 Batch 50 Loss 1.7887\n",
            "Epoch 2 Batch 100 Loss 1.6876\n",
            "Epoch 2 Batch 150 Loss 1.6431\n",
            "\n",
            "Epoch 2 Loss: 1.6999\n",
            "Time taken for 1 epoch 20.48 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.5782\n",
            "Epoch 3 Batch 50 Loss 1.6143\n",
            "Epoch 3 Batch 100 Loss 1.5053\n",
            "Epoch 3 Batch 150 Loss 1.5248\n",
            "\n",
            "Epoch 3 Loss: 1.5410\n",
            "Time taken for 1 epoch 10.97 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.5277\n",
            "Epoch 4 Batch 50 Loss 1.5005\n",
            "Epoch 4 Batch 100 Loss 1.4829\n",
            "Epoch 4 Batch 150 Loss 1.4373\n",
            "\n",
            "Epoch 4 Loss: 1.4434\n",
            "Time taken for 1 epoch 10.99 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.4236\n",
            "Epoch 5 Batch 50 Loss 1.3689\n",
            "Epoch 5 Batch 100 Loss 1.3632\n",
            "Epoch 5 Batch 150 Loss 1.3292\n",
            "\n",
            "Epoch 5 Loss: 1.3766\n",
            "Time taken for 1 epoch 11.35 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3186\n",
            "Epoch 6 Batch 50 Loss 1.3271\n",
            "Epoch 6 Batch 100 Loss 1.3341\n",
            "Epoch 6 Batch 150 Loss 1.3562\n",
            "\n",
            "Epoch 6 Loss: 1.3244\n",
            "Time taken for 1 epoch 12.07 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2847\n",
            "Epoch 7 Batch 50 Loss 1.2799\n",
            "Epoch 7 Batch 100 Loss 1.2817\n",
            "Epoch 7 Batch 150 Loss 1.3062\n",
            "\n",
            "Epoch 7 Loss: 1.2802\n",
            "Time taken for 1 epoch 12.40 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2719\n",
            "Epoch 8 Batch 50 Loss 1.2354\n",
            "Epoch 8 Batch 100 Loss 1.2318\n",
            "Epoch 8 Batch 150 Loss 1.2602\n",
            "\n",
            "Epoch 8 Loss: 1.2400\n",
            "Time taken for 1 epoch 11.28 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1740\n",
            "Epoch 9 Batch 50 Loss 1.1983\n",
            "Epoch 9 Batch 100 Loss 1.2311\n",
            "Epoch 9 Batch 150 Loss 1.2421\n",
            "\n",
            "Epoch 9 Loss: 1.1996\n",
            "Time taken for 1 epoch 11.32 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1494\n",
            "Epoch 10 Batch 50 Loss 1.1390\n",
            "Epoch 10 Batch 100 Loss 1.1530\n",
            "Epoch 10 Batch 150 Loss 1.1565\n",
            "\n",
            "Epoch 10 Loss: 1.1595\n",
            "Time taken for 1 epoch 11.45 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Jalankan kode diatas dan sebutkan perbedaanya dengan praktikum 2?"
      ],
      "metadata": {
        "id": "AjtwVs6uQuc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perbedaan kode di atas dengan kode pada praktikum 2 adalah, terjadinya peningkatan waktu pengerjaan yang cukup drastis pada beberapa epoch. Namun, nilai loss yang dihasilkan cukup menurun daripada kode pada praktikum 2"
      ],
      "metadata": {
        "id": "c5Dm7LAKQuVc"
      }
    }
  ]
}