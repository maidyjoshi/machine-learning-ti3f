{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Library dan Persiapan Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Unduh dataset MNIST dari scikit-learn\n",
    "mnist = fetch_openml('mnist_784')\n",
    "\n",
    "# Pisahkan data gambar dan label\n",
    "images = mnist.data.astype('float32')\n",
    "labels = mnist.target.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images /= 255.0  # Skalakan data\n",
    "\n",
    "# Bagi data menjadi data pelatihan dan pengujian\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    images, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Reshape(target_shape=(28, 28, 1), input_shape=(784,)), #engubah bentuk input dari vektor 1D (784 elemen) menjadi matriks 3D (28x28x1). \n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)), #Menambahkan lapisan konvolusi dengan 32 filter, masing-masing berukuran 3x3, dan fungsi aktivasi ReLU.\n",
    "    layers.MaxPooling2D((2, 2)), #Menambahkan lapisan max pooling dengan ukuran jendela 2x2.\n",
    "    layers.Flatten(), #Mengubah output dari lapisan sebelumnya (berupa matriks) menjadi vektor 1D.\n",
    "    layers.Dense(128, activation='relu'), #Menambahkan lapisan terhubung penuh (fully connected) dengan 128 neuron dan fungsi aktivasi ReLU.\n",
    "    layers.Dense(64, activation='relu'), #Menambahkan lapisan terhubung penuh dengan 64 neuron dan fungsi aktivasi ReLU.\n",
    "    layers.Dense(10, activation='softmax') #Menambahkan lapisan terhubung penuh dengan 10 neuron (sesuai dengan jumlah kelas digit) dan fungsi aktivasi softmax.\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', #mengoptimalkan proses pelatihan\n",
    "              loss='sparse_categorical_crossentropy', # Memilih fungsi kerugian untuk masalah klasifikasi dengan banyak kelas.\n",
    "              metrics=['accuracy']) #Menggunakan metrik akurasi untuk memantau kinerja model s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1400/1400 [==============================] - 70s 48ms/step - loss: 0.1855 - accuracy: 0.9434 - val_loss: 0.0881 - val_accuracy: 0.9724\n",
      "Epoch 2/10\n",
      "1400/1400 [==============================] - 76s 54ms/step - loss: 0.0604 - accuracy: 0.9811 - val_loss: 0.0579 - val_accuracy: 0.9814\n",
      "Epoch 3/10\n",
      "1400/1400 [==============================] - 75s 54ms/step - loss: 0.0395 - accuracy: 0.9875 - val_loss: 0.0751 - val_accuracy: 0.9778\n",
      "Epoch 4/10\n",
      "1400/1400 [==============================] - 75s 53ms/step - loss: 0.0255 - accuracy: 0.9915 - val_loss: 0.0566 - val_accuracy: 0.9846\n",
      "Epoch 5/10\n",
      "1400/1400 [==============================] - 70s 50ms/step - loss: 0.0187 - accuracy: 0.9936 - val_loss: 0.0529 - val_accuracy: 0.9844\n",
      "Epoch 6/10\n",
      "1400/1400 [==============================] - 71s 51ms/step - loss: 0.0144 - accuracy: 0.9956 - val_loss: 0.0663 - val_accuracy: 0.9828\n",
      "Epoch 7/10\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 0.0115 - accuracy: 0.9960 - val_loss: 0.0601 - val_accuracy: 0.9850\n",
      "Epoch 8/10\n",
      "1400/1400 [==============================] - 71s 51ms/step - loss: 0.0086 - accuracy: 0.9971 - val_loss: 0.0731 - val_accuracy: 0.9814\n",
      "Epoch 9/10\n",
      "1400/1400 [==============================] - 69s 49ms/step - loss: 0.0071 - accuracy: 0.9975 - val_loss: 0.0779 - val_accuracy: 0.9819\n",
      "Epoch 10/10\n",
      "1400/1400 [==============================] - 68s 49ms/step - loss: 0.0074 - accuracy: 0.9974 - val_loss: 0.0737 - val_accuracy: 0.9829\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels,\n",
    "                    epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438/438 [==============================] - 6s 12ms/step - loss: 0.0759 - accuracy: 0.9844\n",
      "Akurasi pada data pengujian: 0.98\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Akurasi pada data pengujian: {test_acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1750/1750 [==============================] - 21s 12ms/step - loss: 0.0213 - accuracy: 0.9943\n",
      "Akurasi pada data pelatihan: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Evaluasi akurasi data pelatihan\n",
    "train_loss, train_acc = model.evaluate(train_images, train_labels)\n",
    "print(f'Akurasi pada data pelatihan: {train_acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438/438 [==============================] - 6s 12ms/step\n",
      "Matriks Kebingungan:\n",
      "[[1332    2    0    1    0    0    3    3    1    1]\n",
      " [   0 1591    1    1    0    1    0    5    1    0]\n",
      " [   1   11 1338    6    0    0    0   16    7    1]\n",
      " [   1    3    2 1418    0    1    0    3    4    1]\n",
      " [   0    0    0    0 1287    0    0    4    0    4]\n",
      " [   0    3    1   19    0 1239    5    1    5    0]\n",
      " [   1    6    1    0    4    1 1382    0    1    0]\n",
      " [   1    1    5    1    0    0    0 1494    0    1]\n",
      " [   1    7    1    5    0    2    4   10 1323    4]\n",
      " [   4    2    0    7    9    2    0   17    2 1377]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Membuat prediksi pada data pengujian\n",
    "predictions = model.predict(test_images)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Membuat matriks kebingungan\n",
    "confusion = confusion_matrix(test_labels, predicted_labels)\n",
    "print('Matriks Kebingungan:')\n",
    "print(confusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
