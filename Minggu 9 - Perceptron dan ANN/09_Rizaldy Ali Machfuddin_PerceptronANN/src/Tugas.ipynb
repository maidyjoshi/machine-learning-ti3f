{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUGAS PRAKTIKUM\n",
    "1. Lakukan klasifikasi pada data MNIST dengan menggunakan model ANN\n",
    "2. Anda diperbolehkan melakukan eksplorasi terhadap,\n",
    "- Metode pra pengolahan\n",
    "- Pemilihan fitur\n",
    "- Arsitektur ANN\n",
    "- Fungsi Aktiviasi\n",
    "3. ANN diimplementasikan dengan menggunakan tensowflow\n",
    "4. DIKERJAKAN SECARA BERKELOMPOK\n",
    "5. JELASKAN HASIL YANG ANDA DAPATKAN.\n",
    "- AKURASI\n",
    "- CONFUSION MATRIX\n",
    "- KONFIGURASI MODEL --> MULAI DARI PRA PENGOLAHAN SAMPAI ARSITEKTUR ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langkah 1 - Impor Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\POLINEMA\\2B SEMESTER 5\\Pembelajaran Mesin\\Pembelajaran_Mesin2023\\Week09V\\myenv\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Load dataset MNIST\n",
    "data_mnist = datasets.fetch_openml(\"mnist_784\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_mnist = datasets.fetch_openml(\"mnist_784\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langkah 2 - Eksplorasi Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah sampel MNIST: 70000\n",
      "Dimensi setiap sampel MNIST: 784\n",
      "Jumlah kelas MNIST: 10\n"
     ]
    }
   ],
   "source": [
    "# Melihat dimensi data\n",
    "print(\"Jumlah sampel MNIST:\", data_mnist.data.shape[0])  # Jumlah sampel\n",
    "print(\"Dimensi setiap sampel MNIST:\", data_mnist.data.shape[1])  # Dimensi setiap sampel\n",
    "print(\"Jumlah kelas MNIST:\", len(set(data_mnist.target)))  # Jumlah kelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengambil data dan label\n",
    "X = data_mnist.data.astype('float32')\n",
    "y = data_mnist.target.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling data menggunakan MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scalling = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_scalling[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langkah 3 - Ekstraksi Fitur dan Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=200)\n",
    "X_pca = pca.fit_transform(X_scalling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset menjadi data pelatihan dan data pengujian\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langkah 4 - Membangun Model ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               25728     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36394 (142.16 KB)\n",
      "Trainable params: 36394 (142.16 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Membuat model ANN\n",
    "model = Sequential()\n",
    "\n",
    "# Tambahkan lapisan input\n",
    "model.add(Dense(units=128, activation='relu', input_dim=200))  # 128 unit pada lapisan tersembunyi pertama\n",
    "\n",
    "# Tambahkan lapisan tersembunyi pertama\n",
    "model.add(Dense(units=64, activation='relu'))  # 64 unit pada lapisan tersembunyi kedua\n",
    "\n",
    "# Tambahkan lapisan tersembunyi kedua\n",
    "model.add(Dense(units=32, activation='relu'))  # 32 unit pada lapisan tersembunyi ketiga\n",
    "\n",
    "# Tambahkan lapisan output\n",
    "model.add(Dense(units=10, activation='softmax'))  # 10 unit pada lapisan output untuk klasifikasi 10 kelas\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langkah 5 - Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kompilasi model ANN\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1400/1400 [==============================] - 9s 4ms/step - loss: 0.3015 - accuracy: 0.9122 - val_loss: 0.1405 - val_accuracy: 0.9577\n",
      "Epoch 2/10\n",
      "1400/1400 [==============================] - 6s 4ms/step - loss: 0.1026 - accuracy: 0.9690 - val_loss: 0.1122 - val_accuracy: 0.9672\n",
      "Epoch 3/10\n",
      "1400/1400 [==============================] - 6s 4ms/step - loss: 0.0660 - accuracy: 0.9796 - val_loss: 0.1108 - val_accuracy: 0.9655\n",
      "Epoch 4/10\n",
      "1400/1400 [==============================] - 6s 4ms/step - loss: 0.0435 - accuracy: 0.9867 - val_loss: 0.1101 - val_accuracy: 0.9684\n",
      "Epoch 5/10\n",
      "1400/1400 [==============================] - 6s 4ms/step - loss: 0.0311 - accuracy: 0.9902 - val_loss: 0.1069 - val_accuracy: 0.9693\n",
      "Epoch 6/10\n",
      "1400/1400 [==============================] - 5s 4ms/step - loss: 0.0230 - accuracy: 0.9921 - val_loss: 0.1332 - val_accuracy: 0.9679\n",
      "Epoch 7/10\n",
      "1400/1400 [==============================] - 6s 4ms/step - loss: 0.0201 - accuracy: 0.9930 - val_loss: 0.1144 - val_accuracy: 0.9729\n",
      "Epoch 8/10\n",
      "1400/1400 [==============================] - 6s 4ms/step - loss: 0.0156 - accuracy: 0.9944 - val_loss: 0.1269 - val_accuracy: 0.9691\n",
      "Epoch 9/10\n",
      "1400/1400 [==============================] - 6s 4ms/step - loss: 0.0163 - accuracy: 0.9942 - val_loss: 0.1486 - val_accuracy: 0.9687\n",
      "Epoch 10/10\n",
      "1400/1400 [==============================] - 6s 4ms/step - loss: 0.0148 - accuracy: 0.9950 - val_loss: 0.1523 - val_accuracy: 0.9696\n",
      "438/438 [==============================] - 2s 4ms/step - loss: 0.1603 - accuracy: 0.9666\n",
      "Loss: 0.16030019521713257\n",
      "Accuracy: 0.9665714502334595\n"
     ]
    }
   ],
   "source": [
    "# Melatih model menggunakan data pelatihan\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)  # Contoh menggunakan 10 epoch dan ukuran batch 32\n",
    "\n",
    "# Evaluasi model menggunakan data pengujian\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langkah 6 - Evaluasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1750/1750 [==============================] - 5s 3ms/step - loss: 0.0402 - accuracy: 0.9907\n",
      "Train Loss: 0.040227919816970825\n",
      "Train Accuracy: 0.9906785488128662\n"
     ]
    }
   ],
   "source": [
    "# Evaluasi model pada data pelatihan\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
    "print(\"Train Loss:\", train_loss)\n",
    "print(\"Train Accuracy:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438/438 [==============================] - 1s 3ms/step - loss: 0.1603 - accuracy: 0.9666\n",
      "Test Loss: 0.16030019521713257\n",
      "Test Accuracy: 0.9665714502334595\n"
     ]
    }
   ],
   "source": [
    "# Evaluasi model pada data pengujian\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438/438 [==============================] - 1s 3ms/step\n",
      "Confusion Matrix:\n",
      "[[1315    0    4    1    1    1    3    4   13    1]\n",
      " [   0 1566    8    1    3    0    0    4   18    0]\n",
      " [   0    3 1349    5    4    1    1    7    9    1]\n",
      " [   0    3   20 1369    0   12    0    9   14    6]\n",
      " [   1    1    5    0 1256    0    5    4   11   12]\n",
      " [   0    1    1   13    0 1237    6    1   12    2]\n",
      " [   4    1    3    0    4   12 1362    2    8    0]\n",
      " [   4    1   17    3    3    1    0 1460    4   10]\n",
      " [   2    5   16   11    2   14    3    6 1292    6]\n",
      " [   4    1    1   11   20   10    0   28   19 1326]]\n"
     ]
    }
   ],
   "source": [
    "# Melakukan prediksi menggunakan model pada data pengujian\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Mengubah prediksi dari format one-hot encoded ke kelas\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Menghitung confusion matrix\n",
    "confusion_mtx = confusion_matrix(y_test, predicted_classes)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mtx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
